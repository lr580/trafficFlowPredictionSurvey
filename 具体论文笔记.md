说明：

- 所有论文按自定义编号 **T-XXX**，编号对应表存储在 Zotero 里，自行查阅。收录具体实现论文，不包括综述。综述在 `内容整理.md` 主要记录。

- 引用量截止查询时间(按 2024 暑假算)，参考谷歌学术，按 **CI-xxx** 表示引用量。(CI: cite)

- CCF 评级按照 **CCF-xxx** 来。

- 年份/日期按照 **DT-yyyy** 来，或 DT-yyyy-mm。(DT: datetime)

- 论文简称写在标题

- 按照 **C-XXX** 给论文打标签。(C: category)

  > 具体标签有：-T (target) 预测目标(速度，流量等)

# 具体论文

火爆/经典的标准：引用量特别多或衍生模型特别多，其他是杂鱼

## 经典

### T-28 STGCN

CCF-A CI-3886 DT-2017 C-T-SPEED

#### 阅读笔记

##### 模型

两种 CNN 分别处理时间和空间特征

组成：两个时空卷积块和全连接层输出，每个块由两个时间卷积夹着一个空间卷积，使用了残差连接和瓶颈策略

> (GPT) 瓶颈策略的核心思想是通过引入某种形式的限制（如维度压缩、计算资源限制等），强制模型做出选择，聚焦于最重要的信息。这在提升模型的泛化能力、稳定性以及减少计算资源的使用等方面，都具有重要作用。

![image-20240815222203379](img/image-20240815222203379.png)

图2说明：

Architecture of spatio-temporal graph convolutional networks. The framework STGCN consists of two spatio-temporal convolutional blocks (ST-Conv blocks) and a fully-connected output layer in the end. Each ST-Conv block contains two temporal gated convolution layers and one spatial graph convolution layer in the middle. The residual connection and bottleneck strategy are applied inside each block.

超参数选择的方法：Tree-structured Parzen Estimator (TPE) (有论文)

##### 数据

研究交通速度

数据集是

自建：BJER4

- gathered from the major areas of east ring No.4 routes in Beijing City by double-loop detectors. There are 12 roads selected for our experiment. The traffic data are aggregated every 5 minutes. The time period used is from 1st July to 31st August, 2014 except the weekends. We select the first month of historical speed records as training set, and the rest serves as validation and test set respectively

标准：PeMSD7

- 使用加权矩阵，距离的分类讨论函数是边权

- 数据描述：(参考官方代码仓库 README.md)

  每天的记录次数乘以天数是行，道路数是列，内容是速度记录，节点数是 228 和 1026，行数是 12672(我算出来44天)，值是 60 多和 70 多的占大头，有极少其他值，预测取了 15/30/45min，预测视野越大误差越大

  - PeMSD7_V_{`$num_route`}.csv : Historical Speed Records with shape of `[len_seq * num_road] (len_seq = day_slot * num_dates)`.
  - PeMSD7_W_{`$num_route`}.csv : Weighted Adjacency Matrix with shape of `[num_road * num_road]`.

论文里有图表展示了有一个U形和双U形数据(蓝色是真实)

附录有很多张图表示实验结果

查阅得知，其中的 FC-LSTM 论文出处根本就不是这个领域的，而是该技术的经典论文或迁移应用，怀疑是自建模型

##### 代码

[src](https://github.com/VeritasYin/STGCN_IJCAI-18/tree/master)

#### 相关评价

T-ZS2 总结：

- 堆叠多个时空卷积块，每个块连接两个时间卷积层和一个图卷积层

  > stacks multiple spatio-temporal convolution blocks and each block concatenate two temporal convolution and one graph convolution layer.  

- 使用 ChebNet 作为图卷积操作，用一阶近似比较

  > ChebNet is chosen as the graph convolution operator in STGCN, after a comparison with its first-order approximation  

- CNN 代替 RNN 获取时间加快了训练时间

  > The usage of temporal convolution layers instead of RNNs for temporal modeling accelerates the training phase of STGCN  

#### 相关改版

- T-128

- 变种 ASTGCN 引入两个注意力层分别获取时空动态联系 T-81

  > Attention based Spatio-temporal graph convolutional network (ASTGCN) further introduces two attention layers in STGCN to capture the dynamic correlations in spatial dimension and temporal dimension, respectively  

### T-40 DCRNN

CCF-NONE CI-3654 DT-2018 C-T-SPEED

#### 阅读笔记

##### 模型

空间：图双向随机游走

时间：encoder-decoder

DCRNN captures the spatial dependency using bidirectional random walks on the graph, and the temporal dependency using the encoder-decoder architecture with scheduled sampling

- 空间

  We model the spatial dependency by relating traffic flow to a diffusion process, which explicitly captures the stochastic nature of traffic dynamics. This diffusion process is characterized by a random walk on G with restart probability, and a state transition matrix

- 时间

  We replace the matrix multiplications in GRU with the diffusion convolution

  In multiple step ahead forecasting, Both the encoder and the decoder are recurrent neural networks with Diffusion Convolutional Gated Recurrent Unit (DCGRU)

##### 数据

特征描述

![image-20240817003909585](img/image-20240817003909585.png)

Figure 1: Spatial correlation is dominated by road network structure. (1) Traffic speed in road 1 are similar to road 2 as they locate in the same highway. (2) Road 1 and road 3 locate in the opposite directions of the highway. Though close to each other in the Euclidean space, their road network distance is large, and their traffic speeds differ significantly.

数据集：交通速度预测

- METR-LA 感觉是完全数据集
- PEMS-BAY 该数据集的开山

#### 相关评价

(T-ZS1)

encoder-decoder 图扩散比 FNN, LSTM 好，消融证明扩散卷积更好

have demonstrated that their encoder-decoder model with graph diffusion managed to outperform simple FNN and LSTM. Not only that, they also performed an ablation test to demonstrate that their novel diffusion convolution module manages to outperform simpler variations 

GRU: 矩阵乘法改为扩散卷积操作

replaced the matrix multiplication inside Gated Recurrent Unit with the diffusion convolution operation 



(T-ZS2)

最出名的一个是 DCRNN

> (T-ZS2) The most famous one is diffusion convolutional recurrent neural network (DCRNN)  

- 图卷积网络和 RNN 学习时空

  > which uses diffusion graph convolutional networks and RNN to learn the representations of spatial dependencies and temporal relations  

- 最初用于交通速度预测，现在成为了基准模型

  > DCRNN was originally proposed for traffic speed forecasting and is now widely used as a baseline.  

- 建图方式：距离矩阵

  > To create the traffic graph, the adjacency matrix is defined as the thresholded pairwise road network distances.  

- 支持有向图，引入扩散卷积(DC)

  > Compared with other graph convolutional models that can only operate on undirected graphs, e.g., ChebNet, DCRNN introduces the diffusion convolution (DC) operation for directed graph and is more suitable for transportation scenarios, which is defined as follows:   

  $$
  \mathbf X_{*DC}=\sum_{k=0}^{K-1}(\theta_{k,1}(D_O^{-1}A)^k+\theta_{k,2}(D_I^{-1}A^T))\mathbf X
  $$

  其中 $\mathbf X\in R^{N\times d}$ 是节点特征矩阵，$A$ 是邻接矩阵，$D_O,D_I$ 是对角矩阵：出度和入度，$\theta$ 是两个模型参数，$K$ 是扩散步数。

  区分入度和出度实现了有向图 

  > where X 2 RN×d is the node feature matrix, A is the adjacency matrix, DO and DI are diagonal out-degree and in-degree matrices, θk;1 and θk;2 are model parameters, K is the number of diffusion steps. By defining and using out-degree and in-degree matrices, DCRNN models the bidirectional diffusion process to capture the influence of both upstream and downstream traffic  

- 对无向图不是很实用

  > While DCRNN is a strong baseline, it is not suitable or desirable for the undirected graph cases  

- 扩展版本 T-88：统一构建 RNN，基于任意图卷积

  Then DCRNN is extended with a stronger learning ability in graph GRU a unified method for constructing an RNN based on an arbitrary graph convolution operator is proposed, instead of the single RNN model used in DCRNN

观察图扩散过程和邻节点关联

observed the traffic diffusion along the road network and the correlation between several adjacent traffic sensors

### T-64 Graph WaveNet

CCF-A CI-1984 DT-2019 C-T-SPEED

#### 阅读笔记

##### 模型

空间：自适应依赖矩阵，点嵌入学习，图卷积

时间：堆叠扩张(如图2，倍增) 1D CNN 任意卷积

By developing a novel adaptive dependency matrix and learn it through node embedding, our model can precisely capture the hidden spatial dependency in the data. 

With a stacked dilated 1D convolution component whose receptive field grows exponentially as the number of layers increases, Graph WaveNet is able to handle very long sequences

- We propose a graph convolution layer in which a self-adaptive adjacency matrix can be learned from the data through an end-to-end supervised training
- we adopt stacked dilated casual convolutions to capture temporal dependencies. The receptive field size of stacked dilated casual convolution networks grows exponentially with an increase in the number of hidden layers

> (GPT) “end-to-end”（端到端）指的是一种模型训练方式，在这种方式中，模型从输入的原始数据开始，直接学习到最终输出的目标结果，中间的所有步骤都是在同一个模型中完成的

图3：

![image-20240817224043300](img/image-20240817224043300.png)

##### 数据

METR-LA PEMS-BAY

做了时间效率对比，看样子做了消融实验

静态图可能不能反应真实依赖

However, the explicit graph structure (relation) does not necessarily reflect the true dependency and genuine relation may be missing due to the incomplete connections in the data.

静态的局限性的例子

To give each circumstance an example, let us consider a recommendation system. In the first case, two users are connected, but they may have distinct preferences over products. In the second case, two users may share a similar preference

空间影响的直观图示

<img src="img/image-20240817220918943.png" alt="image-20240817220918943" style="zoom:67%;" />



#### 相关评价

- 自适应矩阵自动发现隐藏图结构，任意卷积学习时间关联

  > constructs a self-adaptive matrix to uncover unseen graph structures automatically from the data and WaveNet, which is based on causal convolutions, is used to learn temporal relations.  

- 训练后矩阵固定，不能适应动态的数据

  > However, the self-adaptive matrix in Graph WaveNet is fixed after training, which is unable to be adjusted dynamically with the data characteristics  

#### 相关改版

- T-136 STEP
- T-135 STD-MAE
- T-131 STAWnet
- T-130 微调

## 火爆

### T-110 AGCRN

CCF-A CI-1060 DT-2020 C-T-FLOW

#### 阅读笔记

##### 模型

空间：GCN 改进

时间：GRU

we propose two adaptive modules for enhancing Graph Convolutional Network (GCN) with new capabilities: 1) a Node Adaptive Parameter Learning (NAPL) module to capture node-specific patterns; 2) a Data Adaptive Graph Generation (DAGG) module to infer the inter-dependencies among different traffic series automatically. We further propose an Adaptive Graph Convolutional Recurrent Network (AGCRN) to capture fine-grained spatial and temporal correlations in traffic series automatically based on the two modules and recurrent networks.

- a Node Adaptive Parameter Learning (NAPL) module to learn node-specific patterns for each traffic series—NAPL factorizes the parameters in traditional GCN and generates node-specific parameters from a weights pool and bias pool shared by all nodes according to the node embedding
- a Data Adaptive Graph Generation (DAGG) module to infer the node embedding (attributes) from data and to generate the graph during training. NAPL and DAGG are independent and can be adapted to existing GCN-based traffic forecasting models both separately and jointly
- we combine NAPL and DAGG with recurrent networks and propose a unified traffic forecasting model - Adaptive Graph Convolutional Recurrent Network (AGCRN). AGCRN can capture fine-grained node-specific spatial and temporal correlations in the traffic series and unify the nodes embeddings in the revised GCNs with the embedding in DAGG

##### 数据

PEMSD4, PEMSD8

![image-20240819010446183](img/image-20240819010446183.png)

### T-142 MTGNN

CCF-A CI-1294 DT-2020 C-T-SPEED

#### 阅读笔记

##### 模型

空间：图卷积

时间：1D CNN

Our approach automatically extracts the uni-directed relations among variables through a graph learning module, into which external knowledge like variable attributes can be easily integrated. A novel mix-hop propagation layer and a dilated inception layer are further proposed to capture the spatial and temporal dependencies within the time series. The graph learning, graph convolution, and temporal convolution modules are jointly learned in an end-to-end framework.

- we propose a novel graph learning layer, which extracts a sparse graph adjacency matrix adaptively based on data. Furthermore, we develop a graph convolution module to address the spatial dependencies among variables, given the adjacency matrix computed by the graph learning layer. This is designed specifically for directed graphs and avoids the over-smoothing problem that frequently occurs in graph convolutional networks
- Finally, we propose a temporal convolution module to capture temporal patterns by modified 1D convolutions. It can both discover temporal patterns with multiple frequencies and process very long sequences.

![image-20240819012120958](img/image-20240819012120958.png)

##### 数据

METR-LA PEMS-BAY 和其他交通外的数据，有输入长度表示为 12

### T-36 T-GCN

CCF-B CI-2248 DT-2019 C-T-SPEED

#### 阅读笔记

##### 数据

SZ-taxi and Los-loop datasets.

> (1) SZ-taxi. This dataset was the taxi trajectory of Shenzhen from Jan. 1 to Jan. 31, 2015. We selected 156 major roads of Luohu District as the study area. The experimental data mainly includes two parts. One is an 156*156 adjacency matrix, which describes the spatial relationship between roads. Each row represents one road and the values in the matrix represent the connectivity between the roads. Another one is a feature matrix, which describes the speed changes over time on each road. Each row represents one road; each column is the traffic speed on the roads in different time periods. We aggregate the traffic speed on each road every 15 minutes. 
>
> (2) Los-loop. This dataset was collected in the highway of Los Angeles County in real time by loop detectors. We selected 207 sensors and its traffic speed from Mar.1 to Mar.7, 2012. We aggregated the traffic speed every 5 minutes. Similarity, the data concludes an adjacency matrix and a feature matrix. The adjacency matrix is calculated by the distance between sensors in the traffic networks. Since the Los-loop dataset contained some missing data, we used the linear interpolation method to fill missing values. 
>
> In the experiments,the input data was normalized to the interval [0,1]. In addition, 80% of the data was used as the training set and the remaining 20% was used as the testing set. We predicted the traffic speed of the next 15 minutes, 30 minutes, 45 minutes and 60 minutes.

#### 相关评价

GCN(空间) + GRU(时间)

used a Gated Recurrent Unit which takes input from a Graph Convolution Network and outputs the predicted traffic

### T-143 GMAN

CCF-A CI-1262 DT-2019

#### 阅读笔记

##### 模型

encoder-decoder 时空都用注意力

In this paper, we focus on the spatio-temporal factors, and propose a graph multi-attention network (GMAN) to predict traffic conditions for time steps ahead at different locations on a road network graph. GMAN adapts an encoder-decoder architecture, where both the encoder and the decoder consist of multiple spatio-temporal attention blocks to model the impact of the spatio-temporal factors on traffic conditions. The encoder encodes the input traffic features and the decoder predicts the output sequence. Between the encoder and the decoder, a transform attention layer is applied to convert the encoded traffic features to generate the sequence representations of future time steps as the input of the decoder. The transform attention mechanism models the direct relationships between historical and future time steps that helps to alleviate the error propagation problem among prediction time steps

![image-20240819013039961](img/image-20240819013039961.png)

##### 数据

traffic volume prediction on the Xiamen dataset

which contains 5 months of data recorded by 95 traffic sensors ranging from August 1st, 2015 to December 31st, 2015 in Xiamen, China

以及 PEMS-BAY

![image-20240819013254073](img/image-20240819013254073.png)

## 杂鱼

### T-129 DCGCN

CCF-NONE CI-24 DT-2023 C-T-SPEED

#### 阅读笔记

我觉得作用不大，放表格里应该就行

##### 模型

动态贝叶斯网络，图卷积网络，动态任意图 (GCN+RNN)

In this work, we propose a novel approach for traffic prediction that embeds time-varying dynamic Bayesian network to capture the fine spatiotemporal topology of traffic data. 

We then use graph convolutional networks to generate traffic forecasts. 

To enable our method to efficiently model nonlinear traffic propagation patterns, we develop a deep learning-based module as a hyper-network to generate stepwise dynamic causal graphs

- we propose a novel causal-embedded approach for traffic prediction. It represents the spatiotemporal traffic network topology using a time-varying DBN (TVDBN), which is designed to adapt to the time-varying traffic propagation patterns by learning DBNs step by step
- The learned TVDBN is able to summarize the dynamic spatiotemporal dependencies between nodes. Built upon it, graph convolution is applied to capture spatial dependencies for traffic prediction.
- We propose a complete deep learning based causal structure learning module that serves as a pretrained hyper-network to generate the graphs of the TVDBN.
- The predefined distance graph is further incorporated into graph generator and traffic prediction module as additional prior information to improve the performance.
- We propose an approach based on GCN and RNN to learn a TVDBN that describes the time-varying causal relationships between different locations in traffic network. The augmented Lagrange method is applied for model training to ensure the acyclicity of the TVDBN



> poe assistant 概括：
>
> **时空特征处理：**
>
> - **动态贝叶斯网络 (DBN)：** 原文中提到 "We propose an approach based on GCN and RNN to learn a TVDBN that describes the time-varying causal relationships between different locations in traffic network." 以及 "The learned TVDBN is able to summarize the dynamic spatiotemporal dependencies between nodes."
> - **递归神经网络 (RNN)：** 原文提到 "We propose to capture the time-varying causal structure in traffic series by RNN, which generates the dynamic causal graphs stepwise from its hidden state." 以及 "The challenge is to construct suitable input features for RNN that contain information about the dynamics of the causal structure."
> - **图卷积网络 (GCN)：** 原文提到 "We extend the linear SEM (6) to a nonlinear version by spatial-based graph convolution [39], i.e., Xt = ∑Kk=0GCONV2L(Xt−k, Bkt) + Zt, where GCONV2L is a L layers spatial-based graph convolution operation with skip connections defined as:" 以及 "The proposed model is evaluated on a real-world traffic dataset to demonstrate the effectiveness of the proposed approach."
>
> **空间特征处理：**
>
> - **图卷积网络 (GCN)：** 原文提到 "We use an attention mechanism with multi-head scaled dot-product [38], to describe the spatiotemporal correlations between nodes, i.e., sdot(Q, K) = (QWQ)(KWK)⊤√d ∈ RN×N, msdot(Q, K) = [sdot1(Q, K), ...,sdoth(Q, K)] ∈ RN×N×h, C̃0t = msdot(Xt, Xt), C̃kt = msdot(Xt, Xt−k), k = 1, ..., K, where WQ and WK are learnable parameters of linear transformation, h is the number of heads and the learnable parameters of each head are different." 以及 "We further flatten the tensors {C̃kt}Kk=0 to matrices in RN2×h, denoted by {Ckt}Kk=0, such that each row of Ckt captures the dynamics of an element in Bkt."
> - **先验图结构：** 原文提到 "The predefined distance graph is further incorporated into graph generator and traffic prediction module as additional prior information to improve the performance." 以及 "We conduct experiments on a widely used traffic benchmark dataset METR-LA to evaluate the traffic prediction performance of the proposed model."
>
> 您是对的，文章中没有明确的“时间特征”处理部分



> Casual discovery 相关：
>
> Casual discovery aims to analyze causal relationships behind statistical correlations of different variables and facilitate better machine learning.
>
> - Typical approaches to incorporate causal discovery include encoding features from domain-specific causal models as input to downstream tasks
> - and learning the structure of causal relationships between features for graph-based models
> - As a powerful graph-based tool for modeling directed causal relationships between variables, Bayesian network (BN) is being applied in traffic prediction
>
> 文心一言：Casual Discovery（因果发现）是因果推理领域中的一个重要研究方向，旨在从观测数据中识别和理解变量之间的潜在因果结构或关系。

#### 数据

METR-LA

A limitation of current casual-embedded traffic prediction models is the assumption of stationary temporal dependencies. However, in reality, the dependencies of traffic data in different places do change over time

### T-131 STAWnet

CCF-NONE CI-93 DT-2021 C-T-SPEED

#### 阅读笔记

##### 模型

时间：CNN (Gated TCN) gated temporal convolution network

空间：自注意力网络 (DAN) dynamic attention network

a multi-step prediction model named Spatial-Temporal Attention Wavenet (STAWnet) is proposed. Temporal convolution is applied to handle long time sequences, and the dynamic spatial dependencies between different nodes can be captured using the self-attention network. Different from existing models, STAWnet does not need prior knowledge of the graph by developing a self-learned node embedding. These components are integrated into an end-to-end framework. 

- self-adaptive node embedding : capture the hidden spatial relationship in the data without knowing the graph structure information

##### 数据

The experimental results on three public traffic prediction datasets (METR-LA, PEMS-BAY, and PEMS07) demonstrate effectiveness.

实验对比提到 T-GCN, FC-LSTM，注意到这俩经常出现特别是 FC-LSTM

### T-132 ADN

CCF-NONE CI-6 DT-2022

#### 阅读笔记

注意力时间，注意力空间

we propose the simplest possible model, called ADN for Attention Diffusion Network, which does not rely on any structural prior whatsoever. We choose an attention based encoder-decoder architecture, where attention is adapted to the bi-dimensionality of events as location-instant pairs

> poe 找的：
>
> - 使用 多头注意力机制 (MHA)
>
>   The model alternates attention in the temporal dimension (MHA(T)), and in the spatial dimension (MHA(N)).
>
> - 论文中提到模型使用 可分离注意力机制 (separable attention)，该机制通过分别处理时间维度和空间维度，来降低注意力矩阵的计算量
>
>   Since attention is a generalised form of convolution [1], separability works all the same with attention, where it is also known as axial attention [7]. Separable attention processes event-indexed objects along each dimension of the events (temporal and spatial) separately and alternately, just as spatially separable convolutions alternate processing images along their width and height dimensions.

数据：

Experiments are conducted on three public traffic datasets PEMS-BAY, METR-LA and PEMS07 released by [12] and [20]. The first two are the most commonly used for measuring model performances, and consist of 207 and 325 locations, respectively. We also tested our model on PEMS07 (883 locations), to check its scalability to larger road traffic networks.

### T-133 RGDAN

CCF-B CI-3 DT-2024 C-T-SPEED

#### 阅读笔记

##### 模型 

空间：GAT(Graph Attention Network, 图注意力网络)

时间：注意力

看论文图还有 encoder-decoder

we propose a Random Graph Diffusion Attention Network (RGDAN) for traffic prediction. RGDAN comprises a graph diffusion attention module and a temporal attention module. The graph diffusion attention module can adjust its weights by learning from data like a CNN to capture more realistic spatial dependencies. The temporal attention module captures the temporal correlations

- Random GATs discard traditional attention interaction (e.g., dot product) and instead, use a learnable matrix to discover the correlations between neighbors. This method omits the process of dot-product interaction in ordinary attention, reduces the time complexity, improves the computational speed of the model, and reduces the memory overhead
- Random Graph Diffusion Attention Network (RGDAN), consists of an attention mechanism with a graph diffusion attention module that extracts the spatial correlations. Temporal attention captures the temporal dependencies. The temporal and spatial features are fused by a gated fusion layer. The transformed attention module is used to alleviate the propagation error caused by the difference between the predicted and historical time steps

![image-20240818142743477](img/image-20240818142743477.png)

- A spatio-temporal embedding (STE) generator, which provides preliminary spatio-temporal information to the encoder and decoder by generating spatio-temporal embeddings

##### 数据

METR-LA PEMS-BAY NE-BJ

- NE-BJ (T-134 是鼻祖)

  This dataset is a collection of public transport speed data taken from the navigation data of Tencent Map. It contains 500 segments, with each road segment data contains 6509 5-min time steps.

##### 其他

提供了一个表格总结了用到的对比实验的相关论文

### T-134 DGCRN

CCF-B CI-298 DT-2021

#### 阅读笔记

##### 模型

encoder-decoder

空间：GNN 图卷积

时间：RNN

we propose a novel traffic prediction framework, named Dynamic Graph Convolutional Recurrent Network (DGCRN)

- In DGCRN, hyper-networks are designed to leverage and extract dynamic characteristics from node attributes, while the parameters of dynamic filters are generated at each time step. We filter the node embeddings and then use them to generate dynamic graph, which is integrated with pre-defined static graph
- We propose a GNN and RNN based model, where the dynamic adjacency matrix is designed to be generated from a hyper-network step by step synchronize with the iteration of RNN. The dynamic graph is incorporated with the pre-defined graph and skip connection to describe the dynamic characteristics of road networks more effectively, enhancing the performance of prediction.

![image-20240819020113960](img/image-20240819020113960.png)

Dynamic Graph Convolutional Recurrent Module (DGCRM)

![image-20240819020343932](img/image-20240819020343932.png)

##### 数据

METR-LA PEMS-BAY

NE-BJ



### T-135 STD-MAE

CCF-NONE CI-3 DT-2023 C-T-SPEED

#### 阅读笔记

##### 模型

时空都是 AE + 自注意力

self-supervised pre-training framework SpatialTemporal-Decoupled Masked Pre-training (STDMAE) that employs two decoupled masked autoencoders to reconstruct spatiotemporal series along the spatial and temporal dimensions

mask 的思想：学习填充缺失。基于这个进行预训练，有详细介绍

- The core idea is to mask parts of the input sequence during pre-training, requiring the model to reconstruct the missing contents
- spatial-temporal-decoupled masking, Such decoupled masking mechanism allows the model to learn representation that can capture clearer heterogeneity
- It consists of a temporal autoencoder (T-MAE) and a spatial autoencoder (S-MAE), both having a similar architecture
- S-MAE applies self-attention along spatial dimension, while T-MAE performs self-attention along temporal dimension

杂项：

- patch embedding technique: The long input is divided into non-overlapping patches
- Moreover, to simultaneously encode spatial and temporal positional information, we implement a two-dimensional positional encoding

模型结构：

- The spatial and temporal decoders each consists of a padding layer, a standard transformer layer, and a regression layer.

![image-20240818161239792](img/image-20240818161239792.png)

- 具体模型：GWNet (Graph WaveNet)

##### 数据

PEMSD3/4/7/8/BAY, METR-LA

### T-136 MegaCRN

#### 阅读笔记

##### 模型

空间 GCN( Graph Convolutional Networks (GCNs) )

时间 Gated Recurrent Unit (GRU)

we implement this idea into Meta-Graph Convolutional Recurrent Network (MegaCRN) by plugging the Meta-Graph Learner powered by a MetaNode Bank into GCRN encoder-decoder.

> Graph Structure Learning (GSL)
>
> spatio-temporal graph (STG)

The term meta-graph is coined to describe the generation of node embeddings (similar in adaptive and momentary) for GSL.

our STG learning consists of two steps: (1) querying node-level prototypes from a Meta-Node Bank; (2) reconstructing node embeddings with Hyper-Network

injecting graph convolution operation into recurrent cell (e.g. LSTM). The derived Graph Convolutional Recurrent Unit (GCRU) can thereby simultaneously capture

- spatial dependency, represented by an input graph topology, and
- temporal dependency in a sequential manner.

##### 其他

经验准则 Tobler’s first law of geography

> 托布勒第一地理学定律指出：“事物彼此靠近”。换句话说，地理上相近的事物比相距较远的事物更相似。
>
> 这个定律在许多领域都有应用，包括：
>
> - 地理学： 理解空间模式和地理现象之间的关系。
> - 城市规划： 规划城市基础设施和服务，以满足居民的需求。
> - 环境科学： 研究污染物和生态系统之间的关系。
> - 社会学： 分析社会现象的空间分布。
>
> 托布勒第一地理学定律是一个重要的概念，它提醒我们地理位置在塑造世界中起着至关重要的作用。

### T-137 STEP

CCF-A CI-146 DT-2022 C-T-SPEED C-T-FLOW

#### 阅读笔记

##### 模型

时间：Transformer

空间：图

以及 encoder-decoder，但是基于 Grah WaveNet 改编

we propose a novel framework, in which <u>ST</u>GNN is <u>E</u>nhanced by a scalable time series <u>P</u>re-training model (STEP). 

Specifically, we design a pre-training model to efficiently learn temporal patterns from very long-term history time series (e.g., the past two weeks) and generate segment-level representations. 

These representations provide contextual information for short-term time series input to STGNNs and facilitate modeling dependencies between time series

- Specifically, we design an efficient unsupervised pre-training model for <u>T</u>ime <u>S</u>eries based on Trans<u>Former</u> blocks (TSFormer)
- which is trained through the masked autoencoding strategy
- design a graph structure learner based on the representation of TSFormer, which learns discrete dependency graph and utilizes the 𝑘NN graph computed based on the representation of TSFormer as a regularization to guide the joint training of graph structure and STGNN

![image-20240818173141567](img/image-20240818173141567.png)

STEP framework can extend to almost any STGNN, and we choose a representative method as our backend, the Graph WaveNet

##### 数据

METR-LA PEMS-BAY PEMS04(flow)

### T-138 D2STGNN

CCF-NONE CI-140 DT-2022 C-T-SPEED C-T-FLOW

#### 阅读笔记

##### 模型

空间：GNN + 自注意力

时间：RNN + 自注意力

to improve modeling performance, we propose a novel Decoupled Spatial-Temporal F ramework (DSTF) that separates the diffusion and inherent traffic information in a data-driven manner, which encompasses a unique estimation gate and a residual decomposition mechanism

- The former (residual) removes the parts of signals that the diffusion and inherent models can approximate well. Thus, the parts of signals that are not learned well is retained. 
- The latter (gate) estimates roughly the proportion of the two kinds of signals to relieve the burden of the first model in each layer, which takes the original signal as input and needs to learn specific parts in it

we propose an instantiation of DSTF, Decoupled Dynamic Spatial-Temporal Graph Neural Network (D2STGNN), that captures spatial-temporal correlations and also features a dynamic graph learning module that targets the learning of the dynamic characteristics of traffic networks

- spatial dependency by learning latent correlations between time series based on the self-attention mechanism.
- A spatial-temporal localized convolution is designed to model the hidden diffusion time series. A recurrent neural network and self-attention mechanism are used jointly to model the hidden inherent time series.

##### 数据

METR-LA PEMS-BAY 速度

PEMS04 PEMS08 流量

### T-139 STAEformer

CCF-B CI-57 DT-2023 C-T-SPEED C-T-FLOW

#### 阅读笔记

##### 模型

时空自适应嵌入 + 朴素 transformer

时空都是自注意力层，如图所示，嵌入可以额外融入日期等信息

In this study, we present a novel component called spatio-temporal adaptive embedding that can yield outstanding results with vanilla transformers. 

Our proposed Spatio-Temporal Adaptive Embedding transformer (STAEformer)

- Specifically, it adds an embedding layer on the input, providing multiple types of embeddings for the model backbone

![image-20240818223717438](img/image-20240818223717438.png)

##### 数据

METR-LA, PEMS-BAY, PEMS03/4/7/8

### T-140 SLCNN

CCF-A CI-234 DT-2020 T-C-SPEED

#### 阅读笔记

##### 模型

空间：CNN + GNN

时间：CNN

we propose a novel framework named Structure Learning Convolution (SLC) that enables to extend the traditional convolutional neural network (CNN) to graph domains and learn the graph structure for traffic forecasting

- We propose a generic graph convolutional formulation, which defines convolution operation as a combination of structure module and kernel module
- Two data-driven and time-vary SLC modules are proposed to capture the global and local structures, respectively
- P3D ConvNet is incorporated in SLCNN to capture the temporal dependencies in traffic data

##### 数据

PeMS-S (找不到来源)

The time range of PeMS-S is the weekdays of May and Jun of 2012, the interval is 5 minute and 228 sensors (nodes) are selected

PEMS-BAY, METR-LA 

BJF, BRF, BRF-L 自建

- are generated from a real-world GPS trajectory data, in which about 80 million GPS points of 30,000 taxis are recoded per day
- BJF. Each node in BJF indicates a junction and 190 important junctions in Beijing are selected. The traffic data of each node is recorded in every 15 minutes and the time range is from November 2015 to October 2016. 
- BRF. BRF has totally 300 nodes and each node indicates a road. The time interval is set to 20 minutes and the time period used of BRF is from November 2015 to May 2016. 
- BRF-L. Similar to BRF, the traffic flow of each road is recoded in the dataset. BRF-L contains 1586 roads and the time interval is set to 10 minutes. The time period used of BRF-L is from November 2015 to December 2015.

#### 相关评价

T-ZS2 有数据，但没论述

### T-74 Traffic Transformer

CCF-NONE CI-277 DT-2020 C-T-SPEED

#### 阅读笔记

##### 模型

we propose to design different strategies for encoding temporal information so thatboth the continuity and periodicity of traffic data can be preserved, and extend Transformer to modeling temporaland spatial dependencies jointly with the help of graph convolutional networks (GCNs)

- We design four novel position encoding strategies to encode the continuity and periodicity of time seriesto facilitate the modeling of temporal dependencies in traffic data
- We introduce a hybrid encoder–decoder architecture, called Traffic Transformer, to coherently model spatial and temporal dependencies of traffic data in an end-to-end training manner, where Transformer is leveraged to model temporal dependencies and GCNs contribute to the modeling of spatial dependencies

![image-20240818235942477](img/image-20240818235942477.png)

![image-20240818235952372](img/image-20240818235952372.png)

##### 数据

METR-LA PEMS-BAY



### T-141 STGM

CCF-C CI-24 DT-2023 C-T-SPEED

#### 阅读笔记

##### 模型

时空依赖：注意力

空间：GNN

时间：CNN

we propose a Spatio-Temporal Graph Mixformer (STGM) network, a highly optimized model with low memory footprint. We address the aforementioned limits by utilizing a novel attention mechanism to capture the correlation between temporal and spatial dependencies. Specifically, we use convolution layers with a variable fields of view for each head to capture long–short term temporal dependency. Additionally, we train an estimator model that express the contribution of a node over the desired prediction. The estimation is fed alongside a distance matrix to the attention mechanism. Meanwhile, we use a gated mechanism and a mixer layer to further select and incorporate the different perspectives.

- We suggest a similarity estimator model that given a set of nodes with a segment of historical traffic, approximates the expected implication of each node over the future traffic signal. 
- Furthermore, we designed a couple of improvements for the original transformer architecture named STGA that fully utilizes the multi-head attention and seamlessly capture both temporal and spatial dependencies. 
- We propose the CT-Mixer module that works in coordination with STGA to further capture the temporal locality and internode channels information

有一段描述 CNN 代替 RNN 的描述，说不定可以参考

![image-20240819004354922](img/image-20240819004354922.png)

##### 数据

PEMS-BAY METR-LA PEMSD7M(参考T-28)