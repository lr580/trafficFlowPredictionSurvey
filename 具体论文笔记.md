è¯´æ˜ï¼š

- æ‰€æœ‰è®ºæ–‡æŒ‰è‡ªå®šä¹‰ç¼–å· **T-XXX**ï¼Œç¼–å·å¯¹åº”è¡¨å­˜å‚¨åœ¨ Zotero é‡Œï¼Œè‡ªè¡ŒæŸ¥é˜…ã€‚æ”¶å½•å…·ä½“å®ç°è®ºæ–‡ï¼Œä¸åŒ…æ‹¬ç»¼è¿°ã€‚ç»¼è¿°åœ¨ `å†…å®¹æ•´ç†.md` ä¸»è¦è®°å½•ã€‚

- å¼•ç”¨é‡æˆªæ­¢æŸ¥è¯¢æ—¶é—´(æŒ‰ 2024 æš‘å‡ç®—)ï¼Œå‚è€ƒè°·æ­Œå­¦æœ¯ï¼ŒæŒ‰ **CI-xxx** è¡¨ç¤ºå¼•ç”¨é‡ã€‚(CI: cite)

- CCF è¯„çº§æŒ‰ç…§ **CCF-xxx** æ¥ã€‚

- å¹´ä»½/æ—¥æœŸæŒ‰ç…§ **DT-yyyy** æ¥ï¼Œæˆ– DT-yyyy-mmã€‚(DT: datetime)

- è®ºæ–‡ç®€ç§°å†™åœ¨æ ‡é¢˜

- æŒ‰ç…§ **C-XXX** ç»™è®ºæ–‡æ‰“æ ‡ç­¾ã€‚(C: category)

  > å…·ä½“æ ‡ç­¾æœ‰ï¼š-T (target) é¢„æµ‹ç›®æ ‡(é€Ÿåº¦ï¼Œæµé‡ç­‰)

# å…·ä½“è®ºæ–‡

ç«çˆ†/ç»å…¸çš„æ ‡å‡†ï¼šå¼•ç”¨é‡ç‰¹åˆ«å¤šæˆ–è¡ç”Ÿæ¨¡å‹ç‰¹åˆ«å¤šï¼Œå…¶ä»–æ˜¯~~æ‚é±¼~~æ–°èµ·

## ç»å…¸

### T-28 STGCN

CCF-A CI-3886 DT-2017 C-T-SPEED

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

ä¸¤ç§ CNN åˆ†åˆ«å¤„ç†æ—¶é—´å’Œç©ºé—´ç‰¹å¾

ç»„æˆï¼šä¸¤ä¸ªæ—¶ç©ºå·ç§¯å—å’Œå…¨è¿æ¥å±‚è¾“å‡ºï¼Œæ¯ä¸ªå—ç”±ä¸¤ä¸ªæ—¶é—´å·ç§¯å¤¹ç€ä¸€ä¸ªç©ºé—´å·ç§¯ï¼Œä½¿ç”¨äº†æ®‹å·®è¿æ¥å’Œç“¶é¢ˆç­–ç•¥

> (GPT) ç“¶é¢ˆç­–ç•¥çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å¼•å…¥æŸç§å½¢å¼çš„é™åˆ¶ï¼ˆå¦‚ç»´åº¦å‹ç¼©ã€è®¡ç®—èµ„æºé™åˆ¶ç­‰ï¼‰ï¼Œå¼ºåˆ¶æ¨¡å‹åšå‡ºé€‰æ‹©ï¼Œèšç„¦äºæœ€é‡è¦çš„ä¿¡æ¯ã€‚è¿™åœ¨æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€ç¨³å®šæ€§ä»¥åŠå‡å°‘è®¡ç®—èµ„æºçš„ä½¿ç”¨ç­‰æ–¹é¢ï¼Œéƒ½å…·æœ‰é‡è¦ä½œç”¨ã€‚

![image-20240815222203379](img/image-20240815222203379.png)

å›¾2è¯´æ˜ï¼š

Architecture of spatio-temporal graph convolutional networks. The framework STGCN consists of two spatio-temporal convolutional blocks (ST-Conv blocks) and a fully-connected output layer in the end. Each ST-Conv block contains two temporal gated convolution layers and one spatial graph convolution layer in the middle. The residual connection and bottleneck strategy are applied inside each block.

è¶…å‚æ•°é€‰æ‹©çš„æ–¹æ³•ï¼šTree-structured Parzen Estimator (TPE) (æœ‰è®ºæ–‡)

##### æ•°æ®

ç ”ç©¶äº¤é€šé€Ÿåº¦

æ•°æ®é›†æ˜¯

è‡ªå»ºï¼šBJER4

- gathered from the major areas of east ring No.4 routes in Beijing City by double-loop detectors. There are 12 roads selected for our experiment. The traffic data are aggregated every 5 minutes. The time period used is from 1st July to 31st August, 2014 except the weekends. We select the first month of historical speed records as training set, and the rest serves as validation and test set respectively

æ ‡å‡†ï¼šPeMSD7

- ä½¿ç”¨åŠ æƒçŸ©é˜µï¼Œè·ç¦»çš„åˆ†ç±»è®¨è®ºå‡½æ•°æ˜¯è¾¹æƒ

- æ•°æ®æè¿°ï¼š(å‚è€ƒå®˜æ–¹ä»£ç ä»“åº“ README.md)

  æ¯å¤©çš„è®°å½•æ¬¡æ•°ä¹˜ä»¥å¤©æ•°æ˜¯è¡Œï¼Œé“è·¯æ•°æ˜¯åˆ—ï¼Œå†…å®¹æ˜¯é€Ÿåº¦è®°å½•ï¼ŒèŠ‚ç‚¹æ•°æ˜¯ 228 å’Œ 1026ï¼Œè¡Œæ•°æ˜¯ 12672(æˆ‘ç®—å‡ºæ¥44å¤©)ï¼Œå€¼æ˜¯ 60 å¤šå’Œ 70 å¤šçš„å å¤§å¤´ï¼Œæœ‰æå°‘å…¶ä»–å€¼ï¼Œé¢„æµ‹å–äº† 15/30/45minï¼Œé¢„æµ‹è§†é‡è¶Šå¤§è¯¯å·®è¶Šå¤§

  - PeMSD7_V_{`$num_route`}.csv : Historical Speed Records with shape of `[len_seq * num_road] (len_seq = day_slot * num_dates)`.
  - PeMSD7_W_{`$num_route`}.csv : Weighted Adjacency Matrix with shape of `[num_road * num_road]`.

è®ºæ–‡é‡Œæœ‰å›¾è¡¨å±•ç¤ºäº†æœ‰ä¸€ä¸ªUå½¢å’ŒåŒUå½¢æ•°æ®(è“è‰²æ˜¯çœŸå®)

é™„å½•æœ‰å¾ˆå¤šå¼ å›¾è¡¨ç¤ºå®éªŒç»“æœ

æŸ¥é˜…å¾—çŸ¥ï¼Œå…¶ä¸­çš„ FC-LSTM è®ºæ–‡å‡ºå¤„æ ¹æœ¬å°±ä¸æ˜¯è¿™ä¸ªé¢†åŸŸçš„ï¼Œè€Œæ˜¯è¯¥æŠ€æœ¯çš„ç»å…¸è®ºæ–‡æˆ–è¿ç§»åº”ç”¨ï¼Œæ€€ç–‘æ˜¯è‡ªå»ºæ¨¡å‹

##### ä»£ç 

[src](https://github.com/VeritasYin/STGCN_IJCAI-18/tree/master)

#### ç›¸å…³è¯„ä»·

T-ZS2 æ€»ç»“ï¼š

- å †å å¤šä¸ªæ—¶ç©ºå·ç§¯å—ï¼Œæ¯ä¸ªå—è¿æ¥ä¸¤ä¸ªæ—¶é—´å·ç§¯å±‚å’Œä¸€ä¸ªå›¾å·ç§¯å±‚

  > stacks multiple spatio-temporal convolution blocks and each block concatenate two temporal convolution and one graph convolution layer.  

- ä½¿ç”¨ ChebNet ä½œä¸ºå›¾å·ç§¯æ“ä½œï¼Œç”¨ä¸€é˜¶è¿‘ä¼¼æ¯”è¾ƒ

  > ChebNet is chosen as the graph convolution operator in STGCN, after a comparison with its first-order approximation  

- CNN ä»£æ›¿ RNN è·å–æ—¶é—´åŠ å¿«äº†è®­ç»ƒæ—¶é—´

  > The usage of temporal convolution layers instead of RNNs for temporal modeling accelerates the training phase of STGCN  

#### ç›¸å…³æ”¹ç‰ˆ

- T-128 (æ”¹æŸå¤±å‡½æ•°)

- T-81

### T-40 DCRNN

CCF-NONE CI-3654 DT-2018 C-T-SPEED

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

ç©ºé—´ï¼šå›¾åŒå‘éšæœºæ¸¸èµ°

æ—¶é—´ï¼šencoder-decoder

DCRNN captures the spatial dependency using bidirectional random walks on the graph, and the temporal dependency using the encoder-decoder architecture with scheduled sampling

- ç©ºé—´

  We model the spatial dependency by relating traffic flow to a diffusion process, which explicitly captures the stochastic nature of traffic dynamics. This diffusion process is characterized by a random walk on G with restart probability, and a state transition matrix

- æ—¶é—´

  We replace the matrix multiplications in GRU with the diffusion convolution

  In multiple step ahead forecasting, Both the encoder and the decoder are recurrent neural networks with Diffusion Convolutional Gated Recurrent Unit (DCGRU)

##### æ•°æ®

ç‰¹å¾æè¿°

![image-20240817003909585](img/image-20240817003909585.png)

Figure 1: Spatial correlation is dominated by road network structure. (1) Traffic speed in road 1 are similar to road 2 as they locate in the same highway. (2) Road 1 and road 3 locate in the opposite directions of the highway. Though close to each other in the Euclidean space, their road network distance is large, and their traffic speeds differ significantly.

æ•°æ®é›†ï¼šäº¤é€šé€Ÿåº¦é¢„æµ‹

- METR-LA æ„Ÿè§‰æ˜¯å®Œå…¨æ•°æ®é›†
- PEMS-BAY è¯¥æ•°æ®é›†çš„å¼€å±±

#### ç›¸å…³è¯„ä»·

(T-ZS1)

encoder-decoder å›¾æ‰©æ•£æ¯” FNN, LSTM å¥½ï¼Œæ¶ˆèè¯æ˜æ‰©æ•£å·ç§¯æ›´å¥½

have demonstrated that their encoder-decoder model with graph diffusion managed to outperform simple FNN and LSTM. Not only that, they also performed an ablation test to demonstrate that their novel diffusion convolution module manages to outperform simpler variations 

GRU: çŸ©é˜µä¹˜æ³•æ”¹ä¸ºæ‰©æ•£å·ç§¯æ“ä½œ

replaced the matrix multiplication inside Gated Recurrent Unit with the diffusion convolution operation 



(T-ZS2)

æœ€å‡ºåçš„ä¸€ä¸ªæ˜¯ DCRNN

> (T-ZS2) The most famous one is diffusion convolutional recurrent neural network (DCRNN)  

- å›¾å·ç§¯ç½‘ç»œå’Œ RNN å­¦ä¹ æ—¶ç©º

  > which uses diffusion graph convolutional networks and RNN to learn the representations of spatial dependencies and temporal relations  

- æœ€åˆç”¨äºäº¤é€šé€Ÿåº¦é¢„æµ‹ï¼Œç°åœ¨æˆä¸ºäº†åŸºå‡†æ¨¡å‹

  > DCRNN was originally proposed for traffic speed forecasting and is now widely used as a baseline.  

- å»ºå›¾æ–¹å¼ï¼šè·ç¦»çŸ©é˜µ

  > To create the traffic graph, the adjacency matrix is defined as the thresholded pairwise road network distances.  

- æ”¯æŒæœ‰å‘å›¾ï¼Œå¼•å…¥æ‰©æ•£å·ç§¯(DC)

  > Compared with other graph convolutional models that can only operate on undirected graphs, e.g., ChebNet, DCRNN introduces the diffusion convolution (DC) operation for directed graph and is more suitable for transportation scenarios, which is defined as follows:   

  $$
  \mathbf X_{*DC}=\sum_{k=0}^{K-1}(\theta_{k,1}(D_O^{-1}A)^k+\theta_{k,2}(D_I^{-1}A^T))\mathbf X
  $$

  å…¶ä¸­ $\mathbf X\in R^{N\times d}$ æ˜¯èŠ‚ç‚¹ç‰¹å¾çŸ©é˜µï¼Œ$A$ æ˜¯é‚»æ¥çŸ©é˜µï¼Œ$D_O,D_I$ æ˜¯å¯¹è§’çŸ©é˜µï¼šå‡ºåº¦å’Œå…¥åº¦ï¼Œ$\theta$ æ˜¯ä¸¤ä¸ªæ¨¡å‹å‚æ•°ï¼Œ$K$ æ˜¯æ‰©æ•£æ­¥æ•°ã€‚

  åŒºåˆ†å…¥åº¦å’Œå‡ºåº¦å®ç°äº†æœ‰å‘å›¾ 

  > where X 2 RNÃ—d is the node feature matrix, A is the adjacency matrix, DO and DI are diagonal out-degree and in-degree matrices, Î¸k;1 and Î¸k;2 are model parameters, K is the number of diffusion steps. By defining and using out-degree and in-degree matrices, DCRNN models the bidirectional diffusion process to capture the influence of both upstream and downstream traffic  

- å¯¹æ— å‘å›¾ä¸æ˜¯å¾ˆå®ç”¨

  > While DCRNN is a strong baseline, it is not suitable or desirable for the undirected graph cases  

- æ‰©å±•ç‰ˆæœ¬ T-88ï¼šç»Ÿä¸€æ„å»º RNNï¼ŒåŸºäºä»»æ„å›¾å·ç§¯

  Then DCRNN is extended with a stronger learning ability in graph GRU a unified method for constructing an RNN based on an arbitrary graph convolution operator is proposed, instead of the single RNN model used in DCRNN

è§‚å¯Ÿå›¾æ‰©æ•£è¿‡ç¨‹å’Œé‚»èŠ‚ç‚¹å…³è”

observed the traffic diffusion along the road network and the correlation between several adjacent traffic sensors

### T-64 Graph WaveNet

CCF-A CI-1984 DT-2019 C-T-SPEED

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

ç©ºé—´ï¼šè‡ªé€‚åº”ä¾èµ–çŸ©é˜µï¼Œç‚¹åµŒå…¥å­¦ä¹ ï¼Œå›¾å·ç§¯

æ—¶é—´ï¼šå †å æ‰©å¼ (å¦‚å›¾2ï¼Œå€å¢) 1D CNN ä»»æ„å·ç§¯

By developing a novel adaptive dependency matrix and learn it through node embedding, our model can precisely capture the hidden spatial dependency in the data. 

With a stacked dilated 1D convolution component whose receptive field grows exponentially as the number of layers increases, Graph WaveNet is able to handle very long sequences

- We propose a graph convolution layer in which a self-adaptive adjacency matrix can be learned from the data through an end-to-end supervised training
- we adopt stacked dilated casual convolutions to capture temporal dependencies. The receptive field size of stacked dilated casual convolution networks grows exponentially with an increase in the number of hidden layers

> (GPT) â€œend-to-endâ€ï¼ˆç«¯åˆ°ç«¯ï¼‰æŒ‡çš„æ˜¯ä¸€ç§æ¨¡å‹è®­ç»ƒæ–¹å¼ï¼Œåœ¨è¿™ç§æ–¹å¼ä¸­ï¼Œæ¨¡å‹ä»è¾“å…¥çš„åŸå§‹æ•°æ®å¼€å§‹ï¼Œç›´æ¥å­¦ä¹ åˆ°æœ€ç»ˆè¾“å‡ºçš„ç›®æ ‡ç»“æœï¼Œä¸­é—´çš„æ‰€æœ‰æ­¥éª¤éƒ½æ˜¯åœ¨åŒä¸€ä¸ªæ¨¡å‹ä¸­å®Œæˆçš„

å›¾3ï¼š

![image-20240817224043300](img/image-20240817224043300.png)

##### æ•°æ®

METR-LA PEMS-BAY

åšäº†æ—¶é—´æ•ˆç‡å¯¹æ¯”ï¼Œçœ‹æ ·å­åšäº†æ¶ˆèå®éªŒ

é™æ€å›¾å¯èƒ½ä¸èƒ½ååº”çœŸå®ä¾èµ–

However, the explicit graph structure (relation) does not necessarily reflect the true dependency and genuine relation may be missing due to the incomplete connections in the data.

é™æ€çš„å±€é™æ€§çš„ä¾‹å­

To give each circumstance an example, let us consider a recommendation system. In the first case, two users are connected, but they may have distinct preferences over products. In the second case, two users may share a similar preference

ç©ºé—´å½±å“çš„ç›´è§‚å›¾ç¤º

<img src="img/image-20240817220918943.png" alt="image-20240817220918943" style="zoom:67%;" />



#### ç›¸å…³è¯„ä»·

- è‡ªé€‚åº”çŸ©é˜µè‡ªåŠ¨å‘ç°éšè—å›¾ç»“æ„ï¼Œä»»æ„å·ç§¯å­¦ä¹ æ—¶é—´å…³è”

  > constructs a self-adaptive matrix to uncover unseen graph structures automatically from the data and WaveNet, which is based on causal convolutions, is used to learn temporal relations.  

- è®­ç»ƒåçŸ©é˜µå›ºå®šï¼Œä¸èƒ½é€‚åº”åŠ¨æ€çš„æ•°æ®

  > However, the self-adaptive matrix in Graph WaveNet is fixed after training, which is unable to be adjusted dynamically with the data characteristics  

#### ç›¸å…³æ”¹ç‰ˆ

- T-136 STEP
- T-135 STD-MAE
- T-131 STAWnet
- T-130 å¾®è°ƒ
- T-128 GWNET-Cov (æ”¹æŸå¤±å‡½æ•°)
- T-147 STWave

## ç«çˆ†

### T-110 AGCRN

CCF-A CI-1060 DT-2020 C-T-FLOW

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

ç©ºé—´ï¼šGCN æ”¹è¿›

æ—¶é—´ï¼šGRU

we propose two adaptive modules for enhancing Graph Convolutional Network (GCN) with new capabilities: 1) a Node Adaptive Parameter Learning (NAPL) module to capture node-specific patterns; 2) a Data Adaptive Graph Generation (DAGG) module to infer the inter-dependencies among different traffic series automatically. We further propose an Adaptive Graph Convolutional Recurrent Network (AGCRN) to capture fine-grained spatial and temporal correlations in traffic series automatically based on the two modules and recurrent networks.

- a Node Adaptive Parameter Learning (NAPL) module to learn node-specific patterns for each traffic seriesâ€”NAPL factorizes the parameters in traditional GCN and generates node-specific parameters from a weights pool and bias pool shared by all nodes according to the node embedding
- a Data Adaptive Graph Generation (DAGG) module to infer the node embedding (attributes) from data and to generate the graph during training. NAPL and DAGG are independent and can be adapted to existing GCN-based traffic forecasting models both separately and jointly
- we combine NAPL and DAGG with recurrent networks and propose a unified traffic forecasting model - Adaptive Graph Convolutional Recurrent Network (AGCRN). AGCRN can capture fine-grained node-specific spatial and temporal correlations in the traffic series and unify the nodes embeddings in the revised GCNs with the embedding in DAGG

##### æ•°æ®

PEMSD4, PEMSD8

![image-20240819010446183](img/image-20240819010446183.png)

#### ç›¸å…³æ”¹ç‰ˆ

- T-151

### T-142 MTGNN

CCF-A CI-1294 DT-2020 C-T-SPEED

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

ç©ºé—´ï¼šå›¾å·ç§¯

æ—¶é—´ï¼š1D CNN

Our approach automatically extracts the uni-directed relations among variables through a graph learning module, into which external knowledge like variable attributes can be easily integrated. A novel mix-hop propagation layer and a dilated inception layer are further proposed to capture the spatial and temporal dependencies within the time series. The graph learning, graph convolution, and temporal convolution modules are jointly learned in an end-to-end framework.

- we propose a novel graph learning layer, which extracts a sparse graph adjacency matrix adaptively based on data. Furthermore, we develop a graph convolution module to address the spatial dependencies among variables, given the adjacency matrix computed by the graph learning layer. This is designed specifically for directed graphs and avoids the over-smoothing problem that frequently occurs in graph convolutional networks
- Finally, we propose a temporal convolution module to capture temporal patterns by modified 1D convolutions. It can both discover temporal patterns with multiple frequencies and process very long sequences.

![image-20240819012120958](img/image-20240819012120958.png)

##### æ•°æ®

METR-LA PEMS-BAY å’Œå…¶ä»–äº¤é€šå¤–çš„æ•°æ®ï¼Œæœ‰è¾“å…¥é•¿åº¦è¡¨ç¤ºä¸º 12

### T-81 ASTGCN

CCF-A CI-2194 DT-2019 C-T-FLOW

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

ç©ºé—´ï¼šå›¾å·ç§¯+æ³¨æ„åŠ›

æ—¶é—´ï¼šæ³¨æ„åŠ›+CNN

In this paper, we propose a novel attention based spatial-temporal graph convolutional network (ASTGCN) model to solve traffic flow forecasting problem

- ASTGCN mainly consists of three independent components to respectively model three temporal properties of traffic flows, i.e., recent, daily-periodic and weekly-periodic dependencies.

- each component contains two major parts: 

  \1 the spatial-temporal attention mechanism to effectively capture the dynamic spatialtemporal correlations in traffic data; 

  \2 the spatial-temporal convolution which simultaneously employs graph convolutions to capture the spatial patterns and common standard convolutions to describe the temporal features

- The output of the three components are weighted fused to generate the final prediction results

![image-20240819145531119](img/image-20240819145531119.png)

- After the graph convolution operations having captured neighboring information for each node on the graph in the spatial dimension, a standard convolution layer in the temporal dimension is further stacked to update the signal of a node by merging the information at the neighboring time slice

##### æ•°æ®

PEMSD4 PEMSD8 å¤´50å¤©è®­ç»ƒï¼Œ12å¤©æµ‹è¯•ï¼Œè¿›è¡Œäº†æµé‡é€Ÿåº¦å’Œå ç”¨ç‡é¢„æµ‹

There are three kinds of traffic measurements considered in our experiments, including total flow, average speed, and average occupancy

#### ç›¸å…³è¯„ä»·

å˜ç§ ASTGCN å¼•å…¥ä¸¤ä¸ªæ³¨æ„åŠ›å±‚åˆ†åˆ«è·å–æ—¶ç©ºåŠ¨æ€è”ç³»

> Attention based Spatio-temporal graph convolutional network (ASTGCN) further introduces two attention layers in STGCN to capture the dynamic correlations in spatial dimension and temporal dimension, respectively  

#### ç›¸å…³æ”¹ç‰ˆ

- T-151

### T-36 T-GCN

CCF-B CI-2248 DT-2019 C-T-SPEED

#### é˜…è¯»ç¬”è®°

##### æ•°æ®

SZ-taxi and Los-loop datasets.

> (1) SZ-taxi. This dataset was the taxi trajectory of Shenzhen from Jan. 1 to Jan. 31, 2015. We selected 156 major roads of Luohu District as the study area. The experimental data mainly includes two parts. One is an 156*156 adjacency matrix, which describes the spatial relationship between roads. Each row represents one road and the values in the matrix represent the connectivity between the roads. Another one is a feature matrix, which describes the speed changes over time on each road. Each row represents one road; each column is the traffic speed on the roads in different time periods. We aggregate the traffic speed on each road every 15 minutes. 
>
> (2) Los-loop. This dataset was collected in the highway of Los Angeles County in real time by loop detectors. We selected 207 sensors and its traffic speed from Mar.1 to Mar.7, 2012. We aggregated the traffic speed every 5 minutes. Similarity, the data concludes an adjacency matrix and a feature matrix. The adjacency matrix is calculated by the distance between sensors in the traffic networks. Since the Los-loop dataset contained some missing data, we used the linear interpolation method to fill missing values. 
>
> In the experiments,the input data was normalized to the interval [0,1]. In addition, 80% of the data was used as the training set and the remaining 20% was used as the testing set. We predicted the traffic speed of the next 15 minutes, 30 minutes, 45 minutes and 60 minutes.

#### ç›¸å…³è¯„ä»·

GCN(ç©ºé—´) + GRU(æ—¶é—´)

used a Gated Recurrent Unit which takes input from a Graph Convolution Network and outputs the predicted traffic

### T-143 GMAN

CCF-A CI-1262 DT-2019

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

encoder-decoder æ—¶ç©ºéƒ½ç”¨æ³¨æ„åŠ›

In this paper, we focus on the spatio-temporal factors, and propose a graph multi-attention network (GMAN) to predict traffic conditions for time steps ahead at different locations on a road network graph. GMAN adapts an encoder-decoder architecture, where both the encoder and the decoder consist of multiple spatio-temporal attention blocks to model the impact of the spatio-temporal factors on traffic conditions. The encoder encodes the input traffic features and the decoder predicts the output sequence. Between the encoder and the decoder, a transform attention layer is applied to convert the encoded traffic features to generate the sequence representations of future time steps as the input of the decoder. The transform attention mechanism models the direct relationships between historical and future time steps that helps to alleviate the error propagation problem among prediction time steps

![image-20240819013039961](img/image-20240819013039961.png)

##### æ•°æ®

traffic volume prediction on the Xiamen dataset

which contains 5 months of data recorded by 95 traffic sensors ranging from August 1st, 2015 to December 31st, 2015 in Xiamen, China

ä»¥åŠ PEMS-BAY

![image-20240819013254073](img/image-20240819013254073.png)

### T-87 STSGCN

CCF-A CI-1038 DT-2020 C-T-FLOW

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

æ—¶ç©ºè”ç³»å¯ä»¥è·å–ï¼Œè®¤ä¸ºæ˜¯ GNN+CNN è§£å†³çš„ã€‚

Spatial-Temporal Synchronous Graph Convolutional Networks (STSGCN)

- he STSGCN model can simultaneously capture the localized spatial-temporal correlations directly, instead of using different types of deep neural networks to model the spatial dependencies and temporal correlations separately. Specifically, we construct localized spatial-temporal graphs which connect individual spatial graphs of adjacent time steps into one graph
- Then we construct a Spatial-Temporal Synchronous Graph Convolutional Module (STSGCM) to capture the complex localized spatial-temporal correlations in these localized spatial-temporal graphs
- Meanwhile, to capture the heterogeneity in long-range spatial-temporal network data, we design a Spatial-Temporal Synchronous Graph Convolutional Layer (STSGCL), which deploys multiple individual STSGCMs on different time periods. Finally, we stack multiple STSGCLs to aggregate long-range spatial-temporal correlations and heterogeneity for prediction.

![image-20240819154032938](img/image-20240819154032938.png)

![image-20240819154119662](img/image-20240819154119662.png)

##### æ•°æ®

PEMS03/4/7/8

#### ç›¸å…³è¯„ä»·

T-ZS2 é‚»æ¥çŸ©é˜µ+å±€éƒ¨æ—¶ç©ºå›¾çš„ GCN

For example, the localized spatio-temporal correlation information is extracted simultaneously with the adjacency matrix of localized spatio-temporal graph, in which a localized spatio-temporal graph that includes both temporal and spatial attributes is constructed first and a spatial-based GCN method is applied then

### T-146 STFGNN

CCF-A CI-641 DT-2021 C-T-FLOW

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

æ€æƒ³ï¼šæŠŠæ—¶é—´ä¾èµ–ä¹Ÿåšæˆå›¾ï¼Œè§£å†³æ—¶ç©ºä¾èµ–

æ—¶é—´ï¼šCNN (Gated dilated CNN)

ç©ºé—´ï¼šGNN å›¾å·ç§¯

To overcome those limitations, our paper proposes a novel Spatial-Temporal Fusion Graph Neural Networks (STFGNN) for traffic flow forecasting

- First, a data-driven method of generating â€œtemporal graphâ€ is proposed to compensate several existing correlations that spatial graph may not reflect
- SFTGNN could effectively learn hidden spatial-temporal dependencies by a novel fusion operation of various spatial and temporal graphs, treated for different time periods in parallel
- Meanwhile, by integrating this fusion graph module and a novel gated convolution module into a unified layer, SFTGNN could handle long sequences by learning more spatial-temporal dependencies with layers stacked

<img src="img/image-20240819234537669.png" alt="image-20240819234537669" style="zoom: 67%;" />

we propose a novel data-driven method for graph construction: the temporal graph learned based on similarities between time series. Then several graphs could be integrated as a spatial-temporal fusion graph to obtain hidden spatial-temporal dependencies. Moreover, to break the local and global correlation tradeoff, gated dilated convolution module is introduced, whose larger dilation rate could capture long-range dependencies

- We construct a novel graph by a data-driven method, which preserve hidden spatial-temporal dependencies. This data-driven adjacency matrix is able to extract correlations that given spatial graph may not present. Then, we propose a novel spatial-temporal fusion graph module to capture spatial-temporal dependencies synchronously.
- We propose an effective framework to capture local and global correlations simultaneously, by assembling a Gated dilated CNN module with spatial-temporal fusion graph module in parallel. Long-range spatial-temporal dependencies could also be extracted with layers stacked

![image-20240819235730314](img/image-20240819235730314.png)

Figure 3: Detailed framework of STFGNN. (a) is the example of input of Spatial-Temporal Fusion Graph, which would be generated iteratively along the time axis. (b) is the example of Spatial-Temporal Fusion Graph, whose size K is 4 and 3, respectively. It consists of three kinds of adjacency matrix âˆˆ N Ã— N : spatial graph ASG, temporal graph AT G and temporal connectivity graph AT C . The AT C within a red circle would be taken for instance in the body. (c) is overall structure of STFGNN, its Gated CNN module and STFGNN modules are in parallel. (d) is detailed architecture of the Spatial-Temporal Fusion Graph Modules, each module will be independently trained for input iteratively generated from (a) in parallel as well

##### æ•°æ®

12 æ­¥é¢„æµ‹ 12 æ­¥ï¼ŒPEMS03/4/7/8

### T-85 ST-UNet

CCF-NONE CI-70 DT-2019 C-T-SPEED

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

ç©ºé—´ï¼šGNN

æ—¶é—´ï¼šRNN(å¤§æ¦‚)

To tackle this problem, we design a novel multi-scale architecture, Spatio-Temporal U-Net (ST-UNet), for graph-structured time series modeling. In this U-shaped network, a paired sampling operation is proposed in spacetime domain accordingly: the pooling (ST-Pool) coarsens the input graph in spatial from its deterministic partition while abstracts multi-resolution temporal dependencies through dilated recurrent skip connections; based on previous settings in the downsampling

- ST-UNet employs multi-granularity graph convolution for extracting both generalized and localized spatial features, 

  and adds dilated recurrent skip-connections for capturing multi-resolution temporal dependencies

ç«çˆ†æ˜¯å› ä¸ºåé¢å¾ˆå¤šæ¨¡å‹è²Œä¼¼éƒ½ç”¨å®ƒä½œåŸºå‡†æµ‹è¯•

##### æ•°æ®

METR-LA PEMS M/L

### T-41 hybrid Seq2Seq

CCF-A CI-235 DT-2018 C-T-SPEED

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

ç©ºé—´ï¼šå›¾å·ç§¯

æ—¶é—´ï¼šç–‘ä¼¼ LSTM

encoder-decoder

In this paper, we intend to improve traffic prediction by appropriate integration of three kinds of implicit but essential factors encoded in auxiliary information. We do this within an encoder-decoder sequence learning framework

- a graph convolution neural network is used to learn the spatial correlation of road segments

##### æ•°æ®

è‡ªåˆ›

we release a large-scale traffic dataset from Baidu Map, the Q-Traffic dataset, which provides various offline and online auxiliary information along with traffic speed data. There are three kinds of auxiliary domains in the Q-Traffic dataset: 1) Offline geographical and social attributes which include public holidays, peak-hour, speed etc; 2) the road intersection information such as local road network and junctions; and 3) online crowd queries which record map search queries from users.

Road segments 15,073 

Total length 738.91 km 

Interval 15 minutes 

Time April 1, 2017 - May 31, 2017 

Total records 265,967,808 

lon/lat bounding box (116.10, 39.69, 116.71, 40.18)

åˆ—è¡¨å¯¹æ¯”äº†å…¶ä»–æ•°æ®é›†ï¼Œå¯ä»¥å‚è€ƒè¿˜æœ‰ä»€ä¹ˆåˆ«çš„æ•°æ®é›†

## æ–°èµ·

### T-128 GWNet-Cov

CCF-A CI-18 DT-2021 C-T-SPEED C-T-FLOW

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

å¼•å…¥æ–°çš„æŸå¤±å‡½æ•°

We introduce a novel loss function, Covariance Loss, which is conceptually equivalent to conditional neural processes and has a form of regularization so that is applicable to many kinds of neural networks. With the proposed loss, mappings from input variables to target variables are highly affected by dependencies of target variables as well as mean activation and mean dependencies of input and target variables. This nature enables the resulting neural networks to become more robust to noisy observations and recapture missing dependencies from prior information.

ä½¿ç”¨äº† STGCN å’Œ Graph WaveNet åšæ”¹è¿›

To demonstrate the validity of the proposed Covariance Loss, we employ neural networks that are designed to explicitly consider the spatial and temporal dependencies, namely, spatio-temporal graph convolutional network (STGCN) and graph wavenet (GWNET)

##### æ•°æ®

æŠŠ STGCN ç”¨åˆ°äº† PEMSD7Mï¼Œè¿‡å» 60 min æ•°æ®é¢„æµ‹æœªæ¥ 15-60min æ•°æ®

æŠŠ GWNET ç”¨åˆ°äº† METR-LA å’Œ PEMS-BAYï¼Œåè€…è¿‡å» 60 é¢„æµ‹æœªæ¥ 60

#### ç›¸å…³è¯„ä»·

### T-129 DCGCN

CCF-NONE CI-24 DT-2023 C-T-SPEED

#### é˜…è¯»ç¬”è®°

æˆ‘è§‰å¾—ä½œç”¨ä¸å¤§ï¼Œæ”¾è¡¨æ ¼é‡Œåº”è¯¥å°±è¡Œ

##### æ¨¡å‹

åŠ¨æ€è´å¶æ–¯ç½‘ç»œï¼Œå›¾å·ç§¯ç½‘ç»œï¼ŒåŠ¨æ€ä»»æ„å›¾ (GCN+RNN)

In this work, we propose a novel approach for traffic prediction that embeds time-varying dynamic Bayesian network to capture the fine spatiotemporal topology of traffic data. 

We then use graph convolutional networks to generate traffic forecasts. 

To enable our method to efficiently model nonlinear traffic propagation patterns, we develop a deep learning-based module as a hyper-network to generate stepwise dynamic causal graphs

- we propose a novel causal-embedded approach for traffic prediction. It represents the spatiotemporal traffic network topology using a time-varying DBN (TVDBN), which is designed to adapt to the time-varying traffic propagation patterns by learning DBNs step by step
- The learned TVDBN is able to summarize the dynamic spatiotemporal dependencies between nodes. Built upon it, graph convolution is applied to capture spatial dependencies for traffic prediction.
- We propose a complete deep learning based causal structure learning module that serves as a pretrained hyper-network to generate the graphs of the TVDBN.
- The predefined distance graph is further incorporated into graph generator and traffic prediction module as additional prior information to improve the performance.
- We propose an approach based on GCN and RNN to learn a TVDBN that describes the time-varying causal relationships between different locations in traffic network. The augmented Lagrange method is applied for model training to ensure the acyclicity of the TVDBN



> poe assistant æ¦‚æ‹¬ï¼š
>
> **æ—¶ç©ºç‰¹å¾å¤„ç†ï¼š**
>
> - **åŠ¨æ€è´å¶æ–¯ç½‘ç»œ (DBN)ï¼š** åŸæ–‡ä¸­æåˆ° "We propose an approach based on GCN and RNN to learn a TVDBN that describes the time-varying causal relationships between different locations in traffic network." ä»¥åŠ "The learned TVDBN is able to summarize the dynamic spatiotemporal dependencies between nodes."
> - **é€’å½’ç¥ç»ç½‘ç»œ (RNN)ï¼š** åŸæ–‡æåˆ° "We propose to capture the time-varying causal structure in traffic series by RNN, which generates the dynamic causal graphs stepwise from its hidden state." ä»¥åŠ "The challenge is to construct suitable input features for RNN that contain information about the dynamics of the causal structure."
> - **å›¾å·ç§¯ç½‘ç»œ (GCN)ï¼š** åŸæ–‡æåˆ° "We extend the linear SEM (6) to a nonlinear version by spatial-based graph convolution [39], i.e., Xt = âˆ‘Kk=0GCONV2L(Xtâˆ’k, Bkt) + Zt, where GCONV2L is a L layers spatial-based graph convolution operation with skip connections defined as:" ä»¥åŠ "The proposed model is evaluated on a real-world traffic dataset to demonstrate the effectiveness of the proposed approach."
>
> **ç©ºé—´ç‰¹å¾å¤„ç†ï¼š**
>
> - **å›¾å·ç§¯ç½‘ç»œ (GCN)ï¼š** åŸæ–‡æåˆ° "We use an attention mechanism with multi-head scaled dot-product [38], to describe the spatiotemporal correlations between nodes, i.e., sdot(Q, K) = (QWQ)(KWK)âŠ¤âˆšd âˆˆ RNÃ—N, msdot(Q, K) = [sdot1(Q, K), ...,sdoth(Q, K)] âˆˆ RNÃ—NÃ—h, CÌƒ0t = msdot(Xt, Xt), CÌƒkt = msdot(Xt, Xtâˆ’k), k = 1, ..., K, where WQ and WK are learnable parameters of linear transformation, h is the number of heads and the learnable parameters of each head are different." ä»¥åŠ "We further flatten the tensors {CÌƒkt}Kk=0 to matrices in RN2Ã—h, denoted by {Ckt}Kk=0, such that each row of Ckt captures the dynamics of an element in Bkt."
> - **å…ˆéªŒå›¾ç»“æ„ï¼š** åŸæ–‡æåˆ° "The predefined distance graph is further incorporated into graph generator and traffic prediction module as additional prior information to improve the performance." ä»¥åŠ "We conduct experiments on a widely used traffic benchmark dataset METR-LA to evaluate the traffic prediction performance of the proposed model."
>
> æ‚¨æ˜¯å¯¹çš„ï¼Œæ–‡ç« ä¸­æ²¡æœ‰æ˜ç¡®çš„â€œæ—¶é—´ç‰¹å¾â€å¤„ç†éƒ¨åˆ†



> Casual discovery ç›¸å…³ï¼š
>
> Casual discovery aims to analyze causal relationships behind statistical correlations of different variables and facilitate better machine learning.
>
> - Typical approaches to incorporate causal discovery include encoding features from domain-specific causal models as input to downstream tasks
> - and learning the structure of causal relationships between features for graph-based models
> - As a powerful graph-based tool for modeling directed causal relationships between variables, Bayesian network (BN) is being applied in traffic prediction
>
> æ–‡å¿ƒä¸€è¨€ï¼šCasual Discoveryï¼ˆå› æœå‘ç°ï¼‰æ˜¯å› æœæ¨ç†é¢†åŸŸä¸­çš„ä¸€ä¸ªé‡è¦ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨ä»è§‚æµ‹æ•°æ®ä¸­è¯†åˆ«å’Œç†è§£å˜é‡ä¹‹é—´çš„æ½œåœ¨å› æœç»“æ„æˆ–å…³ç³»ã€‚

#### æ•°æ®

METR-LA

A limitation of current casual-embedded traffic prediction models is the assumption of stationary temporal dependencies. However, in reality, the dependencies of traffic data in different places do change over time

### T-131 STAWnet

CCF-NONE CI-93 DT-2021 C-T-SPEED

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

æ—¶é—´ï¼šCNN (Gated TCN) gated temporal convolution network

ç©ºé—´ï¼šè‡ªæ³¨æ„åŠ›ç½‘ç»œ (DAN) dynamic attention network

a multi-step prediction model named Spatial-Temporal Attention Wavenet (STAWnet) is proposed. Temporal convolution is applied to handle long time sequences, and the dynamic spatial dependencies between different nodes can be captured using the self-attention network. Different from existing models, STAWnet does not need prior knowledge of the graph by developing a self-learned node embedding. These components are integrated into an end-to-end framework. 

- self-adaptive node embedding : capture the hidden spatial relationship in the data without knowing the graph structure information

##### æ•°æ®

The experimental results on three public traffic prediction datasets (METR-LA, PEMS-BAY, and PEMS07) demonstrate effectiveness.

å®éªŒå¯¹æ¯”æåˆ° T-GCN, FC-LSTMï¼Œæ³¨æ„åˆ°è¿™ä¿©ç»å¸¸å‡ºç°ç‰¹åˆ«æ˜¯ FC-LSTM

### T-132 ADN

CCF-NONE CI-6 DT-2022

#### é˜…è¯»ç¬”è®°

æ³¨æ„åŠ›æ—¶é—´ï¼Œæ³¨æ„åŠ›ç©ºé—´

we propose the simplest possible model, called ADN for Attention Diffusion Network, which does not rely on any structural prior whatsoever. We choose an attention based encoder-decoder architecture, where attention is adapted to the bi-dimensionality of events as location-instant pairs

> poe æ‰¾çš„ï¼š
>
> - ä½¿ç”¨ å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ (MHA)
>
>   The model alternates attention in the temporal dimension (MHA(T)), and in the spatial dimension (MHA(N)).
>
> - è®ºæ–‡ä¸­æåˆ°æ¨¡å‹ä½¿ç”¨ å¯åˆ†ç¦»æ³¨æ„åŠ›æœºåˆ¶ (separable attention)ï¼Œè¯¥æœºåˆ¶é€šè¿‡åˆ†åˆ«å¤„ç†æ—¶é—´ç»´åº¦å’Œç©ºé—´ç»´åº¦ï¼Œæ¥é™ä½æ³¨æ„åŠ›çŸ©é˜µçš„è®¡ç®—é‡
>
>   Since attention is a generalised form of convolution [1], separability works all the same with attention, where it is also known as axial attention [7]. Separable attention processes event-indexed objects along each dimension of the events (temporal and spatial) separately and alternately, just as spatially separable convolutions alternate processing images along their width and height dimensions.

æ•°æ®ï¼š

Experiments are conducted on three public traffic datasets PEMS-BAY, METR-LA and PEMS07 released by [12] and [20]. The first two are the most commonly used for measuring model performances, and consist of 207 and 325 locations, respectively. We also tested our model on PEMS07 (883 locations), to check its scalability to larger road traffic networks.

### T-133 RGDAN

CCF-B CI-3 DT-2024 C-T-SPEED

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹ 

ç©ºé—´ï¼šGAT(Graph Attention Network, å›¾æ³¨æ„åŠ›ç½‘ç»œ)

æ—¶é—´ï¼šæ³¨æ„åŠ›

çœ‹è®ºæ–‡å›¾è¿˜æœ‰ encoder-decoder

we propose a Random Graph Diffusion Attention Network (RGDAN) for traffic prediction. RGDAN comprises a graph diffusion attention module and a temporal attention module. The graph diffusion attention module can adjust its weights by learning from data like a CNN to capture more realistic spatial dependencies. The temporal attention module captures the temporal correlations

- Random GATs discard traditional attention interaction (e.g., dot product) and instead, use a learnable matrix to discover the correlations between neighbors. This method omits the process of dot-product interaction in ordinary attention, reduces the time complexity, improves the computational speed of the model, and reduces the memory overhead
- Random Graph Diffusion Attention Network (RGDAN), consists of an attention mechanism with a graph diffusion attention module that extracts the spatial correlations. Temporal attention captures the temporal dependencies. The temporal and spatial features are fused by a gated fusion layer. The transformed attention module is used to alleviate the propagation error caused by the difference between the predicted and historical time steps

![image-20240818142743477](img/image-20240818142743477.png)

- A spatio-temporal embedding (STE) generator, which provides preliminary spatio-temporal information to the encoder and decoder by generating spatio-temporal embeddings

##### æ•°æ®

METR-LA PEMS-BAY NE-BJ

- NE-BJ (T-134 æ˜¯é¼»ç¥–)

  This dataset is a collection of public transport speed data taken from the navigation data of Tencent Map. It contains 500 segments, with each road segment data contains 6509 5-min time steps.

##### å…¶ä»–

æä¾›äº†ä¸€ä¸ªè¡¨æ ¼æ€»ç»“äº†ç”¨åˆ°çš„å¯¹æ¯”å®éªŒçš„ç›¸å…³è®ºæ–‡

### T-134 DGCRN

CCF-B CI-298 DT-2021

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

encoder-decoder

ç©ºé—´ï¼šGNN å›¾å·ç§¯

æ—¶é—´ï¼šRNN

we propose a novel traffic prediction framework, named Dynamic Graph Convolutional Recurrent Network (DGCRN)

- In DGCRN, hyper-networks are designed to leverage and extract dynamic characteristics from node attributes, while the parameters of dynamic filters are generated at each time step. We filter the node embeddings and then use them to generate dynamic graph, which is integrated with pre-defined static graph
- We propose a GNN and RNN based model, where the dynamic adjacency matrix is designed to be generated from a hyper-network step by step synchronize with the iteration of RNN. The dynamic graph is incorporated with the pre-defined graph and skip connection to describe the dynamic characteristics of road networks more effectively, enhancing the performance of prediction.

![image-20240819020113960](img/image-20240819020113960.png)

Dynamic Graph Convolutional Recurrent Module (DGCRM)

![image-20240819020343932](img/image-20240819020343932.png)

##### æ•°æ®

METR-LA PEMS-BAY

NE-BJ



### T-135 STD-MAE

CCF-NONE CI-3 DT-2023 C-T-SPEED

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

æ—¶ç©ºéƒ½æ˜¯ AE + è‡ªæ³¨æ„åŠ›

self-supervised pre-training framework SpatialTemporal-Decoupled Masked Pre-training (STDMAE) that employs two decoupled masked autoencoders to reconstruct spatiotemporal series along the spatial and temporal dimensions

mask çš„æ€æƒ³ï¼šå­¦ä¹ å¡«å……ç¼ºå¤±ã€‚åŸºäºè¿™ä¸ªè¿›è¡Œé¢„è®­ç»ƒï¼Œæœ‰è¯¦ç»†ä»‹ç»

- The core idea is to mask parts of the input sequence during pre-training, requiring the model to reconstruct the missing contents
- spatial-temporal-decoupled masking, Such decoupled masking mechanism allows the model to learn representation that can capture clearer heterogeneity
- It consists of a temporal autoencoder (T-MAE) and a spatial autoencoder (S-MAE), both having a similar architecture
- S-MAE applies self-attention along spatial dimension, while T-MAE performs self-attention along temporal dimension

æ‚é¡¹ï¼š

- patch embedding technique: The long input is divided into non-overlapping patches
- Moreover, to simultaneously encode spatial and temporal positional information, we implement a two-dimensional positional encoding

æ¨¡å‹ç»“æ„ï¼š

- The spatial and temporal decoders each consists of a padding layer, a standard transformer layer, and a regression layer.

![image-20240818161239792](img/image-20240818161239792.png)

- å…·ä½“æ¨¡å‹ï¼šGWNet (Graph WaveNet)

##### æ•°æ®

PEMSD3/4/7/8/BAY, METR-LA

### T-136 MegaCRN

CCF-A CI-96 DT-2023 C-T-SPEED

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

ç©ºé—´ GCN( Graph Convolutional Networks (GCNs) )

æ—¶é—´ Gated Recurrent Unit (GRU)

we implement this idea into Meta-Graph Convolutional Recurrent Network (MegaCRN) by plugging the Meta-Graph Learner powered by a MetaNode Bank into GCRN encoder-decoder.

> Graph Structure Learning (GSL)
>
> spatio-temporal graph (STG)

The term meta-graph is coined to describe the generation of node embeddings (similar in adaptive and momentary) for GSL.

our STG learning consists of two steps: (1) querying node-level prototypes from a Meta-Node Bank; (2) reconstructing node embeddings with Hyper-Network

injecting graph convolution operation into recurrent cell (e.g. LSTM). The derived Graph Convolutional Recurrent Unit (GCRU) can thereby simultaneously capture

- spatial dependency, represented by an input graph topology, and
- temporal dependency in a sequential manner.

##### æ•°æ®

METR-LA PEMS-BAY EXPR-KEY (è‡ªå»º)

2021/10/1 - 12/31, 10min æ—¶é—´åŒºé—´ï¼Œ13248 æ—¶é—´æ­¥ï¼Œ1843 æ¡é“è·¯è¿æ¥ï¼Œé€Ÿåº¦

contains the traffic speed information and the corresponding traffic incident information in 10-minute interval for 1843 expressway road links in Tokyo over three months (2021/10âˆ¼2021/12).

##### å…¶ä»–

ç»éªŒå‡†åˆ™ Toblerâ€™s first law of geography

> æ‰˜å¸ƒå‹’ç¬¬ä¸€åœ°ç†å­¦å®šå¾‹æŒ‡å‡ºï¼šâ€œäº‹ç‰©å½¼æ­¤é è¿‘â€ã€‚æ¢å¥è¯è¯´ï¼Œåœ°ç†ä¸Šç›¸è¿‘çš„äº‹ç‰©æ¯”ç›¸è·è¾ƒè¿œçš„äº‹ç‰©æ›´ç›¸ä¼¼ã€‚
>
> è¿™ä¸ªå®šå¾‹åœ¨è®¸å¤šé¢†åŸŸéƒ½æœ‰åº”ç”¨ï¼ŒåŒ…æ‹¬ï¼š
>
> - åœ°ç†å­¦ï¼š ç†è§£ç©ºé—´æ¨¡å¼å’Œåœ°ç†ç°è±¡ä¹‹é—´çš„å…³ç³»ã€‚
> - åŸå¸‚è§„åˆ’ï¼š è§„åˆ’åŸå¸‚åŸºç¡€è®¾æ–½å’ŒæœåŠ¡ï¼Œä»¥æ»¡è¶³å±…æ°‘çš„éœ€æ±‚ã€‚
> - ç¯å¢ƒç§‘å­¦ï¼š ç ”ç©¶æ±¡æŸ“ç‰©å’Œç”Ÿæ€ç³»ç»Ÿä¹‹é—´çš„å…³ç³»ã€‚
> - ç¤¾ä¼šå­¦ï¼š åˆ†æç¤¾ä¼šç°è±¡çš„ç©ºé—´åˆ†å¸ƒã€‚
>
> æ‰˜å¸ƒå‹’ç¬¬ä¸€åœ°ç†å­¦å®šå¾‹æ˜¯ä¸€ä¸ªé‡è¦çš„æ¦‚å¿µï¼Œå®ƒæé†’æˆ‘ä»¬åœ°ç†ä½ç½®åœ¨å¡‘é€ ä¸–ç•Œä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚

### T-137 STEP

CCF-A CI-146 DT-2022 C-T-SPEED C-T-FLOW

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

æ—¶é—´ï¼šTransformer

ç©ºé—´ï¼šå›¾

ä»¥åŠ encoder-decoderï¼Œä½†æ˜¯åŸºäº Grah WaveNet æ”¹ç¼–

we propose a novel framework, in which <u>ST</u>GNN is <u>E</u>nhanced by a scalable time series <u>P</u>re-training model (STEP). 

Specifically, we design a pre-training model to efficiently learn temporal patterns from very long-term history time series (e.g., the past two weeks) and generate segment-level representations. 

These representations provide contextual information for short-term time series input to STGNNs and facilitate modeling dependencies between time series

- Specifically, we design an efficient unsupervised pre-training model for <u>T</u>ime <u>S</u>eries based on Trans<u>Former</u> blocks (TSFormer)
- which is trained through the masked autoencoding strategy
- design a graph structure learner based on the representation of TSFormer, which learns discrete dependency graph and utilizes the ğ‘˜NN graph computed based on the representation of TSFormer as a regularization to guide the joint training of graph structure and STGNN

![image-20240818173141567](img/image-20240818173141567.png)

STEP framework can extend to almost any STGNN, and we choose a representative method as our backend, the Graph WaveNet

##### æ•°æ®

METR-LA PEMS-BAY PEMS04(flow)

### T-138 D2STGNN

CCF-NONE CI-140 DT-2022 C-T-SPEED C-T-FLOW

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

ç©ºé—´ï¼šGNN + è‡ªæ³¨æ„åŠ›

æ—¶é—´ï¼šRNN + è‡ªæ³¨æ„åŠ›

to improve modeling performance, we propose a novel Decoupled Spatial-Temporal F ramework (DSTF) that separates the diffusion and inherent traffic information in a data-driven manner, which encompasses a unique estimation gate and a residual decomposition mechanism

- The former (residual) removes the parts of signals that the diffusion and inherent models can approximate well. Thus, the parts of signals that are not learned well is retained. 
- The latter (gate) estimates roughly the proportion of the two kinds of signals to relieve the burden of the first model in each layer, which takes the original signal as input and needs to learn specific parts in it

we propose an instantiation of DSTF, Decoupled Dynamic Spatial-Temporal Graph Neural Network (D2STGNN), that captures spatial-temporal correlations and also features a dynamic graph learning module that targets the learning of the dynamic characteristics of traffic networks

- spatial dependency by learning latent correlations between time series based on the self-attention mechanism.
- A spatial-temporal localized convolution is designed to model the hidden diffusion time series. A recurrent neural network and self-attention mechanism are used jointly to model the hidden inherent time series.

##### æ•°æ®

METR-LA PEMS-BAY é€Ÿåº¦

PEMS04 PEMS08 æµé‡

### T-139 STAEformer

CCF-B CI-57 DT-2023 C-T-SPEED C-T-FLOW

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

æ—¶ç©ºè‡ªé€‚åº”åµŒå…¥ + æœ´ç´  transformer

æ—¶ç©ºéƒ½æ˜¯è‡ªæ³¨æ„åŠ›å±‚ï¼Œå¦‚å›¾æ‰€ç¤ºï¼ŒåµŒå…¥å¯ä»¥é¢å¤–èå…¥æ—¥æœŸç­‰ä¿¡æ¯

In this study, we present a novel component called spatio-temporal adaptive embedding that can yield outstanding results with vanilla transformers. 

Our proposed Spatio-Temporal Adaptive Embedding transformer (STAEformer)

- Specifically, it adds an embedding layer on the input, providing multiple types of embeddings for the model backbone

![image-20240818223717438](img/image-20240818223717438.png)

##### æ•°æ®

METR-LA, PEMS-BAY, PEMS03/4/7/8

### T-140 SLCNN

CCF-A CI-234 DT-2020 T-C-SPEED

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

ç©ºé—´ï¼šCNN + GNN

æ—¶é—´ï¼šCNN

we propose a novel framework named Structure Learning Convolution (SLC) that enables to extend the traditional convolutional neural network (CNN) to graph domains and learn the graph structure for traffic forecasting

- We propose a generic graph convolutional formulation, which defines convolution operation as a combination of structure module and kernel module
- Two data-driven and time-vary SLC modules are proposed to capture the global and local structures, respectively
- P3D ConvNet is incorporated in SLCNN to capture the temporal dependencies in traffic data

##### æ•°æ®

PeMS-S (æ‰¾ä¸åˆ°æ¥æº)

The time range of PeMS-S is the weekdays of May and Jun of 2012, the interval is 5 minute and 228 sensors (nodes) are selected

PEMS-BAY, METR-LA 

BJF, BRF, BRF-L è‡ªå»º

- are generated from a real-world GPS trajectory data, in which about 80 million GPS points of 30,000 taxis are recoded per day
- BJF. Each node in BJF indicates a junction and 190 important junctions in Beijing are selected. The traffic data of each node is recorded in every 15 minutes and the time range is from November 2015 to October 2016. 
- BRF. BRF has totally 300 nodes and each node indicates a road. The time interval is set to 20 minutes and the time period used of BRF is from November 2015 to May 2016. 
- BRF-L. Similar to BRF, the traffic flow of each road is recoded in the dataset. BRF-L contains 1586 roads and the time interval is set to 10 minutes. The time period used of BRF-L is from November 2015 to December 2015.

#### ç›¸å…³è¯„ä»·

T-ZS2 æœ‰æ•°æ®ï¼Œä½†æ²¡è®ºè¿°

### T-74 Traffic Transformer

CCF-NONE CI-277 DT-2020 C-T-SPEED

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

we propose to design different strategies for encoding temporal information so thatboth the continuity and periodicity of traffic data can be preserved, and extend Transformer to modeling temporaland spatial dependencies jointly with the help of graph convolutional networks (GCNs)

- We design four novel position encoding strategies to encode the continuity and periodicity of time seriesto facilitate the modeling of temporal dependencies in traffic data
- We introduce a hybrid encoderâ€“decoder architecture, called Traffic Transformer, to coherently model spatial and temporal dependencies of traffic data in an end-to-end training manner, where Transformer is leveraged to model temporal dependencies and GCNs contribute to the modeling of spatial dependencies

![image-20240818235942477](img/image-20240818235942477.png)

![image-20240818235952372](img/image-20240818235952372.png)

##### æ•°æ®

METR-LA PEMS-BAY



### T-141 STGM

CCF-C CI-24 DT-2023 C-T-SPEED

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

æ—¶ç©ºä¾èµ–ï¼šæ³¨æ„åŠ›

ç©ºé—´ï¼šGNN

æ—¶é—´ï¼šCNN

we propose a Spatio-Temporal Graph Mixformer (STGM) network, a highly optimized model with low memory footprint. We address the aforementioned limits by utilizing a novel attention mechanism to capture the correlation between temporal and spatial dependencies. Specifically, we use convolution layers with a variable fields of view for each head to capture longâ€“short term temporal dependency. Additionally, we train an estimator model that express the contribution of a node over the desired prediction. The estimation is fed alongside a distance matrix to the attention mechanism. Meanwhile, we use a gated mechanism and a mixer layer to further select and incorporate the different perspectives.

- We suggest a similarity estimator model that given a set of nodes with a segment of historical traffic, approximates the expected implication of each node over the future traffic signal. 
- Furthermore, we designed a couple of improvements for the original transformer architecture named STGA that fully utilizes the multi-head attention and seamlessly capture both temporal and spatial dependencies. 
- We propose the CT-Mixer module that works in coordination with STGA to further capture the temporal locality and internode channels information

æœ‰ä¸€æ®µæè¿° CNN ä»£æ›¿ RNN çš„æè¿°ï¼Œè¯´ä¸å®šå¯ä»¥å‚è€ƒ

![image-20240819004354922](img/image-20240819004354922.png)

##### æ•°æ®

PEMS-BAY METR-LA PEMSD7M(å‚è€ƒT-28)

### T-145 STGODE

CCF-A CI-322 DT-2021 C-T-FLOW

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

æ—¶ç©ºï¼šODE

ç©ºé—´ï¼šé‚»æ¥çŸ©é˜µ+GNN

æ—¶é—´ï¼šCNN (TCN temporal dilation convolution)

Spatial-Temporal Graph Ordinary Differential Equation Networks

Specifically, we capture spatial-temporal dynamics through a tensor-based ordinary differential equation (ODE), as a result, deeper networks can be constructed and spatial-temporal features are utilized synchronously

To understand the network more comprehensively, semantical adjacency matrix is considered in our model, and a well-design temporal dilated convolution structure is used to capture long term temporal dependencies

- First, in order to depict spatial correlations from both geographical and semantic views, we construct two types of adjacency matrices, i.e. spatial adjacency matrix and semantic adjacency matrix, based on spatial connectivity and semantical similarity of traffic flow respectively

- residual connections are added between layers to alleviate the over-smoothing problem. Furthermore, it is proved that the discrete layers with residual connections can be viewed as a discretization of an Ordinary Differential Equation (ODE)

- and so a continuous graph neural network (CGNN) is derived

  a continuous GNN with residual connections is introduced to avoid the over-smoothing problem and hence be able to model long-range spatial-temporal dependencies. Last but not least, a spatial-temporal tensor is constructed to consider spatial and temporal patterns simultaneously and model complex spatial-temporal interactions

![image-20240819161324050](img/image-20240819161324050.png)

##### æ•°æ®

PeMSD7(M), PeMSD7(L), PeMS03, PeMS04, PeMS07, and PeMS08 è¿‡å»ä¸€ä¸ªå°æ—¶é¢„æµ‹æœªæ¥ä¸€ä¸ªå°æ—¶ 

#### ç›¸å…³æ”¹ç‰ˆ

- T-155
- T-156

### T-147 STWave

CCF-A CI-26 DT-2023 C-T-FLOW

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

æ—¶é—´ï¼šä»»æ„å·ç§¯+æ³¨æ„åŠ›

ç©ºé—´ï¼šSOTA å…¨ GAT

æ•´ä½“ï¼šencoder-decoder

we aim to provide a novel disentangle-fusion framework STWave to mitigate the distribution shift issue. The framework first decouples the complex traffic data into stable trends and fluctuating events, followed by a dual-channel spatio-temporal network to model trends and events, respectively

Finally, reasonable future traffic can be predicted through the fusion of trends and events. Besides, we incorporate a novel query sampling strategy and graph wavelet-based graph positional encoding into the full graph attention network to efficiently and effectively model dynamic spatial correlations

![image-20240820015255577](img/image-20240820015255577.png)

The end-to-end and our proposed disentangle-fusion traffic forecasting framework, where STNet denotes the spatiotemporal network.

- a dual-channel spatio-temporal network to capture the dualscale temporal changes and spatial correlations.

- Following this principle, we propose a novel traffic forecasting framework named STWave, which first applies the discrete wavelet transform (DWT) to disentangle the traffic time series into the dual-scale trend-event representations because DWT can decompose data into various components, such as the main information (i.e., trend information) and details (i.e., event information)

- STNet that utilizes corresponding sequential and graph-based methods on the different information to capture the various temporal changes and spatial correlations

  the causal convolution with small kernel size, temporal attention with the global temporal receptive field, and the state-of-the-art full GAT are adopted on events, trends, and both of them to capture fluctuating temporal changes, stable temporal changes, and different temporal changes-based dynamic global spatial correlations, respectively

- Moreover, to address the high complexity and insufficient structure information in the full GAT, a novel query sampling strategy and a novel graph wavelet-based graph positional encoding are proposed in STWave. The query sampling strategy reduces the complexity while maintaining the global receptive field according to the hierarchical nature of the traffic system

  and the graph wavelet-based graph positional encoding brings the local-global balanced structure information based on the spectral graph theory

- Finally, an adaptive event fusion module is used in STWave to merge useful information from inaccurate forecast events into easily predict trends

![image-20240820015920131](img/image-20240820015920131.png)

##### æ•°æ®

PeMSD3, PeMSD4, PeMSD7, PeMSD8, PeMSD7(M), and PeMSD7(L) 12 to 12 IO

### T-148 PDFormer

CCF-A CI-135 DT-2023 C-T-FLOW

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

ç©ºé—´ï¼šè‡ªæ³¨æ„åŠ› å›¾çŸ©é˜µ

æ—¶é—´ï¼šTransformer æ³¨æ„åŠ›

we propose a novel Propagation Delayaware dynamic long-range transFormer, namely PDFormer

we design a spatial self-attention module to capture the dynamic spatial dependencies. Then, two graph masking matrices are introduced to highlight spatial dependencies from short- and longrange views.

Moreover, a traffic delay-aware feature transformation module is proposed to empower PDFormer with the capability of explicitly modeling the time delay of spatial information propagation

![image-20240820020802968](img/image-20240820020802968.png)

Based on this module, we further design a delay-aware feature transformation module to integrate historical traffic patterns into spatial self-attention and explicitly model the time delay of spatial information propagation.

Finally, we adopt the temporal self-attention module to identify the dynamic temporal patterns in traffic data

![image-20240820020907709](img/image-20240820020907709.png)

##### æ•°æ®

PeMS04, PeMS07, PeMS08

åŸºäºç½‘æ ¼ NYTaxi, CHBike, TDrive

- NYTaxi 75(15x5) 30min 01/01/2014-12/31/2014 
- CHBike 270(15x18) 30min 07/01/2020-09/30/2020 
- TDrive 1024(32x32) 60min 02/01/2015-06/30/2015

12 - 12

#### ç›¸å…³æ”¹ç‰ˆ

- T-151

### T-150 DDGCRN

CCF- CI-47 DT-2023 C-T-FLOW

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

ç©ºé—´ï¼šåŠ¨æ€å›¾å·ç§¯ï¼Œæ—¶é—´ï¼šåµŒå…¥

> poe è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºç©ºé—´-æ—¶é—´åŒæ­¥å›¾å·ç§¯ç½‘ç»œ (STSGCN) çš„æ¨¡å‹ï¼Œç”¨äºç©ºé—´-æ—¶é—´ç½‘ç»œæ•°æ®çš„é¢„æµ‹ã€‚å®ƒä¸»è¦ä½¿ç”¨äº†ä»¥ä¸‹æŠ€æœ¯å¤„ç†æ—¶é—´ç‰¹å¾å’Œç©ºé—´ç‰¹å¾ï¼š
>
> **æ—¶é—´ç‰¹å¾å¤„ç†ï¼š**
>
> - **æ—¶é—´åµŒå…¥ (Temporal Embedding):** ä¸ºæ¯ä¸ªæ—¶é—´æ­¥å¼•å…¥ä¸€ä¸ªå¯å­¦ä¹ çš„æ—¶é—´åµŒå…¥çŸ©é˜µï¼Œå°†æ—¶é—´ä¿¡æ¯ç¼–ç åˆ°æ¨¡å‹ä¸­ã€‚
> - **å¤šæ¨¡å—å±‚ (Multi-Module Layer):** åœ¨æ¨¡å‹ä¸­ä½¿ç”¨å¤šä¸ªæ¨¡å—ï¼Œæ¯ä¸ªæ¨¡å—å¯¹åº”ä¸€ä¸ªç‰¹å®šçš„æ—¶é—´æ®µï¼Œä»è€Œèƒ½å¤Ÿæ•æ‰ä¸åŒæ—¶é—´æ®µçš„å¼‚è´¨æ€§ã€‚
>
> **ç©ºé—´ç‰¹å¾å¤„ç†ï¼š**
>
> - **å›¾å·ç§¯ (Graph Convolution):** ä½¿ç”¨å›¾å·ç§¯æ“ä½œæ¥æ•è·ç©ºé—´ç½‘ç»œä¸­èŠ‚ç‚¹ä¹‹é—´çš„ç›¸äº’å½±å“ã€‚
> - **æœ¬åœ°åŒ–ç©ºé—´-æ—¶é—´å›¾ (Localized Spatial-Temporal Graph):** å°†ç›¸é‚»æ—¶é—´æ­¥çš„èŠ‚ç‚¹è¿æ¥èµ·æ¥ï¼Œæ„å»ºä¸€ä¸ªæœ¬åœ°åŒ–ç©ºé—´-æ—¶é—´å›¾ï¼Œä»è€Œèƒ½å¤Ÿç›´æ¥æ•è·èŠ‚ç‚¹åœ¨ç©ºé—´å’Œæ—¶é—´ä¸Šçš„ç›¸äº’å½±å“ã€‚

we propose a decomposition dynamic graph convolutional recurrent network (DDGCRN) for traffic forecasting. DDGCRN combines a dynamic graph convolution recurrent network with an RNN-based model that generates dynamic graphs based on time-varying traffic signals, allowing for the extraction of both spatial and temporal features. Additionally, DDGCRN separates abnormal signals from normal traffic signals and models them using a data-driven approach to further improve predictions

- Here, temporal information corresponding to the traffic signal is combined with the spatial embedding to generate a spatio-temporal embedding. Then, a dynamic graph embedding is combined with dynamic signals extracted from the traffic signals to produce a dynamic graph. This method fully considers the periodicity and dynamics of the traffic signals so that the generated dynamic graph captures the most realistic correlations between nodes. Lastly, the spatio-temporal dependencies in the traffic signals are extracted via the Dynamic Graph Convolution Recurrent Module (DGCRM), which is based on an RNN

![image-20240820175526794](img/image-20240820175526794.png)

a dynamic graph convolution gated recurrent unit (DGCRU) is obtained by replacing the matrix product in a GRU with a combination of a dynamic graph convolution method and an NAPL module

##### æ•°æ®

PeMSD3, PeMSD4, PeMSD7, PeMSD8, PeMS07(M), PeMS07(L) 12 - 12

### T-151 ADCSD

CCF-NONE CI-4 DT-2024 C-T-FLOW

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

å¯¹ä¸‰ä¸ªæ¨¡å‹åšå‡ºäº†å¾®è°ƒï¼Œåˆ†åˆ«å¯¹ PDFormer, AGCRN, ASTGCN

æ—¶é—´ï¼šåˆ†è§£

ç©ºé—´ï¼šè‡ªé€‚åº”å‘é‡

we propose an Adaptive Double Correction by Series Decomposition (ADCSD) method, which first decomposes the output of the trained model into seasonal and trend-cyclical parts and then corrects them by two separate modules during the testing phase using the latest observed data entry by entry. In the proposed ADCSD method, instead of fine-tuning the whole trained model during the testing phase, a lite network is attached after the trained model, and only the lite network is fine-tuned in the testing process each time a data entry is observed. Moreover, to satisfy that different time series variables may have different levels of temporal drift, two adaptive vectors are adopted to provide different weights for different time series variables

æ ¸å¿ƒï¼šOnline Test-Time Adaptation (OTTA)

- echniques are widely adopted in computer vision (CV) community to address the distribution shift issue by continuously updating and refining the model during the testing phase, based on the feedback and new data encountered during deployment

##### æ•°æ®

PEMS07, è‡ªå»º BayAREA å’Œç½‘æ ¼æ•°æ® NYCTaxi, T-Drive (contain inflow and outflow data)

### T-152 CorrSTN

CCF-C CI-14 DT-2022 C-T-FLOW

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

encoder decoder

ç©ºé—´ï¼šåŠ¨æ€ GNN

æ—¶é—´ï¼šå¤šå¤´æ³¨æ„åŠ›

n this paper, based on the maximal information coefficient, we present two elaborate spatiotemporal representations, spatial correlation information (SCorr) and temporal correlation information (TCorr). Using SCorr, we propose a correlation information-based spatiotemporal network (CorrSTN) that includes a dynamic graph neural network component for integrating correlation information into spatial structure effectively and a multi-head attention component for modeling dynamic temporal dependencies accurately. Utilizing TCorr, we explore the correlation pattern among different periodic data to identify the most relevant data, and then design an efficient data selection scheme to further enhance model performance

æåˆ° Correlation information å¹¶æåˆ° GWN(Graph WaveNet)

![image-20240820233955651](img/image-20240820233955651.png)

##### æ•°æ®

PEMS03, PEMS04, PEMS07 and PEMS08

- Traffic Flow Prediction (æ²¡æ‰¾åˆ°å‡ºå¤„ï¼Œæ€€ç–‘æ˜¯è‡ªå»º)

  The Traffic Flow Prediction dataset is collected every 15 min at 36 sensor locations along two major highways in the Northern Virginia/Washington, D.C., capital region

HZME

### T-153 Cy2Mixer

CCF-NONE CI-1 DT-2024 C-T-FLOW

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

è®¤ä¸ºæ˜¯ GNN + CNN

Cycle to Mixer (Cy2Mixer), a novel spatiotemporal GNN based on topological non-trivial invariants of spatio-temporal graphs with gated multi-layer perceptrons (gMLP)

The Cy2Mixer is composed of three blocks based on MLPs: A message-passing block for encapsulating spatial information, a cycle message-passing block for enriching topological information through cyclic subgraphs, and a temporal block for capturing temporal properties

- Temporal Block This block employs a 3 Ã— 3 convolution network to get the projection values
- Spatial Message-Passing & Cycle Message-Passing Blocks These blocks employ the MPNN for their projection function in the Gating Unit. Notably, the Spatial Message-Passing block uses the standard adjacency matrix

##### æ•°æ®

PEMS04, PEMS07, and PEMS08 12

### T-154 PM-DMNet

CCF-NONE CI-2 DT-2024 

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

ç©ºé—´ GCN 

æ—¶é—´ æ³¨æ„åŠ›

encoder-decoder

Dynamic Pattern Matching Gated Recurrent Unit (DPMGRU) æ—¶ç©ºéƒ½æœ‰

we propose a Pattern-Matching Dynamic Memory Network (PMDMNet). PM-DMNet employs a novel dynamic memory network to capture traffic pattern features with only O(N ) complexity, significantly reducing computational overhead while achieving excellent performance. The PM-DMNet also introduces two prediction methods: Recursive Multi-step Prediction (RMP) and Parallel Multi-step Prediction (PMP), which leverage the time features of the prediction targets to assist in the forecasting process. Furthermore, a transfer attention mechanism is integrated into PMP, transforming historical data features to better align with the predicted target states, thereby capturing trend changes more accurately and reducing errors

![image-20240821015916254](img/image-20240821015916254.png)

We propose a novel Dynamic Memory Network (DMN) module designed to learn inherent representative traffic patterns within the data associated with each node. By employing a pattern matching approach, this module identifies and extracts traffic pattern features most similar to the input data while effectively reducing computational overhead. â€¢ We introduce a new Transfer Attention Mechanism (TAM). TAM transforms the existing historical hidden states into latent states aligned with the prediction target features, mitigating the error caused by the discrepancy between historical data and prediction targets

![image-20240821020131172](img/image-20240821020131172.png)

![image-20240821020140667](img/image-20240821020140667.png)

##### æ•°æ®

PEMSD4/7/8 D7M D7L NYC-Bike14/15/16 NYC-Taxi15/16 å¯¹ NYC çš„ç»†åˆ†åˆ†åˆ«ç»™å‡ºäº†ä¾æ®å‡ºå¤„è®ºæ–‡

12-12

### T-155 STG-NCDE

CCF-A CI-206 DT-2022 C-T-SPEED C-T-FLOW

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

æ—¶é—´ç©ºé—´éƒ½æ˜¯ NCDE

we present the method of spatio-temporal graph neural controlled differential equation (STG-NCDE)

Neural controlled differential equations (NCDEs) are a breakthrough concept for processing sequential data. We extend the concept and design two NCDEs: one for the temporal processing and the other for the spatial processing. After that, we combine them into a single framework

æ˜¯ä¸€ç§è¿ç»­å½¢å¼åŒ–çš„ RNN

we design a method based on neural controlled differential equations (NCDEs) for the first time. NCDEs, which are considered as a continuous analogue to recurrent neural networks (RNNs)

æœ‰ NCDE ç›¸å…³çš„ç»¼è¿°ï¼Œå³ ODE

NODEs generalize ResNets in a continuous manner. STGODE utilizes this NODE technology to solve the spatiotemporal forecasting problem

> GPTï¼š
>
> ODE æ˜¯å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆOrdinary Differential Equationï¼‰çš„ç¼©å†™ã€‚å¸¸å¾®åˆ†æ–¹ç¨‹æ˜¯æŒ‡ä¸€ä¸ªå‡½æ•°çš„å¯¼æ•°å’Œè¯¥å‡½æ•°ä¹‹é—´å…³ç³»çš„æ•°å­¦è¡¨è¾¾å¼ã€‚åœ¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ä¸­ï¼Œå¸¸å¾®åˆ†æ–¹ç¨‹é€æ¸å—åˆ°å…³æ³¨ï¼Œå°¤å…¶æ˜¯åœ¨åŠ¨æ€ç³»ç»Ÿå»ºæ¨¡ã€è¿ç»­æ—¶é—´æ¨¡å‹å’Œå¯å¾®æ¨¡å‹çš„ç ”ç©¶ä¸­
>
> Neural ODEs æ˜¯ä¸€ç§å°†å¸¸å¾®åˆ†æ–¹ç¨‹å’Œç¥ç»ç½‘ç»œç»“åˆçš„æ¨¡å‹ã€‚è¿™ç§æ¨¡å‹ä¸­çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œå°†ä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œçš„å±‚æ›¿æ¢ä¸ºå¸¸å¾®åˆ†æ–¹ç¨‹çš„æ±‚è§£è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼ŒNeural ODEs å‡è®¾ç¥ç»ç½‘ç»œçš„è¾“å‡ºä¸æ˜¯é€šè¿‡ä¸€å±‚å±‚çš„ç¦»æ•£æ“ä½œå¾—åˆ°çš„ï¼Œè€Œæ˜¯é€šè¿‡ä¸€ä¸ªè¿ç»­çš„æ—¶é—´æ¼”åŒ–è¿‡ç¨‹æè¿°çš„

NCDE ä¸ NODE åŒºåˆ«è§è®ºæ–‡ï¼Œé‚£å‡ ä¸ªå…¬å¼æˆ‘æ²¡çœ‹æ‡‚

##### æ•°æ®

PeMSD7(M), PeMSD7(L), PeMS03, PeMS04 12-12

### T-156 STG-NRDE

CCF-NONE CI-8 DT-2023 C-T-SPEED C-T-FLOW

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

spatio-temporal graph neural rough differential equation (STG-NRDE)

Neural rough differential equations (NRDEs) are a breakthrough concept for processing timeseries data. Their main concept is to use the log-signature transform to convert a time-series sample into a relatively shorter series of feature vectors

We extend the concept and design two NRDEs: one for the temporal processing and the other for the spatial processing. After that, we combine them into a single framework

NRDEs are based on the rough path theory designed to make sense of the controlled differential equation.

è®¤ä¸ºæ˜¯å¯¹ T-155 çš„ä¸€ä¸ªä¼˜åŒ–ï¼Œæ”¹äº†ä¸€ç‚¹ï¼Œå…¶ä»–å†…å®¹éƒ½å·®ä¸å¤š

##### æ•°æ®

PeMSD7(M), PeMSD7(L), PeMS03, PeMS04, PeMS07, and PeMS08

### T-157 HAGCN

CCF-NONE CI-2 DT-2022 C-T-FLOW

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

We propose a network decentralization attention based heterogeneity-aware graph convolution network (HAGCN) method that aggregates the hidden states of adjacent nodes by considering the importance of each channel in a heterogeneous graph

learning spatial relationships through graph convolution

The traffic signal was transformed using temporal convolution networks (TCNs)

##### æ•°æ®

PeMSD4 and PeMSD8

### T-158 HTVGNN

CCF-NONE CI-1 DT-2024 C-T-FLOW

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

æ—¶é—´ï¼šæ³¨æ„åŠ›

ç©ºé—´ï¼šåŠ¨æ€å›¾å·ç§¯

encoder-decoder

we have proposed a novel hybrid time-varying graph neural network (HTVGNN) for traffic flow prediction. 

Firstly, a novel enhanced temporal perception multi-head self-attention mechanism based on time-varying mask enhancement was reported to more accurately model the dynamic temporal dependencies among distinct traffic nodes in the traffic network

Secondly, we have proposed a novel graph learning strategy to concurrently learn both static and dynamic spatial associations between different traffic nodes in road networks. Meanwhile, in order to enhance the learning ability of time-varying graphs, a coupled graph learning mechanism was designed to couple the graphs learned at each time step

Furthermore, we proposed a dynamic graph convolution approach, which utilized a combined topology and semantic matrix as a mask, enabling more precise modeling of the dynamic spatial correlations in traffic networks

- A novel temporal perception multi-head self-attention mechanism enhanced by a time-varying mask, diverging from traditional approaches is proposed. This mechanism dynamically adjusts attention calculations based on the temporal characteristics of input data, enabling more accurate capture of temporal dependencies among traffic nodes. The time-varying mask consists of three components: a static mask embedding for correcting the multi-head self-attention mechanism, and two dynamic mask embeddings to augment the time-awareness of the mechanism

##### æ•°æ®

PEMS03, PEMS04, PEMS07 and PEMS08

### T-159 LightCTS

CCF-NONE CI-16 DT-2023 C-T-SPEED C-T-FLOW

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

æ—¶é—´ï¼šCNN

ç©ºé—´ï¼šTransformer + æ³¨æ„åŠ›

éƒ½æœ‰å›¾ç¤º

Correlated time series (CTS)

On this basis, we propose the LightCTS framework that adopts plain stacking of temporal and spatial operators instead of alternate stacking that is much more computationally expensive. Moreover, LightCTS features light temporal and spatial operator modules, called L-TCN and GL-Former, that offer improved computational efficiency without compromising their feature extraction capabilities. LightCTS also encompasses a last-shot compression scheme to reduce redundant temporal features and speed up subsequent computations

By following these directions, LightCTS offers a set of novel lightweight techniques. First, LightCTS includes a novel T-operator module called Light Temporal Convolutional Network (L-TCN) and a novel S-operator module called GlobalLocal TransFormer (GL-Former) for temporal and spatial feature extraction,

##### æ•°æ®

PEMS04,08 METR-LA PEMS-BAY 12 - 12

### T-160 3D-TGCN

CCF-NONE CI-63 DT-2019 C-T-SPEED

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

æ—¶ç©ºï¼šGNN

we propose a novel deep learning framework to overcome these issues: 3D Temporal Graph Convolutional Networks (3D-TGCN). Two novel components of our model are introduced. (1) Instead of constructing the road graph based on spatial information, we learn it by comparing the similarity between time series for each road, thus providing a spatial information free framework. (2) We propose an original 3D graph convolution model to model the spatio-temporal data more accurately

we propose a 3D graph convolution network where 3D convolution is applied to simultaneously learn the spatial and temporal patterns together.

Furthermore, we offer a spatial information free approach for constructing the graph for traffic network, purely relying on the similarity of time series for each road. This new proposal could capture more effective patterns between different roads than the spatial graph, facilitating superior prediction performance.

##### æ•°æ®

PEMSD7M/L METR-LA 12 - 12

### T-161 DASTNet

CCF-B CI-31 DT-2022 C-T-FLOW

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

è¿ç§»å­¦ä¹ ä½¿ç”¨ï¼ŒæŠŠå…¶ä»–åœ°åŒºçš„äº¤é€šæµé‡çš„æ¨¡å‹å¾®è°ƒ

ç©ºé—´ï¼šåµŒå…¥ graph isomorphic network (Gin)

æ—¶é—´ï¼šGPU

- To the best of our knowledge, this is the first time that the adversarial domain adaption is used in traffic forecasting to effectively learn the transferable knowledge in multiple cities.

propose a novel transferable traffic forecasting framework: Domain Adversarial Spatial-Temporal Network (DastNet). DastNet is pre-trained on multiple source networks and fine-tuned with the target networkâ€™s traffic data. Specifically, we leverage the graph representation learning and adversarial domain adaptation techniques to learn the domain-invariant node embeddings, which are further incorporated to model the temporal traffic data. To the best of our knowledge, we are the first to employ adversarial multi-domain adaptation for network-wide traffic forecasting problems.

![image-20240823231341403](img/image-20240823231341403.png)

##### æ•°æ®

12(1h) pems04 07 08

### T-162 ST-SSL

CCF-A CI-94 DT-2023 

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

éƒ½æ˜¯ CNN(ç¼–ç ?)ï¼Œå›¾ï¼š+å¢å¹¿

we propose a novel Spatio-Temporal Self-Supervised Learning (ST-SSL) traffic prediction framework which enhances the traffic pattern representations to be reflective of both spatial and temporal heterogeneity, with auxiliary self-supervised learning paradigms

- Specifically, our ST-SSL is built over an integrated module with temporal and spatial convolutions for encoding the information across space and time.

##### æ•°æ®

The first kind is about bike rental records in New York City. NYCBike1 (Zhang, Zheng, and Qi 2017) spans from 04/01/2014 to 09/30/2014, and NYCBike2 (Yao et al. 2019) spans from 07/01/2016 to 08/29/2016. They are all measured every 30 minutes. The second kind is about taxi GPS trajectories. NYCTaxi (Yao et al. 2019) spans from 01/01/2015 to 03/01/2015. Its time interval is half an hour. BJTaxi (Zhang, Zheng, and Qi 2017), collected in Beijing, spans from 03/01/2015 to 06/30/2015 on an hourly basis

2/3 å¤© 7:1:2

### T-164 MemDA

CCF-B CI-7 DT-2023 C-T-SPEED

### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

encoder-decoder

In this study, we propose a new urban time series prediction model for the concept drift problem, which encodes the drift by considering the periodicity in the data and makes on-the-fly adjustments to the model based on the drift using a meta-dynamic network

- two key components: a dual memory module and a strategically adjustable meta-dynamic network

> æ ¹æ®è®ºæ–‡æ‘˜è¦å’Œå†…å®¹,è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMemDAçš„æ–°å‹åŸå¸‚æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹,ç”¨äºè§£å†³æ¦‚å¿µæ¼‚ç§»é—®é¢˜ã€‚å®ƒä¸»è¦ä½¿ç”¨äº†ä»¥ä¸‹ä¸¤ä¸ªå…³é”®æŠ€æœ¯:
>
> 1. åŒé‡è®°å¿†æ¨¡å—(Dual Memory Module):
>    - è¯¥æ¨¡å—èƒ½å¤Ÿæœ‰æ•ˆç®¡ç†å¤§é‡å†å²æ•°æ®,ä¿ç•™é•¿æœŸå†å²ä¿¡æ¯çš„åŒæ—¶è·å–æœ€æœ‰ä»·å€¼çš„ä¿¡æ¯,ä»¥åº”å¯¹æ¦‚å¿µæ¼‚ç§»ã€‚
> 2. å¯ç­–ç•¥è°ƒæ•´çš„å…ƒåŠ¨æ€ç½‘ç»œ(Meta-Dynamic Network):
>    - è¯¥ç½‘ç»œèƒ½å¤ŸåŠ¨æ€è°ƒæ•´æ¨¡å‹å‚æ•°,åœ¨ä¿æŒå…ˆå‰å­¦ä¹ çŸ¥è¯†çš„ç¨³å®šæ€§ä¸é€‚åº”æ–°æ•°æ®æ¨¡å¼çš„å¯å¡‘æ€§ä¹‹é—´è¾¾åˆ°å¹³è¡¡,ä»è€Œæœ‰æ•ˆåº”å¯¹æ¦‚å¿µæ¼‚ç§»ã€‚
>
> è¿™ä¸¤ä¸ªå…³é”®ç»„ä»¶çš„ç»“åˆ,ä½¿å¾—MemDAæ¨¡å‹èƒ½å¤Ÿç¼–ç æ•°æ®ä¸­çš„å‘¨æœŸæ€§æ¨¡å¼,å¹¶åŸºäºæ¼‚ç§»å®æ—¶è°ƒæ•´æ¨¡å‹,ä»è€Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„æ¦‚å¿µæ¼‚ç§»å¯¹ç­–æ–¹æ³•ã€‚
>
> æ€»çš„æ¥è¯´,è®ºæ–‡æå‡ºçš„MemDAæ¨¡å‹åˆ©ç”¨äº†è®°å¿†æ¨¡å—å’Œå…ƒåŠ¨æ€ç½‘ç»œè¿™ä¸¤ç§æŠ€æœ¯æ¥æ•è·æ—¶ç©ºç‰¹å¾å’Œé€‚åº”æ¦‚å¿µæ¼‚ç§»,ä»è€Œæé«˜äº†åŸå¸‚æ—¶é—´åºåˆ—é¢„æµ‹çš„æ•ˆæœã€‚

##### æ•°æ®

è®ºæ–‡ç ”ç©¶é¢†åŸŸä¸åªæ˜¯äº¤é€šé¢„æµ‹ï¼Œäº¤é€šé‡Œç”¨äº† PEMS å’Œ Beijing

Beijing 2022/05/12âˆ¼2022/07/25 3126 5 minutes Sudden

### T-165 factorized ST-TGCN

CCF-NONE CI-3 DT-2021 C-T-SPEED

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

ç©ºé—´ï¼šå›¾å·ç§¯ æ—¶é—´ï¼šå CNNåˆ°ä¸Šé¢æˆå¼ é‡

we propose a factorized Spatial-Temporal Tensor Graph Convolutional Network to deal with traffic speed prediction. Traffic networks are modeled and unified into a graph that integrates spatial and temporal information simultaneously. We further extend graph convolution into tensor space and propose a tensor graph convolution network to extract more discriminating features from spatial-temporal graph data. To reduce the computational burden, we take Tucker tensor decomposition and derive factorized a tensor convolution, which performs separate filtering in small-scale space, time, and feature modes. Besides, we can benefit from noise suppression of traffic data when discarding those trivial components in the process of tensor decomposition

##### æ•°æ®

SZ-taxi: This dataset is collected from a taxi trajectory of Shenzhen, China. It defines 156 major roads as nodes, which are characterized by the speed at which taxis pass through the roads. And the adjacency matrix represents the positional connections between roads. These data are collected every 15 minutes, ranging from 1/1/2015 to 1/31/2015.

### T-166 BSTGCN

CCF-NONE CI-14 DT-2020 C-T-SPEED

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

GCN encoder-decoder GRU

we propose a Bayesian Spatio-Temporal Graph Convolutional Network (BSTGCN)

##### æ•°æ®

SZ-taxi Los-Loop 60min

### T-167 MHAST-GCN

CCF-NONE CI-4 DT-2023

#### é˜…è¯»ç¬”è®°

##### æ¨¡å‹

ç©ºé—´ å›¾å·ç§¯ï¼›æ—¶é—´ GRUï¼› æ—¶ç©º æ³¨æ„åŠ›

To solve this challenge, this paper presents a traffic forecasting model which combines a graph convolutional network, a gated recurrent unit, and a multi-head attention mechanism to simultaneously capture and incorporate the spatio-temporal dependence and dynamic variation in the topological sequence of traffic data effectively

##### æ•°æ®

SZ-taxi Los-Loop