说明：

- 所有论文按自定义编号 **T-XXX**，编号对应表存储在 Zotero 里，自行查阅。收录具体实现论文，不包括综述。综述在 `内容整理.md` 主要记录。

- 引用量截止查询时间(按 2024 暑假算)，参考谷歌学术，按 **CI-xxx** 表示引用量。(CI: cite)

- CCF 评级按照 **CCF-xxx** 来。

- 年份/日期按照 **DT-yyyy** 来，或 DT-yyyy-mm。(DT: datetime)

- 论文简称写在标题

- 按照 **C-XXX** 给论文打标签。(C: category)

  > 具体标签有：-T (target) 预测目标(速度，流量等)

# 具体论文

## 经典

### T-28 STGCN

CCF-A CI-3886 DT-2017 C-T-SPEED

#### 阅读笔记

##### 模型

两种 CNN 分别处理时间和空间特征

组成：两个时空卷积块和全连接层输出，每个块由两个时间卷积夹着一个空间卷积，使用了残差连接和瓶颈策略

> (GPT) 瓶颈策略的核心思想是通过引入某种形式的限制（如维度压缩、计算资源限制等），强制模型做出选择，聚焦于最重要的信息。这在提升模型的泛化能力、稳定性以及减少计算资源的使用等方面，都具有重要作用。

![image-20240815222203379](img/image-20240815222203379.png)

图2说明：

Architecture of spatio-temporal graph convolutional networks. The framework STGCN consists of two spatio-temporal convolutional blocks (ST-Conv blocks) and a fully-connected output layer in the end. Each ST-Conv block contains two temporal gated convolution layers and one spatial graph convolution layer in the middle. The residual connection and bottleneck strategy are applied inside each block.

超参数选择的方法：Tree-structured Parzen Estimator (TPE) (有论文)

##### 数据

研究交通速度

数据集是

自建：BJER4

- gathered from the major areas of east ring No.4 routes in Beijing City by double-loop detectors. There are 12 roads selected for our experiment. The traffic data are aggregated every 5 minutes. The time period used is from 1st July to 31st August, 2014 except the weekends. We select the first month of historical speed records as training set, and the rest serves as validation and test set respectively

标准：PeMSD7

- 使用加权矩阵，距离的分类讨论函数是边权

- 数据描述：(参考官方代码仓库 README.md)

  每天的记录次数乘以天数是行，道路数是列，内容是速度记录，节点数是 228 和 1026，行数是 12672(我算出来44天)，值是 60 多和 70 多的占大头，有极少其他值，预测取了 15/30/45min，预测视野越大误差越大

  - PeMSD7_V_{`$num_route`}.csv : Historical Speed Records with shape of `[len_seq * num_road] (len_seq = day_slot * num_dates)`.
  - PeMSD7_W_{`$num_route`}.csv : Weighted Adjacency Matrix with shape of `[num_road * num_road]`.

论文里有图表展示了有一个U形和双U形数据(蓝色是真实)

附录有很多张图表示实验结果

查阅得知，其中的 FC-LSTM 论文出处根本就不是这个领域的，而是该技术的经典论文或迁移应用，怀疑是自建模型

##### 代码

[src](https://github.com/VeritasYin/STGCN_IJCAI-18/tree/master)

#### 相关评价

T-ZS2 总结：

- 堆叠多个时空卷积块，每个块连接两个时间卷积层和一个图卷积层

  > stacks multiple spatio-temporal convolution blocks and each block concatenate two temporal convolution and one graph convolution layer.  

- 使用 ChebNet 作为图卷积操作，用一阶近似比较

  > ChebNet is chosen as the graph convolution operator in STGCN, after a comparison with its first-order approximation  

- CNN 代替 RNN 获取时间加快了训练时间

  > The usage of temporal convolution layers instead of RNNs for temporal modeling accelerates the training phase of STGCN  

- 变种 ASTGCN 引入两个注意力层分别获取时空动态联系 T-81

  > Attention based Spatio-temporal graph convolutional network (ASTGCN) further introduces two attention layers in STGCN to capture the dynamic correlations in spatial dimension and temporal dimension, respectively  

### T-40 DCRNN

CCF-NONE CI-3654 DT-2018 C-T-SPEED

#### 阅读笔记

##### 模型

空间：图双向随机游走

时间：encoder-decoder

DCRNN captures the spatial dependency using bidirectional random walks on the graph, and the temporal dependency using the encoder-decoder architecture with scheduled sampling

- 空间

  We model the spatial dependency by relating traffic flow to a diffusion process, which explicitly captures the stochastic nature of traffic dynamics. This diffusion process is characterized by a random walk on G with restart probability, and a state transition matrix

- 时间

  We replace the matrix multiplications in GRU with the diffusion convolution

  In multiple step ahead forecasting, Both the encoder and the decoder are recurrent neural networks with Diffusion Convolutional Gated Recurrent Unit (DCGRU)

##### 数据

特征描述

![image-20240817003909585](img/image-20240817003909585.png)

Figure 1: Spatial correlation is dominated by road network structure. (1) Traffic speed in road 1 are similar to road 2 as they locate in the same highway. (2) Road 1 and road 3 locate in the opposite directions of the highway. Though close to each other in the Euclidean space, their road network distance is large, and their traffic speeds differ significantly.

数据集：交通速度预测

- METR-LA 感觉是完全数据集
- PEMS-BAY 该数据集的开山

#### 相关评价

(T-ZS1)

encoder-decoder 图扩散比 FNN, LSTM 好，消融证明扩散卷积更好

have demonstrated that their encoder-decoder model with graph diffusion managed to outperform simple FNN and LSTM. Not only that, they also performed an ablation test to demonstrate that their novel diffusion convolution module manages to outperform simpler variations 

GRU: 矩阵乘法改为扩散卷积操作

replaced the matrix multiplication inside Gated Recurrent Unit with the diffusion convolution operation 



(T-ZS2)

最出名的一个是 DCRNN

> (T-ZS2) The most famous one is diffusion convolutional recurrent neural network (DCRNN)  

- 图卷积网络和 RNN 学习时空

  > which uses diffusion graph convolutional networks and RNN to learn the representations of spatial dependencies and temporal relations  

- 最初用于交通速度预测，现在成为了基准模型

  > DCRNN was originally proposed for traffic speed forecasting and is now widely used as a baseline.  

- 建图方式：距离矩阵

  > To create the traffic graph, the adjacency matrix is defined as the thresholded pairwise road network distances.  

- 支持有向图，引入扩散卷积(DC)

  > Compared with other graph convolutional models that can only operate on undirected graphs, e.g., ChebNet, DCRNN introduces the diffusion convolution (DC) operation for directed graph and is more suitable for transportation scenarios, which is defined as follows:   

  $$
  \mathbf X_{*DC}=\sum_{k=0}^{K-1}(\theta_{k,1}(D_O^{-1}A)^k+\theta_{k,2}(D_I^{-1}A^T))\mathbf X
  $$

  其中 $\mathbf X\in R^{N\times d}$ 是节点特征矩阵，$A$ 是邻接矩阵，$D_O,D_I$ 是对角矩阵：出度和入度，$\theta$ 是两个模型参数，$K$ 是扩散步数。

  区分入度和出度实现了有向图 

  > where X 2 RN×d is the node feature matrix, A is the adjacency matrix, DO and DI are diagonal out-degree and in-degree matrices, θk;1 and θk;2 are model parameters, K is the number of diffusion steps. By defining and using out-degree and in-degree matrices, DCRNN models the bidirectional diffusion process to capture the influence of both upstream and downstream traffic  

- 对无向图不是很实用

  > While DCRNN is a strong baseline, it is not suitable or desirable for the undirected graph cases  

- 扩展版本 T-88：统一构建 RNN，基于任意图卷积

  Then DCRNN is extended with a stronger learning ability in graph GRU a unified method for constructing an RNN based on an arbitrary graph convolution operator is proposed, instead of the single RNN model used in DCRNN

观察图扩散过程和邻节点关联

observed the traffic diffusion along the road network and the correlation between several adjacent traffic sensors

### T-64 Graph WaveNet

CCF-A CI-1984 DT-2019 C-T-SPEED

#### 阅读笔记

##### 模型

空间：自适应依赖矩阵，点嵌入学习，图卷积

时间：堆叠扩张(如图2，倍增) 1D CNN 任意卷积

By developing a novel adaptive dependency matrix and learn it through node embedding, our model can precisely capture the hidden spatial dependency in the data. 

With a stacked dilated 1D convolution component whose receptive field grows exponentially as the number of layers increases, Graph WaveNet is able to handle very long sequences

- We propose a graph convolution layer in which a self-adaptive adjacency matrix can be learned from the data through an end-to-end supervised training
- we adopt stacked dilated casual convolutions to capture temporal dependencies. The receptive field size of stacked dilated casual convolution networks grows exponentially with an increase in the number of hidden layers

> (GPT) “end-to-end”（端到端）指的是一种模型训练方式，在这种方式中，模型从输入的原始数据开始，直接学习到最终输出的目标结果，中间的所有步骤都是在同一个模型中完成的

图3：

![image-20240817224043300](img/image-20240817224043300.png)

##### 数据

METR-LA PEMS-BAY

做了时间效率对比，看样子做了消融实验

静态图可能不能反应真实依赖

However, the explicit graph structure (relation) does not necessarily reflect the true dependency and genuine relation may be missing due to the incomplete connections in the data.

静态的局限性的例子

To give each circumstance an example, let us consider a recommendation system. In the first case, two users are connected, but they may have distinct preferences over products. In the second case, two users may share a similar preference

空间影响的直观图示

<img src="img/image-20240817220918943.png" alt="image-20240817220918943" style="zoom:67%;" />



#### 相关评价

- 自适应矩阵自动发现隐藏图结构，任意卷积学习时间关联

  > constructs a self-adaptive matrix to uncover unseen graph structures automatically from the data and WaveNet, which is based on causal convolutions, is used to learn temporal relations.  

- 训练后矩阵固定，不能适应动态的数据

  > However, the self-adaptive matrix in Graph WaveNet is fixed after training, which is unable to be adjusted dynamically with the data characteristics  

### T-36 T-GCN

CCF-B CI-2248 DT-2019 C-T-SPEED

#### 阅读笔记

##### 数据

SZ-taxi and Los-loop datasets.

> (1) SZ-taxi. This dataset was the taxi trajectory of Shenzhen from Jan. 1 to Jan. 31, 2015. We selected 156 major roads of Luohu District as the study area. The experimental data mainly includes two parts. One is an 156*156 adjacency matrix, which describes the spatial relationship between roads. Each row represents one road and the values in the matrix represent the connectivity between the roads. Another one is a feature matrix, which describes the speed changes over time on each road. Each row represents one road; each column is the traffic speed on the roads in different time periods. We aggregate the traffic speed on each road every 15 minutes. 
>
> (2) Los-loop. This dataset was collected in the highway of Los Angeles County in real time by loop detectors. We selected 207 sensors and its traffic speed from Mar.1 to Mar.7, 2012. We aggregated the traffic speed every 5 minutes. Similarity, the data concludes an adjacency matrix and a feature matrix. The adjacency matrix is calculated by the distance between sensors in the traffic networks. Since the Los-loop dataset contained some missing data, we used the linear interpolation method to fill missing values. 
>
> In the experiments,the input data was normalized to the interval [0,1]. In addition, 80% of the data was used as the training set and the remaining 20% was used as the testing set. We predicted the traffic speed of the next 15 minutes, 30 minutes, 45 minutes and 60 minutes.

#### 相关评价

GCN(空间) + GRU(时间)

used a Gated Recurrent Unit which takes input from a Graph Convolution Network and outputs the predicted traffic

## 杂鱼

### T-129 DCGCN

CCF-NONE CI-24 DT-2023 C-T-SPEED

#### 阅读笔记

我觉得作用不大，放表格里应该就行

##### 模型

动态贝叶斯网络，图卷积网络，动态任意图 (GCN+RNN)

In this work, we propose a novel approach for traffic prediction that embeds time-varying dynamic Bayesian network to capture the fine spatiotemporal topology of traffic data. 

We then use graph convolutional networks to generate traffic forecasts. 

To enable our method to efficiently model nonlinear traffic propagation patterns, we develop a deep learning-based module as a hyper-network to generate stepwise dynamic causal graphs

- we propose a novel causal-embedded approach for traffic prediction. It represents the spatiotemporal traffic network topology using a time-varying DBN (TVDBN), which is designed to adapt to the time-varying traffic propagation patterns by learning DBNs step by step
- The learned TVDBN is able to summarize the dynamic spatiotemporal dependencies between nodes. Built upon it, graph convolution is applied to capture spatial dependencies for traffic prediction.
- We propose a complete deep learning based causal structure learning module that serves as a pretrained hyper-network to generate the graphs of the TVDBN.
- The predefined distance graph is further incorporated into graph generator and traffic prediction module as additional prior information to improve the performance.
- We propose an approach based on GCN and RNN to learn a TVDBN that describes the time-varying causal relationships between different locations in traffic network. The augmented Lagrange method is applied for model training to ensure the acyclicity of the TVDBN



> poe assistant 概括：
>
> **时空特征处理：**
>
> - **动态贝叶斯网络 (DBN)：** 原文中提到 "We propose an approach based on GCN and RNN to learn a TVDBN that describes the time-varying causal relationships between different locations in traffic network." 以及 "The learned TVDBN is able to summarize the dynamic spatiotemporal dependencies between nodes."
> - **递归神经网络 (RNN)：** 原文提到 "We propose to capture the time-varying causal structure in traffic series by RNN, which generates the dynamic causal graphs stepwise from its hidden state." 以及 "The challenge is to construct suitable input features for RNN that contain information about the dynamics of the causal structure."
> - **图卷积网络 (GCN)：** 原文提到 "We extend the linear SEM (6) to a nonlinear version by spatial-based graph convolution [39], i.e., Xt = ∑Kk=0GCONV2L(Xt−k, Bkt) + Zt, where GCONV2L is a L layers spatial-based graph convolution operation with skip connections defined as:" 以及 "The proposed model is evaluated on a real-world traffic dataset to demonstrate the effectiveness of the proposed approach."
>
> **空间特征处理：**
>
> - **图卷积网络 (GCN)：** 原文提到 "We use an attention mechanism with multi-head scaled dot-product [38], to describe the spatiotemporal correlations between nodes, i.e., sdot(Q, K) = (QWQ)(KWK)⊤√d ∈ RN×N, msdot(Q, K) = [sdot1(Q, K), ...,sdoth(Q, K)] ∈ RN×N×h, C̃0t = msdot(Xt, Xt), C̃kt = msdot(Xt, Xt−k), k = 1, ..., K, where WQ and WK are learnable parameters of linear transformation, h is the number of heads and the learnable parameters of each head are different." 以及 "We further flatten the tensors {C̃kt}Kk=0 to matrices in RN2×h, denoted by {Ckt}Kk=0, such that each row of Ckt captures the dynamics of an element in Bkt."
> - **先验图结构：** 原文提到 "The predefined distance graph is further incorporated into graph generator and traffic prediction module as additional prior information to improve the performance." 以及 "We conduct experiments on a widely used traffic benchmark dataset METR-LA to evaluate the traffic prediction performance of the proposed model."
>
> 您是对的，文章中没有明确的“时间特征”处理部分



> Casual discovery 相关：
>
> Casual discovery aims to analyze causal relationships behind statistical correlations of different variables and facilitate better machine learning.
>
> - Typical approaches to incorporate causal discovery include encoding features from domain-specific causal models as input to downstream tasks
> - and learning the structure of causal relationships between features for graph-based models
> - As a powerful graph-based tool for modeling directed causal relationships between variables, Bayesian network (BN) is being applied in traffic prediction
>
> 文心一言：Casual Discovery（因果发现）是因果推理领域中的一个重要研究方向，旨在从观测数据中识别和理解变量之间的潜在因果结构或关系。

#### 数据

METR-LA

A limitation of current casual-embedded traffic prediction models is the assumption of stationary temporal dependencies. However, in reality, the dependencies of traffic data in different places do change over time

### T-131 STAWnet

CCF-NONE CI-93 DT-2021 C-T-SPEED

#### 阅读笔记

##### 模型

时间：CNN (Gated TCN) gated temporal convolution network

空间：自注意力网络 (DAN) dynamic attention network

a multi-step prediction model named Spatial-Temporal Attention Wavenet (STAWnet) is proposed. Temporal convolution is applied to handle long time sequences, and the dynamic spatial dependencies between different nodes can be captured using the self-attention network. Different from existing models, STAWnet does not need prior knowledge of the graph by developing a self-learned node embedding. These components are integrated into an end-to-end framework. 

- self-adaptive node embedding : capture the hidden spatial relationship in the data without knowing the graph structure information

##### 数据

The experimental results on three public traffic prediction datasets (METR-LA, PEMS-BAY, and PEMS07) demonstrate effectiveness.

实验对比提到 T-GCN, FC-LSTM，注意到这俩经常出现特别是 FC-LSTM

### T-132 ADN

CCF-NONE CI-6 DT-2022

#### 阅读笔记

注意力时间，注意力空间

we propose the simplest possible model, called ADN for Attention Diffusion Network, which does not rely on any structural prior whatsoever. We choose an attention based encoder-decoder architecture, where attention is adapted to the bi-dimensionality of events as location-instant pairs

> poe 找的：
>
> - 使用 多头注意力机制 (MHA)
>
>   The model alternates attention in the temporal dimension (MHA(T)), and in the spatial dimension (MHA(N)).
>
> - 论文中提到模型使用 可分离注意力机制 (separable attention)，该机制通过分别处理时间维度和空间维度，来降低注意力矩阵的计算量
>
>   Since attention is a generalised form of convolution [1], separability works all the same with attention, where it is also known as axial attention [7]. Separable attention processes event-indexed objects along each dimension of the events (temporal and spatial) separately and alternately, just as spatially separable convolutions alternate processing images along their width and height dimensions.

数据：

Experiments are conducted on three public traffic datasets PEMS-BAY, METR-LA and PEMS07 released by [12] and [20]. The first two are the most commonly used for measuring model performances, and consist of 207 and 325 locations, respectively. We also tested our model on PEMS07 (883 locations), to check its scalability to larger road traffic networks.