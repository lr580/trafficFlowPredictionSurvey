说明：

- 所有论文按自定义编号 **T-XXX**，编号对应表存储在 Zotero 里，自行查阅。收录具体实现论文，不包括综述。综述在 `内容整理.md` 主要记录。

- 引用量截止查询时间(按 2024 暑假算)，参考谷歌学术，按 **CI-xxx** 表示引用量。(CI: cite)

- CCF 评级按照 **CCF-xxx** 来。

- 年份/日期按照 **DT-yyyy** 来，或 DT-yyyy-mm。(DT: datetime)

- 论文简称写在标题

- 按照 **C-XXX** 给论文打标签。(C: category)

  > 具体标签有：-T (target) 预测目标(速度，流量等)

# 具体论文

火爆/经典的标准：引用量特别多或衍生模型特别多，其他是~~杂鱼~~新起

## 经典

### T-28 STGCN

CCF-A CI-3886 DT-2017 C-T-SPEED

#### 阅读笔记

##### 模型

两种 CNN 分别处理时间和空间特征

组成：两个时空卷积块和全连接层输出，每个块由两个时间卷积夹着一个空间卷积，使用了残差连接和瓶颈策略

> (GPT) 瓶颈策略的核心思想是通过引入某种形式的限制（如维度压缩、计算资源限制等），强制模型做出选择，聚焦于最重要的信息。这在提升模型的泛化能力、稳定性以及减少计算资源的使用等方面，都具有重要作用。

![image-20240815222203379](img/image-20240815222203379.png)

图2说明：

Architecture of spatio-temporal graph convolutional networks. The framework STGCN consists of two spatio-temporal convolutional blocks (ST-Conv blocks) and a fully-connected output layer in the end. Each ST-Conv block contains two temporal gated convolution layers and one spatial graph convolution layer in the middle. The residual connection and bottleneck strategy are applied inside each block.

超参数选择的方法：Tree-structured Parzen Estimator (TPE) (有论文)

##### 数据

研究交通速度

数据集是

自建：BJER4

- gathered from the major areas of east ring No.4 routes in Beijing City by double-loop detectors. There are 12 roads selected for our experiment. The traffic data are aggregated every 5 minutes. The time period used is from 1st July to 31st August, 2014 except the weekends. We select the first month of historical speed records as training set, and the rest serves as validation and test set respectively

标准：PeMSD7

- 使用加权矩阵，距离的分类讨论函数是边权

- 数据描述：(参考官方代码仓库 README.md)

  每天的记录次数乘以天数是行，道路数是列，内容是速度记录，节点数是 228 和 1026，行数是 12672(我算出来44天)，值是 60 多和 70 多的占大头，有极少其他值，预测取了 15/30/45min，预测视野越大误差越大

  - PeMSD7_V_{`$num_route`}.csv : Historical Speed Records with shape of `[len_seq * num_road] (len_seq = day_slot * num_dates)`.
  - PeMSD7_W_{`$num_route`}.csv : Weighted Adjacency Matrix with shape of `[num_road * num_road]`.

论文里有图表展示了有一个U形和双U形数据(蓝色是真实)

附录有很多张图表示实验结果

查阅得知，其中的 FC-LSTM 论文出处根本就不是这个领域的，而是该技术的经典论文或迁移应用，怀疑是自建模型

##### 代码

[src](https://github.com/VeritasYin/STGCN_IJCAI-18/tree/master)

#### 相关评价

T-ZS2 总结：

- 堆叠多个时空卷积块，每个块连接两个时间卷积层和一个图卷积层

  > stacks multiple spatio-temporal convolution blocks and each block concatenate two temporal convolution and one graph convolution layer.  

- 使用 ChebNet 作为图卷积操作，用一阶近似比较

  > ChebNet is chosen as the graph convolution operator in STGCN, after a comparison with its first-order approximation  

- CNN 代替 RNN 获取时间加快了训练时间

  > The usage of temporal convolution layers instead of RNNs for temporal modeling accelerates the training phase of STGCN  

#### 相关改版

- T-128 (改损失函数)

- T-81

### T-40 DCRNN

CCF-NONE CI-3654 DT-2018 C-T-SPEED

#### 阅读笔记

##### 模型

空间：图双向随机游走

时间：encoder-decoder

DCRNN captures the spatial dependency using bidirectional random walks on the graph, and the temporal dependency using the encoder-decoder architecture with scheduled sampling

- 空间

  We model the spatial dependency by relating traffic flow to a diffusion process, which explicitly captures the stochastic nature of traffic dynamics. This diffusion process is characterized by a random walk on G with restart probability, and a state transition matrix

- 时间

  We replace the matrix multiplications in GRU with the diffusion convolution

  In multiple step ahead forecasting, Both the encoder and the decoder are recurrent neural networks with Diffusion Convolutional Gated Recurrent Unit (DCGRU)

##### 数据

特征描述

![image-20240817003909585](img/image-20240817003909585.png)

Figure 1: Spatial correlation is dominated by road network structure. (1) Traffic speed in road 1 are similar to road 2 as they locate in the same highway. (2) Road 1 and road 3 locate in the opposite directions of the highway. Though close to each other in the Euclidean space, their road network distance is large, and their traffic speeds differ significantly.

数据集：交通速度预测

- METR-LA 感觉是完全数据集
- PEMS-BAY 该数据集的开山

#### 相关评价

(T-ZS1)

encoder-decoder 图扩散比 FNN, LSTM 好，消融证明扩散卷积更好

have demonstrated that their encoder-decoder model with graph diffusion managed to outperform simple FNN and LSTM. Not only that, they also performed an ablation test to demonstrate that their novel diffusion convolution module manages to outperform simpler variations 

GRU: 矩阵乘法改为扩散卷积操作

replaced the matrix multiplication inside Gated Recurrent Unit with the diffusion convolution operation 



(T-ZS2)

最出名的一个是 DCRNN

> (T-ZS2) The most famous one is diffusion convolutional recurrent neural network (DCRNN)  

- 图卷积网络和 RNN 学习时空

  > which uses diffusion graph convolutional networks and RNN to learn the representations of spatial dependencies and temporal relations  

- 最初用于交通速度预测，现在成为了基准模型

  > DCRNN was originally proposed for traffic speed forecasting and is now widely used as a baseline.  

- 建图方式：距离矩阵

  > To create the traffic graph, the adjacency matrix is defined as the thresholded pairwise road network distances.  

- 支持有向图，引入扩散卷积(DC)

  > Compared with other graph convolutional models that can only operate on undirected graphs, e.g., ChebNet, DCRNN introduces the diffusion convolution (DC) operation for directed graph and is more suitable for transportation scenarios, which is defined as follows:   

  $$
  \mathbf X_{*DC}=\sum_{k=0}^{K-1}(\theta_{k,1}(D_O^{-1}A)^k+\theta_{k,2}(D_I^{-1}A^T))\mathbf X
  $$

  其中 $\mathbf X\in R^{N\times d}$ 是节点特征矩阵，$A$ 是邻接矩阵，$D_O,D_I$ 是对角矩阵：出度和入度，$\theta$ 是两个模型参数，$K$ 是扩散步数。

  区分入度和出度实现了有向图 

  > where X 2 RN×d is the node feature matrix, A is the adjacency matrix, DO and DI are diagonal out-degree and in-degree matrices, θk;1 and θk;2 are model parameters, K is the number of diffusion steps. By defining and using out-degree and in-degree matrices, DCRNN models the bidirectional diffusion process to capture the influence of both upstream and downstream traffic  

- 对无向图不是很实用

  > While DCRNN is a strong baseline, it is not suitable or desirable for the undirected graph cases  

- 扩展版本 T-88：统一构建 RNN，基于任意图卷积

  Then DCRNN is extended with a stronger learning ability in graph GRU a unified method for constructing an RNN based on an arbitrary graph convolution operator is proposed, instead of the single RNN model used in DCRNN

观察图扩散过程和邻节点关联

observed the traffic diffusion along the road network and the correlation between several adjacent traffic sensors

### T-64 Graph WaveNet

CCF-A CI-1984 DT-2019 C-T-SPEED

#### 阅读笔记

##### 模型

空间：自适应依赖矩阵，点嵌入学习，图卷积

时间：堆叠扩张(如图2，倍增) 1D CNN 任意卷积

By developing a novel adaptive dependency matrix and learn it through node embedding, our model can precisely capture the hidden spatial dependency in the data. 

With a stacked dilated 1D convolution component whose receptive field grows exponentially as the number of layers increases, Graph WaveNet is able to handle very long sequences

- We propose a graph convolution layer in which a self-adaptive adjacency matrix can be learned from the data through an end-to-end supervised training
- we adopt stacked dilated casual convolutions to capture temporal dependencies. The receptive field size of stacked dilated casual convolution networks grows exponentially with an increase in the number of hidden layers

> (GPT) “end-to-end”（端到端）指的是一种模型训练方式，在这种方式中，模型从输入的原始数据开始，直接学习到最终输出的目标结果，中间的所有步骤都是在同一个模型中完成的

图3：

![image-20240817224043300](img/image-20240817224043300.png)

##### 数据

METR-LA PEMS-BAY

做了时间效率对比，看样子做了消融实验

静态图可能不能反应真实依赖

However, the explicit graph structure (relation) does not necessarily reflect the true dependency and genuine relation may be missing due to the incomplete connections in the data.

静态的局限性的例子

To give each circumstance an example, let us consider a recommendation system. In the first case, two users are connected, but they may have distinct preferences over products. In the second case, two users may share a similar preference

空间影响的直观图示

<img src="img/image-20240817220918943.png" alt="image-20240817220918943" style="zoom:67%;" />



#### 相关评价

- 自适应矩阵自动发现隐藏图结构，任意卷积学习时间关联

  > constructs a self-adaptive matrix to uncover unseen graph structures automatically from the data and WaveNet, which is based on causal convolutions, is used to learn temporal relations.  

- 训练后矩阵固定，不能适应动态的数据

  > However, the self-adaptive matrix in Graph WaveNet is fixed after training, which is unable to be adjusted dynamically with the data characteristics  

#### 相关改版

- T-136 STEP
- T-135 STD-MAE
- T-131 STAWnet
- T-130 微调
- T-128 GWNET-Cov (改损失函数)
- T-147 STWave

## 火爆

### T-110 AGCRN

CCF-A CI-1060 DT-2020 C-T-FLOW

#### 阅读笔记

##### 模型

空间：GCN 改进

时间：GRU

we propose two adaptive modules for enhancing Graph Convolutional Network (GCN) with new capabilities: 1) a Node Adaptive Parameter Learning (NAPL) module to capture node-specific patterns; 2) a Data Adaptive Graph Generation (DAGG) module to infer the inter-dependencies among different traffic series automatically. We further propose an Adaptive Graph Convolutional Recurrent Network (AGCRN) to capture fine-grained spatial and temporal correlations in traffic series automatically based on the two modules and recurrent networks.

- a Node Adaptive Parameter Learning (NAPL) module to learn node-specific patterns for each traffic series—NAPL factorizes the parameters in traditional GCN and generates node-specific parameters from a weights pool and bias pool shared by all nodes according to the node embedding
- a Data Adaptive Graph Generation (DAGG) module to infer the node embedding (attributes) from data and to generate the graph during training. NAPL and DAGG are independent and can be adapted to existing GCN-based traffic forecasting models both separately and jointly
- we combine NAPL and DAGG with recurrent networks and propose a unified traffic forecasting model - Adaptive Graph Convolutional Recurrent Network (AGCRN). AGCRN can capture fine-grained node-specific spatial and temporal correlations in the traffic series and unify the nodes embeddings in the revised GCNs with the embedding in DAGG

##### 数据

PEMSD4, PEMSD8

![image-20240819010446183](img/image-20240819010446183.png)

#### 相关改版

- T-151

### T-142 MTGNN

CCF-A CI-1294 DT-2020 C-T-SPEED

#### 阅读笔记

##### 模型

空间：图卷积

时间：1D CNN

Our approach automatically extracts the uni-directed relations among variables through a graph learning module, into which external knowledge like variable attributes can be easily integrated. A novel mix-hop propagation layer and a dilated inception layer are further proposed to capture the spatial and temporal dependencies within the time series. The graph learning, graph convolution, and temporal convolution modules are jointly learned in an end-to-end framework.

- we propose a novel graph learning layer, which extracts a sparse graph adjacency matrix adaptively based on data. Furthermore, we develop a graph convolution module to address the spatial dependencies among variables, given the adjacency matrix computed by the graph learning layer. This is designed specifically for directed graphs and avoids the over-smoothing problem that frequently occurs in graph convolutional networks
- Finally, we propose a temporal convolution module to capture temporal patterns by modified 1D convolutions. It can both discover temporal patterns with multiple frequencies and process very long sequences.

![image-20240819012120958](img/image-20240819012120958.png)

##### 数据

METR-LA PEMS-BAY 和其他交通外的数据，有输入长度表示为 12

### T-81 ASTGCN

CCF-A CI-2194 DT-2019 C-T-FLOW

#### 阅读笔记

##### 模型

空间：图卷积+注意力

时间：注意力+CNN

In this paper, we propose a novel attention based spatial-temporal graph convolutional network (ASTGCN) model to solve traffic flow forecasting problem

- ASTGCN mainly consists of three independent components to respectively model three temporal properties of traffic flows, i.e., recent, daily-periodic and weekly-periodic dependencies.

- each component contains two major parts: 

  \1 the spatial-temporal attention mechanism to effectively capture the dynamic spatialtemporal correlations in traffic data; 

  \2 the spatial-temporal convolution which simultaneously employs graph convolutions to capture the spatial patterns and common standard convolutions to describe the temporal features

- The output of the three components are weighted fused to generate the final prediction results

![image-20240819145531119](img/image-20240819145531119.png)

- After the graph convolution operations having captured neighboring information for each node on the graph in the spatial dimension, a standard convolution layer in the temporal dimension is further stacked to update the signal of a node by merging the information at the neighboring time slice

##### 数据

PEMSD4 PEMSD8 头50天训练，12天测试，进行了流量速度和占用率预测

There are three kinds of traffic measurements considered in our experiments, including total flow, average speed, and average occupancy

#### 相关评价

变种 ASTGCN 引入两个注意力层分别获取时空动态联系

> Attention based Spatio-temporal graph convolutional network (ASTGCN) further introduces two attention layers in STGCN to capture the dynamic correlations in spatial dimension and temporal dimension, respectively  

#### 相关改版

- T-151

### T-36 T-GCN

CCF-B CI-2248 DT-2019 C-T-SPEED

#### 阅读笔记

##### 数据

SZ-taxi and Los-loop datasets.

> (1) SZ-taxi. This dataset was the taxi trajectory of Shenzhen from Jan. 1 to Jan. 31, 2015. We selected 156 major roads of Luohu District as the study area. The experimental data mainly includes two parts. One is an 156*156 adjacency matrix, which describes the spatial relationship between roads. Each row represents one road and the values in the matrix represent the connectivity between the roads. Another one is a feature matrix, which describes the speed changes over time on each road. Each row represents one road; each column is the traffic speed on the roads in different time periods. We aggregate the traffic speed on each road every 15 minutes. 
>
> (2) Los-loop. This dataset was collected in the highway of Los Angeles County in real time by loop detectors. We selected 207 sensors and its traffic speed from Mar.1 to Mar.7, 2012. We aggregated the traffic speed every 5 minutes. Similarity, the data concludes an adjacency matrix and a feature matrix. The adjacency matrix is calculated by the distance between sensors in the traffic networks. Since the Los-loop dataset contained some missing data, we used the linear interpolation method to fill missing values. 
>
> In the experiments,the input data was normalized to the interval [0,1]. In addition, 80% of the data was used as the training set and the remaining 20% was used as the testing set. We predicted the traffic speed of the next 15 minutes, 30 minutes, 45 minutes and 60 minutes.

#### 相关评价

GCN(空间) + GRU(时间)

used a Gated Recurrent Unit which takes input from a Graph Convolution Network and outputs the predicted traffic

### T-143 GMAN

CCF-A CI-1262 DT-2019

#### 阅读笔记

##### 模型

encoder-decoder 时空都用注意力

In this paper, we focus on the spatio-temporal factors, and propose a graph multi-attention network (GMAN) to predict traffic conditions for time steps ahead at different locations on a road network graph. GMAN adapts an encoder-decoder architecture, where both the encoder and the decoder consist of multiple spatio-temporal attention blocks to model the impact of the spatio-temporal factors on traffic conditions. The encoder encodes the input traffic features and the decoder predicts the output sequence. Between the encoder and the decoder, a transform attention layer is applied to convert the encoded traffic features to generate the sequence representations of future time steps as the input of the decoder. The transform attention mechanism models the direct relationships between historical and future time steps that helps to alleviate the error propagation problem among prediction time steps

![image-20240819013039961](img/image-20240819013039961.png)

##### 数据

traffic volume prediction on the Xiamen dataset

which contains 5 months of data recorded by 95 traffic sensors ranging from August 1st, 2015 to December 31st, 2015 in Xiamen, China

以及 PEMS-BAY

![image-20240819013254073](img/image-20240819013254073.png)

### T-87 STSGCN

CCF-A CI-1038 DT-2020 C-T-FLOW

#### 阅读笔记

##### 模型

时空联系可以获取，认为是 GNN+CNN 解决的。

Spatial-Temporal Synchronous Graph Convolutional Networks (STSGCN)

- he STSGCN model can simultaneously capture the localized spatial-temporal correlations directly, instead of using different types of deep neural networks to model the spatial dependencies and temporal correlations separately. Specifically, we construct localized spatial-temporal graphs which connect individual spatial graphs of adjacent time steps into one graph
- Then we construct a Spatial-Temporal Synchronous Graph Convolutional Module (STSGCM) to capture the complex localized spatial-temporal correlations in these localized spatial-temporal graphs
- Meanwhile, to capture the heterogeneity in long-range spatial-temporal network data, we design a Spatial-Temporal Synchronous Graph Convolutional Layer (STSGCL), which deploys multiple individual STSGCMs on different time periods. Finally, we stack multiple STSGCLs to aggregate long-range spatial-temporal correlations and heterogeneity for prediction.

![image-20240819154032938](img/image-20240819154032938.png)

![image-20240819154119662](img/image-20240819154119662.png)

##### 数据

PEMS03/4/7/8

#### 相关评价

T-ZS2 邻接矩阵+局部时空图的 GCN

For example, the localized spatio-temporal correlation information is extracted simultaneously with the adjacency matrix of localized spatio-temporal graph, in which a localized spatio-temporal graph that includes both temporal and spatial attributes is constructed first and a spatial-based GCN method is applied then

### T-146 STFGNN

CCF-A CI-641 DT-2021 C-T-FLOW

#### 阅读笔记

##### 模型

思想：把时间依赖也做成图，解决时空依赖

时间：CNN (Gated dilated CNN)

空间：GNN 图卷积

To overcome those limitations, our paper proposes a novel Spatial-Temporal Fusion Graph Neural Networks (STFGNN) for traffic flow forecasting

- First, a data-driven method of generating “temporal graph” is proposed to compensate several existing correlations that spatial graph may not reflect
- SFTGNN could effectively learn hidden spatial-temporal dependencies by a novel fusion operation of various spatial and temporal graphs, treated for different time periods in parallel
- Meanwhile, by integrating this fusion graph module and a novel gated convolution module into a unified layer, SFTGNN could handle long sequences by learning more spatial-temporal dependencies with layers stacked

<img src="img/image-20240819234537669.png" alt="image-20240819234537669" style="zoom: 67%;" />

we propose a novel data-driven method for graph construction: the temporal graph learned based on similarities between time series. Then several graphs could be integrated as a spatial-temporal fusion graph to obtain hidden spatial-temporal dependencies. Moreover, to break the local and global correlation tradeoff, gated dilated convolution module is introduced, whose larger dilation rate could capture long-range dependencies

- We construct a novel graph by a data-driven method, which preserve hidden spatial-temporal dependencies. This data-driven adjacency matrix is able to extract correlations that given spatial graph may not present. Then, we propose a novel spatial-temporal fusion graph module to capture spatial-temporal dependencies synchronously.
- We propose an effective framework to capture local and global correlations simultaneously, by assembling a Gated dilated CNN module with spatial-temporal fusion graph module in parallel. Long-range spatial-temporal dependencies could also be extracted with layers stacked

![image-20240819235730314](img/image-20240819235730314.png)

Figure 3: Detailed framework of STFGNN. (a) is the example of input of Spatial-Temporal Fusion Graph, which would be generated iteratively along the time axis. (b) is the example of Spatial-Temporal Fusion Graph, whose size K is 4 and 3, respectively. It consists of three kinds of adjacency matrix ∈ N × N : spatial graph ASG, temporal graph AT G and temporal connectivity graph AT C . The AT C within a red circle would be taken for instance in the body. (c) is overall structure of STFGNN, its Gated CNN module and STFGNN modules are in parallel. (d) is detailed architecture of the Spatial-Temporal Fusion Graph Modules, each module will be independently trained for input iteratively generated from (a) in parallel as well

##### 数据

12 步预测 12 步，PEMS03/4/7/8

### T-85 ST-UNet

CCF-NONE CI-70 DT-2019 C-T-SPEED

#### 阅读笔记

##### 模型

空间：GNN

时间：RNN(大概)

To tackle this problem, we design a novel multi-scale architecture, Spatio-Temporal U-Net (ST-UNet), for graph-structured time series modeling. In this U-shaped network, a paired sampling operation is proposed in spacetime domain accordingly: the pooling (ST-Pool) coarsens the input graph in spatial from its deterministic partition while abstracts multi-resolution temporal dependencies through dilated recurrent skip connections; based on previous settings in the downsampling

- ST-UNet employs multi-granularity graph convolution for extracting both generalized and localized spatial features, 

  and adds dilated recurrent skip-connections for capturing multi-resolution temporal dependencies

火爆是因为后面很多模型貌似都用它作基准测试

##### 数据

METR-LA PEMS M/L

### T-41 hybrid Seq2Seq

CCF-A CI-235 DT-2018 C-T-SPEED

#### 阅读笔记

##### 模型

空间：图卷积

时间：疑似 LSTM

encoder-decoder

In this paper, we intend to improve traffic prediction by appropriate integration of three kinds of implicit but essential factors encoded in auxiliary information. We do this within an encoder-decoder sequence learning framework

- a graph convolution neural network is used to learn the spatial correlation of road segments

##### 数据

自创

we release a large-scale traffic dataset from Baidu Map, the Q-Traffic dataset, which provides various offline and online auxiliary information along with traffic speed data. There are three kinds of auxiliary domains in the Q-Traffic dataset: 1) Offline geographical and social attributes which include public holidays, peak-hour, speed etc; 2) the road intersection information such as local road network and junctions; and 3) online crowd queries which record map search queries from users.

Road segments 15,073 

Total length 738.91 km 

Interval 15 minutes 

Time April 1, 2017 - May 31, 2017 

Total records 265,967,808 

lon/lat bounding box (116.10, 39.69, 116.71, 40.18)

列表对比了其他数据集，可以参考还有什么别的数据集

## 新起

### T-128 GWNet-Cov

CCF-A CI-18 DT-2021 C-T-SPEED C-T-FLOW

#### 阅读笔记

##### 模型

引入新的损失函数

We introduce a novel loss function, Covariance Loss, which is conceptually equivalent to conditional neural processes and has a form of regularization so that is applicable to many kinds of neural networks. With the proposed loss, mappings from input variables to target variables are highly affected by dependencies of target variables as well as mean activation and mean dependencies of input and target variables. This nature enables the resulting neural networks to become more robust to noisy observations and recapture missing dependencies from prior information.

使用了 STGCN 和 Graph WaveNet 做改进

To demonstrate the validity of the proposed Covariance Loss, we employ neural networks that are designed to explicitly consider the spatial and temporal dependencies, namely, spatio-temporal graph convolutional network (STGCN) and graph wavenet (GWNET)

##### 数据

把 STGCN 用到了 PEMSD7M，过去 60 min 数据预测未来 15-60min 数据

把 GWNET 用到了 METR-LA 和 PEMS-BAY，后者过去 60 预测未来 60

#### 相关评价

### T-129 DCGCN

CCF-NONE CI-24 DT-2023 C-T-SPEED

#### 阅读笔记

我觉得作用不大，放表格里应该就行

##### 模型

动态贝叶斯网络，图卷积网络，动态任意图 (GCN+RNN)

In this work, we propose a novel approach for traffic prediction that embeds time-varying dynamic Bayesian network to capture the fine spatiotemporal topology of traffic data. 

We then use graph convolutional networks to generate traffic forecasts. 

To enable our method to efficiently model nonlinear traffic propagation patterns, we develop a deep learning-based module as a hyper-network to generate stepwise dynamic causal graphs

- we propose a novel causal-embedded approach for traffic prediction. It represents the spatiotemporal traffic network topology using a time-varying DBN (TVDBN), which is designed to adapt to the time-varying traffic propagation patterns by learning DBNs step by step
- The learned TVDBN is able to summarize the dynamic spatiotemporal dependencies between nodes. Built upon it, graph convolution is applied to capture spatial dependencies for traffic prediction.
- We propose a complete deep learning based causal structure learning module that serves as a pretrained hyper-network to generate the graphs of the TVDBN.
- The predefined distance graph is further incorporated into graph generator and traffic prediction module as additional prior information to improve the performance.
- We propose an approach based on GCN and RNN to learn a TVDBN that describes the time-varying causal relationships between different locations in traffic network. The augmented Lagrange method is applied for model training to ensure the acyclicity of the TVDBN



> poe assistant 概括：
>
> **时空特征处理：**
>
> - **动态贝叶斯网络 (DBN)：** 原文中提到 "We propose an approach based on GCN and RNN to learn a TVDBN that describes the time-varying causal relationships between different locations in traffic network." 以及 "The learned TVDBN is able to summarize the dynamic spatiotemporal dependencies between nodes."
> - **递归神经网络 (RNN)：** 原文提到 "We propose to capture the time-varying causal structure in traffic series by RNN, which generates the dynamic causal graphs stepwise from its hidden state." 以及 "The challenge is to construct suitable input features for RNN that contain information about the dynamics of the causal structure."
> - **图卷积网络 (GCN)：** 原文提到 "We extend the linear SEM (6) to a nonlinear version by spatial-based graph convolution [39], i.e., Xt = ∑Kk=0GCONV2L(Xt−k, Bkt) + Zt, where GCONV2L is a L layers spatial-based graph convolution operation with skip connections defined as:" 以及 "The proposed model is evaluated on a real-world traffic dataset to demonstrate the effectiveness of the proposed approach."
>
> **空间特征处理：**
>
> - **图卷积网络 (GCN)：** 原文提到 "We use an attention mechanism with multi-head scaled dot-product [38], to describe the spatiotemporal correlations between nodes, i.e., sdot(Q, K) = (QWQ)(KWK)⊤√d ∈ RN×N, msdot(Q, K) = [sdot1(Q, K), ...,sdoth(Q, K)] ∈ RN×N×h, C̃0t = msdot(Xt, Xt), C̃kt = msdot(Xt, Xt−k), k = 1, ..., K, where WQ and WK are learnable parameters of linear transformation, h is the number of heads and the learnable parameters of each head are different." 以及 "We further flatten the tensors {C̃kt}Kk=0 to matrices in RN2×h, denoted by {Ckt}Kk=0, such that each row of Ckt captures the dynamics of an element in Bkt."
> - **先验图结构：** 原文提到 "The predefined distance graph is further incorporated into graph generator and traffic prediction module as additional prior information to improve the performance." 以及 "We conduct experiments on a widely used traffic benchmark dataset METR-LA to evaluate the traffic prediction performance of the proposed model."
>
> 您是对的，文章中没有明确的“时间特征”处理部分



> Casual discovery 相关：
>
> Casual discovery aims to analyze causal relationships behind statistical correlations of different variables and facilitate better machine learning.
>
> - Typical approaches to incorporate causal discovery include encoding features from domain-specific causal models as input to downstream tasks
> - and learning the structure of causal relationships between features for graph-based models
> - As a powerful graph-based tool for modeling directed causal relationships between variables, Bayesian network (BN) is being applied in traffic prediction
>
> 文心一言：Casual Discovery（因果发现）是因果推理领域中的一个重要研究方向，旨在从观测数据中识别和理解变量之间的潜在因果结构或关系。

#### 数据

METR-LA

A limitation of current casual-embedded traffic prediction models is the assumption of stationary temporal dependencies. However, in reality, the dependencies of traffic data in different places do change over time

### T-131 STAWnet

CCF-NONE CI-93 DT-2021 C-T-SPEED

#### 阅读笔记

##### 模型

时间：CNN (Gated TCN) gated temporal convolution network

空间：自注意力网络 (DAN) dynamic attention network

a multi-step prediction model named Spatial-Temporal Attention Wavenet (STAWnet) is proposed. Temporal convolution is applied to handle long time sequences, and the dynamic spatial dependencies between different nodes can be captured using the self-attention network. Different from existing models, STAWnet does not need prior knowledge of the graph by developing a self-learned node embedding. These components are integrated into an end-to-end framework. 

- self-adaptive node embedding : capture the hidden spatial relationship in the data without knowing the graph structure information

##### 数据

The experimental results on three public traffic prediction datasets (METR-LA, PEMS-BAY, and PEMS07) demonstrate effectiveness.

实验对比提到 T-GCN, FC-LSTM，注意到这俩经常出现特别是 FC-LSTM

### T-132 ADN

CCF-NONE CI-6 DT-2022

#### 阅读笔记

注意力时间，注意力空间

we propose the simplest possible model, called ADN for Attention Diffusion Network, which does not rely on any structural prior whatsoever. We choose an attention based encoder-decoder architecture, where attention is adapted to the bi-dimensionality of events as location-instant pairs

> poe 找的：
>
> - 使用 多头注意力机制 (MHA)
>
>   The model alternates attention in the temporal dimension (MHA(T)), and in the spatial dimension (MHA(N)).
>
> - 论文中提到模型使用 可分离注意力机制 (separable attention)，该机制通过分别处理时间维度和空间维度，来降低注意力矩阵的计算量
>
>   Since attention is a generalised form of convolution [1], separability works all the same with attention, where it is also known as axial attention [7]. Separable attention processes event-indexed objects along each dimension of the events (temporal and spatial) separately and alternately, just as spatially separable convolutions alternate processing images along their width and height dimensions.

数据：

Experiments are conducted on three public traffic datasets PEMS-BAY, METR-LA and PEMS07 released by [12] and [20]. The first two are the most commonly used for measuring model performances, and consist of 207 and 325 locations, respectively. We also tested our model on PEMS07 (883 locations), to check its scalability to larger road traffic networks.

### T-133 RGDAN

CCF-B CI-3 DT-2024 C-T-SPEED

#### 阅读笔记

##### 模型 

空间：GAT(Graph Attention Network, 图注意力网络)

时间：注意力

看论文图还有 encoder-decoder

we propose a Random Graph Diffusion Attention Network (RGDAN) for traffic prediction. RGDAN comprises a graph diffusion attention module and a temporal attention module. The graph diffusion attention module can adjust its weights by learning from data like a CNN to capture more realistic spatial dependencies. The temporal attention module captures the temporal correlations

- Random GATs discard traditional attention interaction (e.g., dot product) and instead, use a learnable matrix to discover the correlations between neighbors. This method omits the process of dot-product interaction in ordinary attention, reduces the time complexity, improves the computational speed of the model, and reduces the memory overhead
- Random Graph Diffusion Attention Network (RGDAN), consists of an attention mechanism with a graph diffusion attention module that extracts the spatial correlations. Temporal attention captures the temporal dependencies. The temporal and spatial features are fused by a gated fusion layer. The transformed attention module is used to alleviate the propagation error caused by the difference between the predicted and historical time steps

![image-20240818142743477](img/image-20240818142743477.png)

- A spatio-temporal embedding (STE) generator, which provides preliminary spatio-temporal information to the encoder and decoder by generating spatio-temporal embeddings

##### 数据

METR-LA PEMS-BAY NE-BJ

- NE-BJ (T-134 是鼻祖)

  This dataset is a collection of public transport speed data taken from the navigation data of Tencent Map. It contains 500 segments, with each road segment data contains 6509 5-min time steps.

##### 其他

提供了一个表格总结了用到的对比实验的相关论文

### T-134 DGCRN

CCF-B CI-298 DT-2021

#### 阅读笔记

##### 模型

encoder-decoder

空间：GNN 图卷积

时间：RNN

we propose a novel traffic prediction framework, named Dynamic Graph Convolutional Recurrent Network (DGCRN)

- In DGCRN, hyper-networks are designed to leverage and extract dynamic characteristics from node attributes, while the parameters of dynamic filters are generated at each time step. We filter the node embeddings and then use them to generate dynamic graph, which is integrated with pre-defined static graph
- We propose a GNN and RNN based model, where the dynamic adjacency matrix is designed to be generated from a hyper-network step by step synchronize with the iteration of RNN. The dynamic graph is incorporated with the pre-defined graph and skip connection to describe the dynamic characteristics of road networks more effectively, enhancing the performance of prediction.

![image-20240819020113960](img/image-20240819020113960.png)

Dynamic Graph Convolutional Recurrent Module (DGCRM)

![image-20240819020343932](img/image-20240819020343932.png)

##### 数据

METR-LA PEMS-BAY

NE-BJ



### T-135 STD-MAE

CCF-NONE CI-3 DT-2023 C-T-SPEED

#### 阅读笔记

##### 模型

时空都是 AE + 自注意力

self-supervised pre-training framework SpatialTemporal-Decoupled Masked Pre-training (STDMAE) that employs two decoupled masked autoencoders to reconstruct spatiotemporal series along the spatial and temporal dimensions

mask 的思想：学习填充缺失。基于这个进行预训练，有详细介绍

- The core idea is to mask parts of the input sequence during pre-training, requiring the model to reconstruct the missing contents
- spatial-temporal-decoupled masking, Such decoupled masking mechanism allows the model to learn representation that can capture clearer heterogeneity
- It consists of a temporal autoencoder (T-MAE) and a spatial autoencoder (S-MAE), both having a similar architecture
- S-MAE applies self-attention along spatial dimension, while T-MAE performs self-attention along temporal dimension

杂项：

- patch embedding technique: The long input is divided into non-overlapping patches
- Moreover, to simultaneously encode spatial and temporal positional information, we implement a two-dimensional positional encoding

模型结构：

- The spatial and temporal decoders each consists of a padding layer, a standard transformer layer, and a regression layer.

![image-20240818161239792](img/image-20240818161239792.png)

- 具体模型：GWNet (Graph WaveNet)

##### 数据

PEMSD3/4/7/8/BAY, METR-LA

### T-136 MegaCRN

CCF-A CI-96 DT-2023 C-T-SPEED

#### 阅读笔记

##### 模型

空间 GCN( Graph Convolutional Networks (GCNs) )

时间 Gated Recurrent Unit (GRU)

we implement this idea into Meta-Graph Convolutional Recurrent Network (MegaCRN) by plugging the Meta-Graph Learner powered by a MetaNode Bank into GCRN encoder-decoder.

> Graph Structure Learning (GSL)
>
> spatio-temporal graph (STG)

The term meta-graph is coined to describe the generation of node embeddings (similar in adaptive and momentary) for GSL.

our STG learning consists of two steps: (1) querying node-level prototypes from a Meta-Node Bank; (2) reconstructing node embeddings with Hyper-Network

injecting graph convolution operation into recurrent cell (e.g. LSTM). The derived Graph Convolutional Recurrent Unit (GCRU) can thereby simultaneously capture

- spatial dependency, represented by an input graph topology, and
- temporal dependency in a sequential manner.

##### 数据

METR-LA PEMS-BAY EXPR-KEY (自建)

2021/10/1 - 12/31, 10min 时间区间，13248 时间步，1843 条道路连接，速度

contains the traffic speed information and the corresponding traffic incident information in 10-minute interval for 1843 expressway road links in Tokyo over three months (2021/10∼2021/12).

##### 其他

经验准则 Tobler’s first law of geography

> 托布勒第一地理学定律指出：“事物彼此靠近”。换句话说，地理上相近的事物比相距较远的事物更相似。
>
> 这个定律在许多领域都有应用，包括：
>
> - 地理学： 理解空间模式和地理现象之间的关系。
> - 城市规划： 规划城市基础设施和服务，以满足居民的需求。
> - 环境科学： 研究污染物和生态系统之间的关系。
> - 社会学： 分析社会现象的空间分布。
>
> 托布勒第一地理学定律是一个重要的概念，它提醒我们地理位置在塑造世界中起着至关重要的作用。

### T-137 STEP

CCF-A CI-146 DT-2022 C-T-SPEED C-T-FLOW

#### 阅读笔记

##### 模型

时间：Transformer

空间：图

以及 encoder-decoder，但是基于 Grah WaveNet 改编

we propose a novel framework, in which <u>ST</u>GNN is <u>E</u>nhanced by a scalable time series <u>P</u>re-training model (STEP). 

Specifically, we design a pre-training model to efficiently learn temporal patterns from very long-term history time series (e.g., the past two weeks) and generate segment-level representations. 

These representations provide contextual information for short-term time series input to STGNNs and facilitate modeling dependencies between time series

- Specifically, we design an efficient unsupervised pre-training model for <u>T</u>ime <u>S</u>eries based on Trans<u>Former</u> blocks (TSFormer)
- which is trained through the masked autoencoding strategy
- design a graph structure learner based on the representation of TSFormer, which learns discrete dependency graph and utilizes the 𝑘NN graph computed based on the representation of TSFormer as a regularization to guide the joint training of graph structure and STGNN

![image-20240818173141567](img/image-20240818173141567.png)

STEP framework can extend to almost any STGNN, and we choose a representative method as our backend, the Graph WaveNet

##### 数据

METR-LA PEMS-BAY PEMS04(flow)

### T-138 D2STGNN

CCF-NONE CI-140 DT-2022 C-T-SPEED C-T-FLOW

#### 阅读笔记

##### 模型

空间：GNN + 自注意力

时间：RNN + 自注意力

to improve modeling performance, we propose a novel Decoupled Spatial-Temporal F ramework (DSTF) that separates the diffusion and inherent traffic information in a data-driven manner, which encompasses a unique estimation gate and a residual decomposition mechanism

- The former (residual) removes the parts of signals that the diffusion and inherent models can approximate well. Thus, the parts of signals that are not learned well is retained. 
- The latter (gate) estimates roughly the proportion of the two kinds of signals to relieve the burden of the first model in each layer, which takes the original signal as input and needs to learn specific parts in it

we propose an instantiation of DSTF, Decoupled Dynamic Spatial-Temporal Graph Neural Network (D2STGNN), that captures spatial-temporal correlations and also features a dynamic graph learning module that targets the learning of the dynamic characteristics of traffic networks

- spatial dependency by learning latent correlations between time series based on the self-attention mechanism.
- A spatial-temporal localized convolution is designed to model the hidden diffusion time series. A recurrent neural network and self-attention mechanism are used jointly to model the hidden inherent time series.

##### 数据

METR-LA PEMS-BAY 速度

PEMS04 PEMS08 流量

### T-139 STAEformer

CCF-B CI-57 DT-2023 C-T-SPEED C-T-FLOW

#### 阅读笔记

##### 模型

时空自适应嵌入 + 朴素 transformer

时空都是自注意力层，如图所示，嵌入可以额外融入日期等信息

In this study, we present a novel component called spatio-temporal adaptive embedding that can yield outstanding results with vanilla transformers. 

Our proposed Spatio-Temporal Adaptive Embedding transformer (STAEformer)

- Specifically, it adds an embedding layer on the input, providing multiple types of embeddings for the model backbone

![image-20240818223717438](img/image-20240818223717438.png)

##### 数据

METR-LA, PEMS-BAY, PEMS03/4/7/8

### T-140 SLCNN

CCF-A CI-234 DT-2020 T-C-SPEED

#### 阅读笔记

##### 模型

空间：CNN + GNN

时间：CNN

we propose a novel framework named Structure Learning Convolution (SLC) that enables to extend the traditional convolutional neural network (CNN) to graph domains and learn the graph structure for traffic forecasting

- We propose a generic graph convolutional formulation, which defines convolution operation as a combination of structure module and kernel module
- Two data-driven and time-vary SLC modules are proposed to capture the global and local structures, respectively
- P3D ConvNet is incorporated in SLCNN to capture the temporal dependencies in traffic data

##### 数据

PeMS-S (找不到来源)

The time range of PeMS-S is the weekdays of May and Jun of 2012, the interval is 5 minute and 228 sensors (nodes) are selected

PEMS-BAY, METR-LA 

BJF, BRF, BRF-L 自建

- are generated from a real-world GPS trajectory data, in which about 80 million GPS points of 30,000 taxis are recoded per day
- BJF. Each node in BJF indicates a junction and 190 important junctions in Beijing are selected. The traffic data of each node is recorded in every 15 minutes and the time range is from November 2015 to October 2016. 
- BRF. BRF has totally 300 nodes and each node indicates a road. The time interval is set to 20 minutes and the time period used of BRF is from November 2015 to May 2016. 
- BRF-L. Similar to BRF, the traffic flow of each road is recoded in the dataset. BRF-L contains 1586 roads and the time interval is set to 10 minutes. The time period used of BRF-L is from November 2015 to December 2015.

#### 相关评价

T-ZS2 有数据，但没论述

### T-74 Traffic Transformer

CCF-NONE CI-277 DT-2020 C-T-SPEED

#### 阅读笔记

##### 模型

we propose to design different strategies for encoding temporal information so thatboth the continuity and periodicity of traffic data can be preserved, and extend Transformer to modeling temporaland spatial dependencies jointly with the help of graph convolutional networks (GCNs)

- We design four novel position encoding strategies to encode the continuity and periodicity of time seriesto facilitate the modeling of temporal dependencies in traffic data
- We introduce a hybrid encoder–decoder architecture, called Traffic Transformer, to coherently model spatial and temporal dependencies of traffic data in an end-to-end training manner, where Transformer is leveraged to model temporal dependencies and GCNs contribute to the modeling of spatial dependencies

![image-20240818235942477](img/image-20240818235942477.png)

![image-20240818235952372](img/image-20240818235952372.png)

##### 数据

METR-LA PEMS-BAY



### T-141 STGM

CCF-C CI-24 DT-2023 C-T-SPEED

#### 阅读笔记

##### 模型

时空依赖：注意力

空间：GNN

时间：CNN

we propose a Spatio-Temporal Graph Mixformer (STGM) network, a highly optimized model with low memory footprint. We address the aforementioned limits by utilizing a novel attention mechanism to capture the correlation between temporal and spatial dependencies. Specifically, we use convolution layers with a variable fields of view for each head to capture long–short term temporal dependency. Additionally, we train an estimator model that express the contribution of a node over the desired prediction. The estimation is fed alongside a distance matrix to the attention mechanism. Meanwhile, we use a gated mechanism and a mixer layer to further select and incorporate the different perspectives.

- We suggest a similarity estimator model that given a set of nodes with a segment of historical traffic, approximates the expected implication of each node over the future traffic signal. 
- Furthermore, we designed a couple of improvements for the original transformer architecture named STGA that fully utilizes the multi-head attention and seamlessly capture both temporal and spatial dependencies. 
- We propose the CT-Mixer module that works in coordination with STGA to further capture the temporal locality and internode channels information

有一段描述 CNN 代替 RNN 的描述，说不定可以参考

![image-20240819004354922](img/image-20240819004354922.png)

##### 数据

PEMS-BAY METR-LA PEMSD7M(参考T-28)

### T-145 STGODE

CCF-A CI-322 DT-2021 C-T-FLOW

#### 阅读笔记

##### 模型

时空：ODE

空间：邻接矩阵+GNN

时间：CNN (TCN temporal dilation convolution)

Spatial-Temporal Graph Ordinary Differential Equation Networks

Specifically, we capture spatial-temporal dynamics through a tensor-based ordinary differential equation (ODE), as a result, deeper networks can be constructed and spatial-temporal features are utilized synchronously

To understand the network more comprehensively, semantical adjacency matrix is considered in our model, and a well-design temporal dilated convolution structure is used to capture long term temporal dependencies

- First, in order to depict spatial correlations from both geographical and semantic views, we construct two types of adjacency matrices, i.e. spatial adjacency matrix and semantic adjacency matrix, based on spatial connectivity and semantical similarity of traffic flow respectively

- residual connections are added between layers to alleviate the over-smoothing problem. Furthermore, it is proved that the discrete layers with residual connections can be viewed as a discretization of an Ordinary Differential Equation (ODE)

- and so a continuous graph neural network (CGNN) is derived

  a continuous GNN with residual connections is introduced to avoid the over-smoothing problem and hence be able to model long-range spatial-temporal dependencies. Last but not least, a spatial-temporal tensor is constructed to consider spatial and temporal patterns simultaneously and model complex spatial-temporal interactions

![image-20240819161324050](img/image-20240819161324050.png)

##### 数据

PeMSD7(M), PeMSD7(L), PeMS03, PeMS04, PeMS07, and PeMS08 过去一个小时预测未来一个小时 

#### 相关改版

- T-155
- T-156

### T-147 STWave

CCF-A CI-26 DT-2023 C-T-FLOW

#### 阅读笔记

##### 模型

时间：任意卷积+注意力

空间：SOTA 全 GAT

整体：encoder-decoder

we aim to provide a novel disentangle-fusion framework STWave to mitigate the distribution shift issue. The framework first decouples the complex traffic data into stable trends and fluctuating events, followed by a dual-channel spatio-temporal network to model trends and events, respectively

Finally, reasonable future traffic can be predicted through the fusion of trends and events. Besides, we incorporate a novel query sampling strategy and graph wavelet-based graph positional encoding into the full graph attention network to efficiently and effectively model dynamic spatial correlations

![image-20240820015255577](img/image-20240820015255577.png)

The end-to-end and our proposed disentangle-fusion traffic forecasting framework, where STNet denotes the spatiotemporal network.

- a dual-channel spatio-temporal network to capture the dualscale temporal changes and spatial correlations.

- Following this principle, we propose a novel traffic forecasting framework named STWave, which first applies the discrete wavelet transform (DWT) to disentangle the traffic time series into the dual-scale trend-event representations because DWT can decompose data into various components, such as the main information (i.e., trend information) and details (i.e., event information)

- STNet that utilizes corresponding sequential and graph-based methods on the different information to capture the various temporal changes and spatial correlations

  the causal convolution with small kernel size, temporal attention with the global temporal receptive field, and the state-of-the-art full GAT are adopted on events, trends, and both of them to capture fluctuating temporal changes, stable temporal changes, and different temporal changes-based dynamic global spatial correlations, respectively

- Moreover, to address the high complexity and insufficient structure information in the full GAT, a novel query sampling strategy and a novel graph wavelet-based graph positional encoding are proposed in STWave. The query sampling strategy reduces the complexity while maintaining the global receptive field according to the hierarchical nature of the traffic system

  and the graph wavelet-based graph positional encoding brings the local-global balanced structure information based on the spectral graph theory

- Finally, an adaptive event fusion module is used in STWave to merge useful information from inaccurate forecast events into easily predict trends

![image-20240820015920131](img/image-20240820015920131.png)

##### 数据

PeMSD3, PeMSD4, PeMSD7, PeMSD8, PeMSD7(M), and PeMSD7(L) 12 to 12 IO

### T-148 PDFormer

CCF-A CI-135 DT-2023 C-T-FLOW

#### 阅读笔记

##### 模型

空间：自注意力 图矩阵

时间：Transformer 注意力

we propose a novel Propagation Delayaware dynamic long-range transFormer, namely PDFormer

we design a spatial self-attention module to capture the dynamic spatial dependencies. Then, two graph masking matrices are introduced to highlight spatial dependencies from short- and longrange views.

Moreover, a traffic delay-aware feature transformation module is proposed to empower PDFormer with the capability of explicitly modeling the time delay of spatial information propagation

![image-20240820020802968](img/image-20240820020802968.png)

Based on this module, we further design a delay-aware feature transformation module to integrate historical traffic patterns into spatial self-attention and explicitly model the time delay of spatial information propagation.

Finally, we adopt the temporal self-attention module to identify the dynamic temporal patterns in traffic data

![image-20240820020907709](img/image-20240820020907709.png)

##### 数据

PeMS04, PeMS07, PeMS08

基于网格 NYTaxi, CHBike, TDrive

- NYTaxi 75(15x5) 30min 01/01/2014-12/31/2014 
- CHBike 270(15x18) 30min 07/01/2020-09/30/2020 
- TDrive 1024(32x32) 60min 02/01/2015-06/30/2015

12 - 12

#### 相关改版

- T-151

### T-150 DDGCRN

CCF- CI-47 DT-2023 C-T-FLOW

#### 阅读笔记

##### 模型

空间：动态图卷积，时间：嵌入

> poe 这篇论文提出了一种名为空间-时间同步图卷积网络 (STSGCN) 的模型，用于空间-时间网络数据的预测。它主要使用了以下技术处理时间特征和空间特征：
>
> **时间特征处理：**
>
> - **时间嵌入 (Temporal Embedding):** 为每个时间步引入一个可学习的时间嵌入矩阵，将时间信息编码到模型中。
> - **多模块层 (Multi-Module Layer):** 在模型中使用多个模块，每个模块对应一个特定的时间段，从而能够捕捉不同时间段的异质性。
>
> **空间特征处理：**
>
> - **图卷积 (Graph Convolution):** 使用图卷积操作来捕获空间网络中节点之间的相互影响。
> - **本地化空间-时间图 (Localized Spatial-Temporal Graph):** 将相邻时间步的节点连接起来，构建一个本地化空间-时间图，从而能够直接捕获节点在空间和时间上的相互影响。

we propose a decomposition dynamic graph convolutional recurrent network (DDGCRN) for traffic forecasting. DDGCRN combines a dynamic graph convolution recurrent network with an RNN-based model that generates dynamic graphs based on time-varying traffic signals, allowing for the extraction of both spatial and temporal features. Additionally, DDGCRN separates abnormal signals from normal traffic signals and models them using a data-driven approach to further improve predictions

- Here, temporal information corresponding to the traffic signal is combined with the spatial embedding to generate a spatio-temporal embedding. Then, a dynamic graph embedding is combined with dynamic signals extracted from the traffic signals to produce a dynamic graph. This method fully considers the periodicity and dynamics of the traffic signals so that the generated dynamic graph captures the most realistic correlations between nodes. Lastly, the spatio-temporal dependencies in the traffic signals are extracted via the Dynamic Graph Convolution Recurrent Module (DGCRM), which is based on an RNN

![image-20240820175526794](img/image-20240820175526794.png)

a dynamic graph convolution gated recurrent unit (DGCRU) is obtained by replacing the matrix product in a GRU with a combination of a dynamic graph convolution method and an NAPL module

##### 数据

PeMSD3, PeMSD4, PeMSD7, PeMSD8, PeMS07(M), PeMS07(L) 12 - 12

### T-151 ADCSD

CCF-NONE CI-4 DT-2024 C-T-FLOW

#### 阅读笔记

##### 模型

对三个模型做出了微调，分别对 PDFormer, AGCRN, ASTGCN

时间：分解

空间：自适应向量

we propose an Adaptive Double Correction by Series Decomposition (ADCSD) method, which first decomposes the output of the trained model into seasonal and trend-cyclical parts and then corrects them by two separate modules during the testing phase using the latest observed data entry by entry. In the proposed ADCSD method, instead of fine-tuning the whole trained model during the testing phase, a lite network is attached after the trained model, and only the lite network is fine-tuned in the testing process each time a data entry is observed. Moreover, to satisfy that different time series variables may have different levels of temporal drift, two adaptive vectors are adopted to provide different weights for different time series variables

核心：Online Test-Time Adaptation (OTTA)

- echniques are widely adopted in computer vision (CV) community to address the distribution shift issue by continuously updating and refining the model during the testing phase, based on the feedback and new data encountered during deployment

##### 数据

PEMS07, 自建 BayAREA 和网格数据 NYCTaxi, T-Drive (contain inflow and outflow data)

### T-152 CorrSTN

CCF-C CI-14 DT-2022 C-T-FLOW

#### 阅读笔记

##### 模型

encoder decoder

空间：动态 GNN

时间：多头注意力

n this paper, based on the maximal information coefficient, we present two elaborate spatiotemporal representations, spatial correlation information (SCorr) and temporal correlation information (TCorr). Using SCorr, we propose a correlation information-based spatiotemporal network (CorrSTN) that includes a dynamic graph neural network component for integrating correlation information into spatial structure effectively and a multi-head attention component for modeling dynamic temporal dependencies accurately. Utilizing TCorr, we explore the correlation pattern among different periodic data to identify the most relevant data, and then design an efficient data selection scheme to further enhance model performance

提到 Correlation information 并提到 GWN(Graph WaveNet)

![image-20240820233955651](img/image-20240820233955651.png)

##### 数据

PEMS03, PEMS04, PEMS07 and PEMS08

- Traffic Flow Prediction (没找到出处，怀疑是自建)

  The Traffic Flow Prediction dataset is collected every 15 min at 36 sensor locations along two major highways in the Northern Virginia/Washington, D.C., capital region

HZME

### T-153 Cy2Mixer

CCF-NONE CI-1 DT-2024 C-T-FLOW

#### 阅读笔记

##### 模型

认为是 GNN + CNN

Cycle to Mixer (Cy2Mixer), a novel spatiotemporal GNN based on topological non-trivial invariants of spatio-temporal graphs with gated multi-layer perceptrons (gMLP)

The Cy2Mixer is composed of three blocks based on MLPs: A message-passing block for encapsulating spatial information, a cycle message-passing block for enriching topological information through cyclic subgraphs, and a temporal block for capturing temporal properties

- Temporal Block This block employs a 3 × 3 convolution network to get the projection values
- Spatial Message-Passing & Cycle Message-Passing Blocks These blocks employ the MPNN for their projection function in the Gating Unit. Notably, the Spatial Message-Passing block uses the standard adjacency matrix

##### 数据

PEMS04, PEMS07, and PEMS08 12

### T-154 PM-DMNet

CCF-NONE CI-2 DT-2024 

#### 阅读笔记

##### 模型

空间 GCN 

时间 注意力

encoder-decoder

Dynamic Pattern Matching Gated Recurrent Unit (DPMGRU) 时空都有

we propose a Pattern-Matching Dynamic Memory Network (PMDMNet). PM-DMNet employs a novel dynamic memory network to capture traffic pattern features with only O(N ) complexity, significantly reducing computational overhead while achieving excellent performance. The PM-DMNet also introduces two prediction methods: Recursive Multi-step Prediction (RMP) and Parallel Multi-step Prediction (PMP), which leverage the time features of the prediction targets to assist in the forecasting process. Furthermore, a transfer attention mechanism is integrated into PMP, transforming historical data features to better align with the predicted target states, thereby capturing trend changes more accurately and reducing errors

![image-20240821015916254](img/image-20240821015916254.png)

We propose a novel Dynamic Memory Network (DMN) module designed to learn inherent representative traffic patterns within the data associated with each node. By employing a pattern matching approach, this module identifies and extracts traffic pattern features most similar to the input data while effectively reducing computational overhead. • We introduce a new Transfer Attention Mechanism (TAM). TAM transforms the existing historical hidden states into latent states aligned with the prediction target features, mitigating the error caused by the discrepancy between historical data and prediction targets

![image-20240821020131172](img/image-20240821020131172.png)

![image-20240821020140667](img/image-20240821020140667.png)

##### 数据

PEMSD4/7/8 D7M D7L NYC-Bike14/15/16 NYC-Taxi15/16 对 NYC 的细分分别给出了依据出处论文

12-12

### T-155 STG-NCDE

CCF-A CI-206 DT-2022 C-T-SPEED C-T-FLOW

#### 阅读笔记

##### 模型

时间空间都是 NCDE

we present the method of spatio-temporal graph neural controlled differential equation (STG-NCDE)

Neural controlled differential equations (NCDEs) are a breakthrough concept for processing sequential data. We extend the concept and design two NCDEs: one for the temporal processing and the other for the spatial processing. After that, we combine them into a single framework

是一种连续形式化的 RNN

we design a method based on neural controlled differential equations (NCDEs) for the first time. NCDEs, which are considered as a continuous analogue to recurrent neural networks (RNNs)

有 NCDE 相关的综述，即 ODE

NODEs generalize ResNets in a continuous manner. STGODE utilizes this NODE technology to solve the spatiotemporal forecasting problem

> GPT：
>
> ODE 是常微分方程（Ordinary Differential Equation）的缩写。常微分方程是指一个函数的导数和该函数之间关系的数学表达式。在机器学习和深度学习中，常微分方程逐渐受到关注，尤其是在动态系统建模、连续时间模型和可微模型的研究中
>
> Neural ODEs 是一种将常微分方程和神经网络结合的模型。这种模型中的核心思想是，将传统的神经网络的层替换为常微分方程的求解过程。具体来说，Neural ODEs 假设神经网络的输出不是通过一层层的离散操作得到的，而是通过一个连续的时间演化过程描述的

NCDE 与 NODE 区别见论文，那几个公式我没看懂

##### 数据

PeMSD7(M), PeMSD7(L), PeMS03, PeMS04 12-12

### T-156 STG-NRDE

CCF-NONE CI-8 DT-2023 C-T-SPEED C-T-FLOW

#### 阅读笔记

##### 模型

spatio-temporal graph neural rough differential equation (STG-NRDE)

Neural rough differential equations (NRDEs) are a breakthrough concept for processing timeseries data. Their main concept is to use the log-signature transform to convert a time-series sample into a relatively shorter series of feature vectors

We extend the concept and design two NRDEs: one for the temporal processing and the other for the spatial processing. After that, we combine them into a single framework

NRDEs are based on the rough path theory designed to make sense of the controlled differential equation.

认为是对 T-155 的一个优化，改了一点，其他内容都差不多

##### 数据

PeMSD7(M), PeMSD7(L), PeMS03, PeMS04, PeMS07, and PeMS08

### T-157 HAGCN

CCF-NONE CI-2 DT-2022 C-T-FLOW

#### 阅读笔记

##### 模型

We propose a network decentralization attention based heterogeneity-aware graph convolution network (HAGCN) method that aggregates the hidden states of adjacent nodes by considering the importance of each channel in a heterogeneous graph

learning spatial relationships through graph convolution

The traffic signal was transformed using temporal convolution networks (TCNs)

##### 数据

PeMSD4 and PeMSD8

### T-158 HTVGNN

CCF-NONE CI-1 DT-2024 C-T-FLOW

#### 阅读笔记

##### 模型

时间：注意力

空间：动态图卷积

encoder-decoder

we have proposed a novel hybrid time-varying graph neural network (HTVGNN) for traffic flow prediction. 

Firstly, a novel enhanced temporal perception multi-head self-attention mechanism based on time-varying mask enhancement was reported to more accurately model the dynamic temporal dependencies among distinct traffic nodes in the traffic network

Secondly, we have proposed a novel graph learning strategy to concurrently learn both static and dynamic spatial associations between different traffic nodes in road networks. Meanwhile, in order to enhance the learning ability of time-varying graphs, a coupled graph learning mechanism was designed to couple the graphs learned at each time step

Furthermore, we proposed a dynamic graph convolution approach, which utilized a combined topology and semantic matrix as a mask, enabling more precise modeling of the dynamic spatial correlations in traffic networks

- A novel temporal perception multi-head self-attention mechanism enhanced by a time-varying mask, diverging from traditional approaches is proposed. This mechanism dynamically adjusts attention calculations based on the temporal characteristics of input data, enabling more accurate capture of temporal dependencies among traffic nodes. The time-varying mask consists of three components: a static mask embedding for correcting the multi-head self-attention mechanism, and two dynamic mask embeddings to augment the time-awareness of the mechanism

##### 数据

PEMS03, PEMS04, PEMS07 and PEMS08

### T-159 LightCTS

CCF-NONE CI-16 DT-2023 C-T-SPEED C-T-FLOW

#### 阅读笔记

##### 模型

时间：CNN

空间：Transformer + 注意力

都有图示

Correlated time series (CTS)

On this basis, we propose the LightCTS framework that adopts plain stacking of temporal and spatial operators instead of alternate stacking that is much more computationally expensive. Moreover, LightCTS features light temporal and spatial operator modules, called L-TCN and GL-Former, that offer improved computational efficiency without compromising their feature extraction capabilities. LightCTS also encompasses a last-shot compression scheme to reduce redundant temporal features and speed up subsequent computations

By following these directions, LightCTS offers a set of novel lightweight techniques. First, LightCTS includes a novel T-operator module called Light Temporal Convolutional Network (L-TCN) and a novel S-operator module called GlobalLocal TransFormer (GL-Former) for temporal and spatial feature extraction,

##### 数据

PEMS04,08 METR-LA PEMS-BAY 12 - 12

### T-160 3D-TGCN

CCF-NONE CI-63 DT-2019 C-T-SPEED

#### 阅读笔记

##### 模型

时空：GNN

we propose a novel deep learning framework to overcome these issues: 3D Temporal Graph Convolutional Networks (3D-TGCN). Two novel components of our model are introduced. (1) Instead of constructing the road graph based on spatial information, we learn it by comparing the similarity between time series for each road, thus providing a spatial information free framework. (2) We propose an original 3D graph convolution model to model the spatio-temporal data more accurately

we propose a 3D graph convolution network where 3D convolution is applied to simultaneously learn the spatial and temporal patterns together.

Furthermore, we offer a spatial information free approach for constructing the graph for traffic network, purely relying on the similarity of time series for each road. This new proposal could capture more effective patterns between different roads than the spatial graph, facilitating superior prediction performance.

##### 数据

PEMSD7M/L METR-LA 12 - 12

### T-161 DASTNet

CCF-B CI-31 DT-2022 C-T-FLOW

#### 阅读笔记

##### 模型

迁移学习使用，把其他地区的交通流量的模型微调

空间：嵌入 graph isomorphic network (Gin)

时间：GPU

- To the best of our knowledge, this is the first time that the adversarial domain adaption is used in traffic forecasting to effectively learn the transferable knowledge in multiple cities.

propose a novel transferable traffic forecasting framework: Domain Adversarial Spatial-Temporal Network (DastNet). DastNet is pre-trained on multiple source networks and fine-tuned with the target network’s traffic data. Specifically, we leverage the graph representation learning and adversarial domain adaptation techniques to learn the domain-invariant node embeddings, which are further incorporated to model the temporal traffic data. To the best of our knowledge, we are the first to employ adversarial multi-domain adaptation for network-wide traffic forecasting problems.

![image-20240823231341403](img/image-20240823231341403.png)

##### 数据

12(1h) pems04 07 08

### T-162 ST-SSL

CCF-A CI-94 DT-2023 

#### 阅读笔记

##### 模型

都是 CNN(编码?)，图：+增广

we propose a novel Spatio-Temporal Self-Supervised Learning (ST-SSL) traffic prediction framework which enhances the traffic pattern representations to be reflective of both spatial and temporal heterogeneity, with auxiliary self-supervised learning paradigms

- Specifically, our ST-SSL is built over an integrated module with temporal and spatial convolutions for encoding the information across space and time.

##### 数据

The first kind is about bike rental records in New York City. NYCBike1 (Zhang, Zheng, and Qi 2017) spans from 04/01/2014 to 09/30/2014, and NYCBike2 (Yao et al. 2019) spans from 07/01/2016 to 08/29/2016. They are all measured every 30 minutes. The second kind is about taxi GPS trajectories. NYCTaxi (Yao et al. 2019) spans from 01/01/2015 to 03/01/2015. Its time interval is half an hour. BJTaxi (Zhang, Zheng, and Qi 2017), collected in Beijing, spans from 03/01/2015 to 06/30/2015 on an hourly basis

2/3 天 7:1:2

### T-164 MemDA

CCF-B CI-7 DT-2023 C-T-SPEED

### 阅读笔记

##### 模型

encoder-decoder

In this study, we propose a new urban time series prediction model for the concept drift problem, which encodes the drift by considering the periodicity in the data and makes on-the-fly adjustments to the model based on the drift using a meta-dynamic network

- two key components: a dual memory module and a strategically adjustable meta-dynamic network

> 根据论文摘要和内容,这篇论文提出了一种名为MemDA的新型城市时间序列预测模型,用于解决概念漂移问题。它主要使用了以下两个关键技术:
>
> 1. 双重记忆模块(Dual Memory Module):
>    - 该模块能够有效管理大量历史数据,保留长期历史信息的同时获取最有价值的信息,以应对概念漂移。
> 2. 可策略调整的元动态网络(Meta-Dynamic Network):
>    - 该网络能够动态调整模型参数,在保持先前学习知识的稳定性与适应新数据模式的可塑性之间达到平衡,从而有效应对概念漂移。
>
> 这两个关键组件的结合,使得MemDA模型能够编码数据中的周期性模式,并基于漂移实时调整模型,从而显著优于现有的概念漂移对策方法。
>
> 总的来说,论文提出的MemDA模型利用了记忆模块和元动态网络这两种技术来捕获时空特征和适应概念漂移,从而提高了城市时间序列预测的效果。

##### 数据

论文研究领域不只是交通预测，交通里用了 PEMS 和 Beijing

Beijing 2022/05/12∼2022/07/25 3126 5 minutes Sudden

### T-165 factorized ST-TGCN

CCF-NONE CI-3 DT-2021 C-T-SPEED

#### 阅读笔记

##### 模型

空间：图卷积 时间：叠CNN到上面成张量

we propose a factorized Spatial-Temporal Tensor Graph Convolutional Network to deal with traffic speed prediction. Traffic networks are modeled and unified into a graph that integrates spatial and temporal information simultaneously. We further extend graph convolution into tensor space and propose a tensor graph convolution network to extract more discriminating features from spatial-temporal graph data. To reduce the computational burden, we take Tucker tensor decomposition and derive factorized a tensor convolution, which performs separate filtering in small-scale space, time, and feature modes. Besides, we can benefit from noise suppression of traffic data when discarding those trivial components in the process of tensor decomposition

##### 数据

SZ-taxi: This dataset is collected from a taxi trajectory of Shenzhen, China. It defines 156 major roads as nodes, which are characterized by the speed at which taxis pass through the roads. And the adjacency matrix represents the positional connections between roads. These data are collected every 15 minutes, ranging from 1/1/2015 to 1/31/2015.

### T-166 BSTGCN

CCF-NONE CI-14 DT-2020 C-T-SPEED

#### 阅读笔记

##### 模型

GCN encoder-decoder GRU

we propose a Bayesian Spatio-Temporal Graph Convolutional Network (BSTGCN)

##### 数据

SZ-taxi Los-Loop 60min

### T-167 MHAST-GCN

CCF-NONE CI-4 DT-2023

#### 阅读笔记

##### 模型

空间 图卷积；时间 GRU； 时空 注意力

To solve this challenge, this paper presents a traffic forecasting model which combines a graph convolutional network, a gated recurrent unit, and a multi-head attention mechanism to simultaneously capture and incorporate the spatio-temporal dependence and dynamic variation in the topological sequence of traffic data effectively

##### 数据

SZ-taxi Los-Loop