> 诸如 T-ZS1 的标号参见 Zotero 目录(如无请联系我获取)

## 背景介绍

### 交通拥堵

- 面临的主要问题

  (T-ZS1) Traffic congestion is a major problem faced by metropolitan cities -> Most congestion mitigation measures are costly, difficult to implement, or both.   
  
  其解决：交通预测
  
  Educated traffic decision made through accurate prediction is a far cheaper and easier to implement alternative for reducing road congestion  

### 交通数据等

- 交通拥挤的代价

  (T-ZS1) In 2015, it is estimated that the avoidable cost of traffic congestion for Australian capital cities is approximately \$16.5 billion, up from the 2010 estimate of \$12.8 billion. Furthermore, this value is estimated to increase to about $30 billion by 2030

  > [参考 Cosgove D. Traffic and congestion cost trends for Australian capital cities[J]. Canberra: Department of Infrastructure and Regional Development, Bureau of Infrastructure, Transport and Regional Economics, 2015.](https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Cosgove%2C+Traffic+and+congestion+cost+trends+for+Australian+capital+cities&btnG=#d=gs_cit&t=1721488944765&u=%2Fscholar%3Fq%3Dinfo%3ACPeX2qr_RW8J%3Ascholar.google.com%2F%26output%3Dcite%26scirp%3D0%26hl%3Dzh-CN)

- 交通监管政策

  (T-ZS1) Singapore implemented regulations on the number of vehicles on roads -> which is infeasible for countries with poor public transportation systems

  > [参考 “Singapore to freeze car numbers,” https://www.bbc.
  > com/news/business-41730778, accessed: 20 November
  > 2018](https://www.bbc.
  > com/news/business-41730778)

- 修路成本

  (T-ZS1) the estimated per mile cost of a standard one lane road in New Jersey, USA is $220,490 -> Constructing new roads to ease congestion is also difficult due to the extremely high cost  
  
  > [参考 J. Carnegie and A. M. Voorhees, “The cost of roadway
  > construction, operations and maintenance in new jersey,” pp. 557–566, 2016.  ](https://trid.trb.org/View/1408290)

### 前置条件

- 传感器的普及 -> 大数据获取

  (T-ZS1) With the advancements and widespread adoption of traffic sensors, access to large traffic databases is now available.  -> This has led to the development of traffic prediction as a research field.   
  
- 库 

  Due to the availability of deep neural network libraries such as Keras, PyTorch , and TensorFlow, development of complex neural network models has become much easier  

### 问题定义

#### 交通预测

##### 描述

- 交通预测

  (T-ZS1) 

  描述：

  Future traffic prediction involves creating a prediction model from historical traffic data to predict the short-term future traffic state ranging from 5 to 60 minutes into the future
  
  特点：
  
  > 交通预测与其他时序分析的不同在于：
  >
  > - 一个地点可能影响另一个地点
  > - 存在全局外部因素如天气、节假日
  
  Traffic prediction is different from conventional time-series analysis in that traffic prediction is subject to spatial as well as many other external factors.  ->  the prediction of traffic at one site depends on the traffic at other sites and all of the sites are affected by external factors such as weather and holidays.  
  
  定义：使用可学习的函数，使用历史交通数据输入，预测未来交通
  
  Traffic prediction concerns the usage of a learnable function that takes as input the historical traffic data from several previous time-steps in order to predict the traffic in the future.  
  
  > 近义句 Traffic prediction is a task of training an arbitrary function to predict future traffic given past traffic data  
  
  > 还有其他，参考下文
  
  $$
  \hat y_{t+T'}=f([X_{t-T+1},X_{t-T},\cdots,X_t])
  $$
  
  目标：找到模型参数，最小化误差：
  
  The objective is to find the model parameters which minimize the error between the predicted traffic and the observed traffic:  
  $$
  \theta^*=\arg\min_{\theta^*}L(y_{t+T'},\hat y_{t+T'};\theta^*)
  $$
  
  > 其中 $y_t$ 是时间 $t$ 的观察值，$\hat y_t$ 是预测值，$T$ 是输入序列长度，$T'$ 是预测范围，$L$ 是损失函数，$f$ 是任意函数，$\theta^*$ 是最优参数。
  >
  > - The observed traffic at time $t$
  > - The predicted traffic at time $t$
  > - Input sequence length, i.e., how many time steps of past traffic data are used as the input 
  > - Prediction horizon, i.e., how many time steps in the future the prediction is for 
  > - An arbitrary function that calculates the traffic prediction based on the input data  
  > - Loss function, which is the function that calculates the quality of the prediction
  > - The optimal set of parameters for the function $f$

##### 类型

(T-ZS1)

- 交通流量 某地某段时间车辆总数

  Traffic flow is denoted as the total number of vehicles detected in a target detection site during a certain time period.   

- 交通速度 某地某段时间各车辆平均速度

  Traffic speed is denoted as the average traveling speed of vehicles detected in a target detection site during a certain time period  

T-4 交通状况 traffic condition, which consists of four categories: fluency, slow, congestion and extreme congestion  

T-28 [T-30](https://www.researchgate.net/profile/Leye-Wang/publication/322886199_Crowd_Flow_Prediction_by_Deep_Spatio-Temporal_Transfer_Learning/links/5aa9bd6b0f7e9b88266f6529/Crowd-Flow-Prediction-by-Deep-Spatio-Temporal-Transfer-Learning.pdf) 人流量 crowd flow instead of traffic flow: Crowd flow measurements are the same as traffic flow, but they are designed for general human mobility instead of automobile mobility  

- [T-31](https://dl.acm.org/doi/abs/10.1145/3292500.3330646) 细粒度而不是历史数据的  a fine-grained prediction is performed using a coarser data (e.g., predicting crowd flow of different school buildings given crowd flow of the entire university area) instead of using historical data  

### 发展历史

#### 概述

(T-ZS1) The earliest class of models used is the classical statistical models. Afterwards, machine learning models improve upon the performance of classical statistical models. Then, the deep neural network class of models dominates the field due to its capability in capturing the complex and nonlinear patterns in traffic data.  

#### 统计模型

> ##### ARIMA

(T-ZS1) classical statistical models, of which the Autoregressive Integrated Moving Average (ARIMA) family of models is the most popular. 

局限：(T-ZS1) 线性模型，假设数据规律不变；只能预测小时间段，参数手调

- T-3 T-5 [T-ZS6](https://www.sciencedirect.com/science/article/pii/S0968090X10001610) simple linear models which assume that the traffic is stationary  ->  frequently fail when handling the complex, nonlinear traffic data 
- [T-12](https://arxiv.org/abs/1801.02143) were proposed at a time where traffic data were simpler and much smaller in size  -> a condition that no longer holds true in the present day where the ubiquity of traffic sensors has caused an explosion in traffic flow data   
- [O-1](https://www.tandfonline.com/doi/abs/10.1080/00031305.1996.10473554) the function parameters are manually defined a priori  

(stated by T-ZS1)

- [T-6](https://trid.trb.org/View/148123) [1979] first researchers to apply ARIMA to traffic prediction 
- [T-7](https://trid.trb.org/View/167550) [1980] found that the ARIMA(0,1,1) model is the most statistically significant
- [T-8](https://journals.sagepub.com/doi/abs/10.3141/1678-22) [1999] applied subset ARIMA and found that it provides stable and accurate results
- [T-9](https://journals.sagepub.com/doi/abs/10.3141/1776-25) [2001] discovered the impact of upstream traffic sensors to downstream ones and applied ARIMAX model for traffic flow prediction
- [T-10](https://ascelibrary.org/doi/abs/10.1061/(ASCE)0733-947X(2003)129:6(664)) [2003] applied Seasonal ARIMA to the United States and the United Kingdom traffic data 
- [T-11](https://journals.sagepub.com/doi/abs/10.3141/1857-09) [2003] discussed and compared the Vector Autoregressive Moving Average and Single space-time ARIMA model

#### 机器学习

(T-ZS1) machine learning models during the 2000s utilize shallow and simple structures, limiting their prediction power  

(stated by T-ZS1)

- [T-13](https://research.aber.ac.uk/en/publications/the-use-of-neural-networks-to-recognise-and-predict-traffic-conge) [1993] One of the first neural network applications in traffic flow prediction  
- [T-14](https://www.sciencedirect.com/science/article/pii/S0968090X05000276) [2005] proposed a genetic algorithm approach to optimally tune the network
- [T-15](https://ascelibrary.org/doi/abs/10.1061/(ASCE)0733-947X(2006)132:2(114)) [2006] used multiple neural network predictors which are combined using the theory of conditional probability and the Bayes rule  
- [T-16](https://ascelibrary.org/doi/abs/10.1061/(ASCE)0887-3801(2005)19:1(94)) [2005] neural network model was applied to traffic prediction  
- [T-17](https://ieeexplore.ieee.org/abstract/document/6088012) [2012] imbued a neural network model with the hybrid exponential smoothing method to preprocess training data and the Levenberg-Marquardt algorithm to train the network weights

非 NN：

(stated by T-ZS1)

- k-Nearest Neighbor [T-18](https://ascelibrary.org/doi/abs/10.1061/(ASCE)0733-947X(1991)117:2(178)) [T-19](https://www.sciencedirect.com/science/article/pii/S1877042813022027) [T-20](https://www.sciencedirect.com/science/article/pii/S0968090X15003812)
- support vector regression (SVR) [T-21](https://www.sciencedirect.com/science/article/abs/pii/S0957417408004740) [T-22](https://ieeexplore.ieee.org/abstract/document/4344269) [T-23](https://link.springer.com/chapter/10.1007/978-3-540-72393-6_121)

#### 深度学习

### 常用技术

#### 机器学习

- 局限性：数据密集

  (T-ZS1) The main weakness is that machine learning models are data intensive 

- 局限性：

  (T-ZS1) powerful but very hard to train efficiently  

- 局限性：手动提取特征

  (T-ZS1) many other machine learning models’ feature extraction phase, which helps extract useful patterns and information from the data to help the prediction, is done manually (i.e., using manually tuned kernels)  

- 优点：

  (T-ZS1) Machine learning models are flexible as they can learn from the data. That is, the parameters of the prediction function are adjusted automatically as the model traverses through the dataset  

#### 神经网络

深度神经网络

(T-ZS1) Amongst all the available traffic prediction methods, deep neural network is the most prominent.  

- 概念：

  (T-ZS1) Deep neural networks consist of complex neural network models with a large number of layers. 

- 条件：

  (T-ZS1) increasing computational power, as well as theoretical and software improvements in recent times had made increasingly complex neural network models feasible to train. Thus, in the middle of the 2010s, researchers started to apply deep neural network models for traffic prediction  

- 理由：

  (T-ZS1) This is due to its sheer predictive power that can model the complex and nonlinear traffic patterns

  > 参考几个具体应用论文：[T-1](https://www.mdpi.com/1424-8220/17/7/1501)、[T-2](https://www.mdpi.com/1424-8220/17/7/1501)、[T-3](https://arxiv.org/abs/1612.01022)、[T-4](https://ieeexplore.ieee.org/abstract/document/8489600)、[T-5](https://arxiv.org/abs/1707.03213)，引用都不错
  
- 类别：
  
  (T-ZS1) The three most common deep neural network models used for traffic prediction are Convolutional Neural Networks, Recurrent Neural Networks, and Feedforward Neural Networks
  
- 适用理由：CNN/RNN抓时空+层数
  
  (T-ZS1) Some of the deep neural network models can explicitly capture different aspects of traffic data, which made them even more attractive. For instance, CNN can explicitly capture the spatial aspect of traffic data while RNN can explicitly capture the temporal aspect of traffic data. Additionally, the increased number of layers improves the models’ prediction capability. This factor allows them to model traffic fluctuations more accurately  
  
- 优点：自动学习特征
  
  (T-ZS1) (The reason behind its prominence is that) neural networks perform automatic feature extraction as well as the actual prediction in one model 
  
- 局限性：当前状态和未来方向不明

  (T-ZS1) The increasing popularity of deep neural network models for traffic prediction has led to numerous publications, but issues such as the wide variety of hybrid deep neural network structures have made it difficult to assess the current state and future directions of this research field. This problem is compounded by the fact that survey works focusing specifically on deep neural network models are rare
  
- 缺点：

  (T-ZS1) 需要大数据，训练代价高，难以理解(如参数含义)

  - Deep neural network models require a large amount of data that covers all traffic conditions. 

    If the amount of data is too small or if the data is not diverse enough, the model’s generalization capability is compromised.

  - Deep neural network models still take a long time to train. 

    As deep neural network models are complex and have a large number of layers, the training time can be very long. This problem is compounded on hybrid deep neural network models. As classical statistical and older machine learning models are not as complex, their training time is much shorter.

  - Deep neural network models are difficult to interpret. 

    This is because of two reasons: the number of internal parameters is very large, and the parameters are learned from training, not set manually. Thus, while they can predict well, it is hard to understand their parameters. Understanding the parameters may reveal important information such as the spatiotemporal dynamics in the road network 




## 现有综述

### 列表

年份，模型分类，问题分类，数据集分类

- T-ZS1

  CNN, RNN, FNN 2014-2019

- [T-ZS3](https://www.sciencedirect.com/science/article/pii/S0968090X14000096)

  statistical, neural network, hybrid model 2006-2013

  time series, function approximation 浏览了一下，深度学习较少
  
- [T-ZS4](https://www.sciencedirect.com/science/article/pii/S1574119217306521)

  浏览了一下，对深度学习介绍很少，传统方法介绍篇幅很多

- [T-ZS5](https://ieeexplore.ieee.org/abstract/document/8344848/)

  浏览了一下，主要讲大数据技术，感觉相关性不大

- [T-ZS6](https://www.sciencedirect.com/science/article/pii/S0968090X10001610)

  读摘要，对比统计方法和 NN

- [T-ZS7](https://dl.acm.org/doi/abs/10.1145/3231541.3231544)

  没搞到 pdf，只读了摘要，NN 相关 2018

### 评价

- T-ZS3

  (T-ZS1 评价) 分类老旧、深度学习少

  do not include the now ubiquitous deep neural network models

  their work categorized the models based on several criteria such as the type of model (e.g. statistical, neural network, hybrid model) and the problem (e.g. time series, function approximation). This taxonomy is outdated because modern traffic prediction models are mainly based on deep neural network, which under their taxonomy will all fall under the neural network category of model and function approximation category of problem  

- T-ZS4

  (T-ZS1 评价) 分类比较少、没讨论挑战

  their model taxonomy only has a few points of comparison, which are: whether or not the model integrates environmental data, contains spatial property, handles nonlinearity and handles nonstationarity  

  Additionally, their work does not have a future challenges section that discusses how the field can be advanced  

- T-ZS5

  (T-ZS1 评价) 与机器学习关联不大

  their work focuses on big data analytics without much focus on the actual models  

### 关系

T-ZS1 介绍了 T-ZS3, T-ZS4

## 具体技术

### 概述

(T-ZS1) RNN, CNN, FNN

- RNN is commonly used to capture the temporal trends of traffic data–the dynamics of how past traffic can influence future traffic. CNN is commonly used to capture the spatial trends of the data–how traffic propagates through the road network. FNN can aggregate the output from different subnetworks and also can process external data such as weather information  

### FNN

概念：

- (T-ZS1) 名称 A Feedforward Neural Network (FNN), which is also commonly referred to as Fully Connected Neural Network (FC or FCNN), is one of the earliest and simplest neural network models  

  组成 It consists of several layers of fully connected computational nodes organized in many layers  

  计算方式 The value of every node in the hidden or output layers is computed by taking the weighted sum of all of the previous layer’s nodes and then passing the value to a nonlinear function such as sigmoid, tanh and relu  

缺点：

- 参数多、训练久

  (T-ZS1) The FNN’s fully connected structure enables each of its layers to process the combination of all the previous layer’s features. However, this also serves as a weakness because its full connection results in a large amount of parameters  

  Consequently, the training process of FNNs can be quite time consuming. In addition  

- 没有明确获取时空信息

  (T-ZS1) do not have the capability of explicitly capturing spatial or temporal data. Because of this, FNNs are rarely used as the main predictor in deep neural network literatures.  

作用：工具组件：聚合输出、维度转换、引入数据

- (T-ZS1) For traffic flow prediction, FNNs usually serve a utility role in a hybrid deep network, whose main purpose is to perform tasks such as aggregating outputs from different components within the network, dimensionality transformation and incorporating external data such as weather.  

  - 维度转换：This is because the size of input layer or output layer can be set manually, which gives FNN the capability to transform inputs of an arbitrary dimensionality to an output of an arbitrary dimensionality  
  - 聚合输出：When used to integrate external data, the input depends on the type of external data. Numerical values can be provided as it is while categorical values need to be transformed first (e.g., using one-hot encoding)  
  - 引入数据：For aggregating outputs and dimensionality transformation, the inputs depend entirely on the model  

### CNN

#### 基本

组成：卷积层、池化层

(T-ZS1) 

A CNN consists of several “convolution” and “pooling” layers. 

- Convolution’s purpose is to extract features from the input, 

  > Mathematically, convolution layers extract features by computing the dot product between a matrix of some preset values (referred to as filter) and a subset of cells from the original grid, which produces a matrix that is called feature map  

  > The example in Figure 1 shows, (i) the top-left 3 × 3 subset of cells produces the value 470, and (ii) the bottom-right 3 × 3 subset of cells produces the value 170 in the feature map. This can be interpreted as the top-left subset having a much higher number of vehicles in that region than the bottom-right subset  

  <img src="img/image-20240722103753648.png" alt="image-20240722103753648" style="zoom: 67%;" />

- whereas pooling’s purpose is to reduce the dimensionality of each feature map but preserve the most important information.  

功能概述：

(T-ZS1) A Convolutional Neural Network (CNN) has the capability to learn inherent features progressively, starting from low level features and then building up to more abstract concepts through a series of convolutional layers.  

能用的理由：交通流读入可以建模为图像，每个像素点对应一个交通密集的地区，因此可以用图像识别技术，即把区域网格化，像素值是如车辆数目，不同时间即像素值不一样

(T-ZS1) 

- Although this strength contributes to its popularity in image recognition, CNNs have been regularly applied to traffic flow prediction. The intuition is, traffic flow readings can be modeled as an image, where each pixel corresponds to the traffic intensity at a certain block of area. Thus, similar techniques developed for image recognition can be easily applied 
- Given a road network, the input of a CNN is preprocessed by partitioning the network as a grid, which is essentially a set of cells with each cell representing an area in the data space and the value associated with the cell representing the number of vehicles detected in that cell at a certain point in a time period (e.g., 5×5 cells in Figure 1). The traffic flow reading for each time period will be represented with the same grid but different number of vehicles. Thus, the entire traffic data modeled this way can be seen as several images with the same size but different pixel values   

优点：不全连接，参数少

- (T-ZS1) Unlike most neural networks, CNN’s layers are not fully connected. Consequently, the number of parameters and training time are significantly reduced 

  不全连接的另一个优点在于可以学习空间的局部相关性

  (T-ZS1) Since CNN’s layers are not fully connected, one layer of CNN does not learn from all of the previous layer’s features. However, this actually proves to be an advantage in many applications as CNN can learn how the different aspects of the input relate to each other spatially  

优点：权重共享

- (T-ZS1) Additionally, CNN uses a weight sharing mechanism, which further reduces the number of required parameters  

具体应用：混合网络、空间特征 (如晚高峰商业区和居住区间流量强关联)

- (T-ZS1)  In the application of traffic prediction, CNN is often used as a component in a hybrid deep neural network, whose task is to capture the spatial aspect of traffic data.
  This is because different roads in different locations may be correlated and these correlated roads share similar traffic trend. Therefore, the traffic of the correlated roads may rise or fall, depending on their historical data 

  For instance, during the evening, there is a strong correlation between the road traffic of commercial and residential districts because employees are heading off from work  

### RNN

#### 基本

定义：

- (T-ZS1) An RNN consists of a single node with a recurrent connection, but is often visualized as a chain of nodes, with each node representing the network state at a particular recurrence/time step  

  <img src="img/image-20240722110704917.png" alt="image-20240722110704917" style="zoom:67%;" />

  节点状态 $s_t$ 处理 $t$ 时刻的输入数据 $x_t$，和截止 $t-1$ 为止的全部信息($s_{t-1}$)一起传入到该节点

  The node state $s_t$ processes the input data $x_t$ at time $t$, as well as a ‘summary’ of all the information obtained up to time $t - 1$. This summary is stored in $s_{t-1}$, and it memorizes which parts of the sequence are important. Node $s_t$ then has the summary up to time $t$ and this information is passed to the next node state
  $s_{t+1}$. Thus, the node state $s_t$ stores the state of nodes for all the previous time steps until the beginning of the input (i.e., $s_{t-1}$, $s_{t-2}$, . . . ). The output $o_t$ is then compared with the ground truth $y_t$ in order to calculate the loss, which is used to fine-tune the model parameters.  

具体输入：

- (T-ZS1) In traffic prediction applications, the input to an RNN consists of past traffic readings. A continuous time period is divided into discrete time blocks and the traffic flow reading from each block is fed into the RNN.  

  > In the field of traffic prediction, LSTM as well as other RNN-based methods are commonly used as a component in hybrid deep neural network models. Its task is to capture the temporal patterns of traffic data; learning how traffic evolves over time  

> 应用：命名实体识别、声音识别、音乐识别、图像字幕生成等
>
> RNN-based methods in general possess the major advantage in the form of its memorization capability. The ability of learning important parts of the sequence and knowing when to memorize or forget them had led RNN to be the prime choice for sequence data. Due to this, RNN based models have been applied in many fields such as named entity recognition [40](https://link.springer.com/chapter/10.1007/978-3-319-55699-4_33), voice recognition [41](https://dl.acm.org/doi/abs/10.1145/3132847.3132893), music composition [42](https://people.idsia.ch/~juergen/blues/IDSIA-07-02.pdf), and image caption generation [43](https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Vinyals_Show_and_Tell_2015_CVPR_paper.html)
>
> 这些论文大部分引用不多或比较古老，除了 43 感觉都很一般

优点：长短的时间依赖都可以记忆，而且比其他记得更长

- (T-ZS1) Recurrent Neural Networks (RNN) are commonly applied to sequence data because of their memorization capability, which can learn both long and short term dependencies between parts of the sequence. Additionally, RNN is able to scale to longer sequences compared to other network architectures. Its unique capability makes it one of the most popular deep neural networks.  

缺点：梯度消失

- (T-ZS1) By its nature of being able to take in possibly very long sequences, RNN suffers from the vanishing gradient problem, which hinders the network’s ability to memorize information for a long time  

缺点：训练时间长

- (T-ZS1) RNN’s recurrent structure leads to significantly longer training time compared to other deep neural network models  

#### LSTM

开山(1997) [O-2](https://ieeexplore.ieee.org/abstract/document/6795963) ；改进(2000) [O-3](https://ieeexplore.ieee.org/abstract/document/6789445)

概念：

- (T-ZS1) maintains the RNN’s recurrent structure, but introduces the three gates to control the cell value.  

  > also contains multiple layers, each possessing a cell with the memorization capability. In addition, it contains three gates, which control how information propagates throughout the network.  

  These gates are: 种门控制信息的传播($i,o$，分别控制遗忘多少之前信息、当前与下一层的相关性)

  - the input gate $i$, which controls the importance of the inputs $x_t$ and $h_{t-1}$,

  - forget gate $f$ which controls how much of the previous information $C_{t-1}$ is to be forgotten, 

  - and the output gate $o$, which controls how relevant is the current information $C_t$ for the next step.  


<img src="img/image-20240722113840626.png" alt="image-20240722113840626" style="zoom: 67%;" />

适用的原因：交通数据是时序的

(T-ZS1) We speculate that this is because traffic data constitutes a temporal sequence, which fits LSTM’s purpose. Additionally, most available traffic flow data is compatible with LSTM, as these traffic flow data can easily be modeled as a sequence of traffic flow readings  

> For instance, the traffic flow between 11:00 and 12:00 can be captured as the aggregated traffic reading for four periods, including 11:00-11:15, 11:15-11:30, 11:30-11:45, and 11:45-12:00. This data can be fed into an RNN, resulting in an RNN with four recurrences  

### 混合模型

集各家之长

(T-ZS1) As complex deep neural networks are becoming viable to train, most authors have utilized the hybrid neural network setting, which combines different neural network structures into a larger entity, to maximize the prediction performance  

The popularity of hybrid neural network structure is contributed by its power and flexibility of utilizing the different strengths of its individual components.   

## 相关数据

### 概述

#### 分类

参考 T-ZS1 的分类标准，主要数据集可以分为两类：

- 固定式采集数据集(point data)：安装在固定地方的探测器所采集的数据

  > (T-ZS1) 标准数据集为 PeMS

- 移动式交通数据(trajectory data)：GPS 等收集的车辆轨迹信息

  > (T-ZS1) 无标准 do not have a standard dataset  
  >
  > Different works use different datasets with different properties, including the origin country (mostly America or China), method of transportation (cars, taxis or bicycles) and time range
  >
  > trajectory data from Beijing is relatively more popular    

此外，可能还需要一些辅助数据信息：

- 交通网络数据：探测器的分布图(欧氏空间网格或无向加权图等)
- 气候数据、日期(节假日)数据、事件(如车祸)数据等

### 评价

#### 日期

> 日期容易结合：
>
> (T-ZS1) Conversely, time-of-day and day-of-week data are much easier to incorporate
>
> 日期少于一年的缺点：
>
> (T-ZS1) 26 out of 37 literatures use less than one year’s worth of data. This deficiency will have an adverse impact on sub-tropical regions, as seasonal changes may affect temperature and weather, which in turn can affect traffic. By using data from only one or several months, the model cannot generalize to different seasons. This can be mitigated by incorporating weather data, but as mentioned before, this is a difficult and time-consuming task.  
>
> 时间不完整一天/周的缺点：
>
> (T-ZS1) e-consuming task. Some authors also use data from only a certain range of hours or use data from weekdays only. This will also cause problems as the model cannot generalize well to situations outside the boundaries of the provided data. For instance, using traffic data from 07.00 AM to 11.00 PM only may reduce the model’s performance on the excluded hours, and using only weekdays data may adversely impact the model’s performance when predicting weekend traffic  

#### 结合数据

> 结合数据的困难：
>
> (T-ZS1) one model that uses the Caltrans data covering a long highway will need to match the time stamp, the latitude, and the longitude of each reading in order to find the appropriate weather and accidents data  
>
> 成本变大：
>
> (T-ZS1) added time complexity of aggregating the different data together , which is undesirable, especially in an already time-consuming hybrid deep neural network structure  

#### 真实性

> 数据要趋于真实的理由：
>
> (T-ZS1) using vastly different datasets may result in an entirely different model. Thereby, for a model to be applicable to real application scenarios, it is important to use a dataset that closely resembles those scenarios.  

#### 时间粒度

> 时间粒度：大部分 5min (默认)，推荐 15min
>
> (T-ZS1) using vastly different datasets may result in an entirely different model. Thereby, for a model to be applicable to real application scenarios, it is important to use a dataset that closely resembles those scenarios.  
>
> (T-ZS1) 指出[文献](https://onlinepubs.trb.org/Onlinepubs/trnews/rpo/rpo.trn129.pdf)推荐 15min 粒度
>
> 数据粒度的辩证讨论：对结果和训练/输入的影响
>
> Depending on the dataset, the data granularity is a potentially important hyperparameter. Using a data granularity that is too small may cause a lot of zero values, especially during conditions where traffic is very sparse. For example, it is highly likely for a traffic loop detector to not detect any cars in 2 or 5 minute periods during off-peak hours (e.g. 02:00 04:00 AM) while this becomes less likely if the granularity is increased to 15 minutes or more. On the other hand, using a granularity that is too high might result in the smoothness of the traffic flow reading where important trends are lost. For instance, if the traffic experiences periodic shifts during 12:30PM, this trend might not be detected if the data granularity is one hour.
>
> Data granularity also impacts the number of possible data points as well as the size of the input sequence. Using a smaller granularity will increase the length of the required data sequence. For instance, one hour’s worth of data can be captured with only a sequence of length 4 when the granularity is 15 minutes, but when the granularity is 5 minutes, the sequence length is 12. This can impact training time, especially for RNN-based models.  
>
> Due to the aforementioned reasons, choosing the correct data granularity becomes a decision based on trade-offs and should be considered carefully depending on the data, the model, as well as the application scenarios  

#### 输入输出长度

> (T-ZS1) 多个长度都试一下，一般输入输出正相关，未得到充分研究：超参搜索耗时。多数是人为设置的，因为搜索超参太慢。可以用小数据集做超参搜索来缓解该问题。
>
> many authors perform experiments with different prediction horizons and use different input sequence lengths for each of the selected prediction horizons  
>
> Intuitively, as we increase the prediction horizon, the input sequence length also needs to be increased. This is because the increase in prediction horizon means predicting the traffic of further time frame in the future and thus, increasing the task complexity. Increasing the size of the data points by extending the input sequence may help in tackling the complex problem.  
>
> Unfortunately, the relationship between the input sequence length and the prediction horizon is rarely explored by the literature. Most of the input sequence lengths were chosen arbitrarily without iterating through different possible values. This is because hybrid deep neural network structures take a long time to train, which makes iterating through different settings unwieldy. Despite this issue, hyperparameter search remains an important facet of deep neural network development that cannot be omitted. One possible remedy of this problem is to first use a smaller data, chosen randomly from the main dataset, to find the optimal parameter setting.  

### 列表

列出部分常用的数据集：

- [PeMS](http://pems.dot.ca.gov/) (Caltrans Performance Measurement System)

  研究最广泛的数据集，由加利福尼亚州主要公路的上万探测器收集，每半分钟采集一次，包含容量、速度、交通流量等多种数据

  有多个子集广泛用于论文中，包括 PeMS-BAY、PeMSD3、PeMSD4、PeMSD7、PeMSD8 等，可以参考 [这篇综述](https://arxiv.org/pdf/2101.11174?trk=public_post_comment-text)

  > 优点 (T-ZS1) public availability, ease of download, simple structure and long historical data  
  >
  > 提供的数据、粒度 (T-ZS1) provides information regarding date, time stamp, traffic flow per lane, and aggregated traffic flow. Traffic flow is the most commonly used field, but occupancy and speed information is also available. The data granularity can be set to 5 minutes, hourly, daily, weekly and monthly depending on user requirements  

- [METR-LA](https://github.com/liyaguang/DCRNN) (Metro Traffic Los Angeles)

  洛杉矶公路网，207 个探测器，5 分钟间隔收集数据

- [Seattle Loop](https://github.com/zhiyongc/Seattle-Loop-Data)

  西雅图 4 条路数据，323 个探测器，5 分钟间隔收集数据，2015 年 1 月数据

- [SZ-Taxi](https://github.com/lehaifeng/T-GCN)

  深圳罗湖区 156 条路的数据，15 分钟粒度，2015 年 1 月数据

- [Beijing Traffic](https://github.com/deepkashiwa20/Urban_Concept_Drift)

  北京市 3126 个路段在 2022 年 5-7 月的 5 分钟粒度的数据

- [Q-Traffic](https://github.com/JingqingZ/BaiduTraffic)

  北京 2017 年 4-5 月一万多个路段每 15 分钟采样一次的百度地图数据

北京：

(T-ZS1) it is unclear as to whether or not all of the Beijing-based datasets come from one unified dataset source  

(T-ZS1 stated:) usually cover the Ring Road area

- 用到的 (2015, 2k引用) [T-25](https://www.sciencedirect.com/science/article/pii/S0968090X15000935)、(2017, 100引用) [T-26](https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/iet-its.2016.0257)、(2016, 180引用) [T-27](https://ieeexplore.ieee.org/abstract/document/7795712)、(2018，近4k引用) [T-28 STGCN](https://arxiv.org/abs/1709.04875)
- T-25 contains the traffic volume, occupancy and speed data (2017, 近500引用) [T-29](https://epubs.siam.org/doi/abs/10.1137/1.9781611974973.87)

其他数据集：参见 [paperwithcode 网站：Traffic Prediction Task](https://paperswithcode.com/task/traffic-prediction)、[这篇综述](https://arxiv.org/pdf/2101.11174?trk=public_post_comment-text)、[TKDE2020综述](https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=6998&context=sis_research)、[这篇综述](https://kns.cnki.net/kcms2/article/abstract?v=7sQefmFxFK3uEKBquRla5qDHveK9oCCRpBWf04Zyi-hciTPIaXDHO5AckFT2OGZGmxYUV8QI8BcEApyz73mJ280tQxDTOIZYSnF6llnWzinghcTnd6z1lC2pEY218-lrd9AoSHndDepAUNkp_yiHGfr7Tsk5vANL&uniplatform=NZKPT&language=CHS)。

### 统计

(T-ZS1) 

表格的评价尺度：

- 参考([xx])、作者、年份、主要数据类型、主数据集、时间范围、粒度、次要数据集、输入序列长度、预测视野

- 模型分类、预测值、时/空、模型子分类

  > 时空分类的具体定义参见论文，指模型专门用来处理哪一部分，or both

数据集

- PeMS 14/37
- 北京点数据和轨迹数据，各 6/37

辅助数据

- 天气 6/37 ；时间(time of day / day of week) 3/37  ；路网(road network) 3/37

数据跨度

- 一个月 5/37；数个月 22/37；一年 6/37；超过一年 4/37

模型：

- LSTM 18/37 混合模型 21/37

## 具体论文

### RNN

#### basic

(T-ZS1)

- 首次应用 T-25 [T-32](https://ieeexplore.ieee.org/abstract/document/7463717) the first few applications of basic LSTM  
- 建模为矩阵以融合空间信息 [T-33](https://ieeexplore.ieee.org/abstract/document/7966128) [T-34](https://ieeexplore.ieee.org/abstract/document/8317872) use an LSTM that takes in readings from multiple time slots as well as multiple detectors. The data is modeled in a matrix, which captures both the spatial and temporal aspects of the data  

#### hybrid

(T-ZS1) 三种思路：

> 1. RNN 输出特征融入到融合层(fusion layer)(如 FNN) (最简单)
>
> 2. 输出作为下个组成部分的输入(流水线)(先时再空或反过来或多次时间都行)
>
> 3. 作为主预测器，修改模型内部结构 (最复杂) 
>
>    (如反向传播->RTBL, 融入图卷积等)

(T-ZS1) 即：

1) Outputting features to be fed into a fusion layer.

   the simplest because models that fall into this category usually consist of several simpler subnetworks that only interact at the final fusion layer  

2) Outputting features to be fed into subsequent components within the model.

   treats LSTM as a pipeline that transforms one feature representation to another

   > As observed, in this category of method, some preprocessing steps such as the masking of missing values can be a part of the architecture.  

3. Used as the main predictor, but with modifications
  to the internal structure  

  the most complex one, as it requires modifying the internal LSTM structure

(T-ZS1) 分别：

1. 融合层

   - T-3 CNN+2LSTM (空间，短时间特征，周期时间特征)

     a combination of a CNN and two LSTMs to capture spatial features, the short-term temporal feature, and the periodic temporal feature respectively. The outputs from these three networks are then fed into a FNN to fuse the features  

   - [T-35](https://ieeexplore.ieee.org/abstract/document/8258813) CNN+LSTM

     used a combination of a CNN component and an LSTM component to capture spatial features and temporal features respectively. The outputs from these networks are combined to form the prediction  

   - T-29 SAE + LSTM

     a combination of a Stacked Autoencoder to encode traffic accidents data and an LSTM to capture the temporal aspect of the data  

2. 流水线

   - T-1 CNN -> LSTM

   - first used a CNN to encode the spatial aspect of the data and then fed this processed information to an LSTM to learn the temporal aspect  

   - T-4 CNN -> LSTM

     used an LSTM to process the outputs from a CNN before passing them to a max-pooling layer  

   - T-12 缺失值处理 -> 双向LSTM(特征转换) -> LSTM

     performed masking to fill in missing values in the data before passing it to a bidirectional LSTM for feature transformation and then a regular LSTM for the prediction  

   - [T-36](https://arxiv.org/abs/1811.05320) GCN(空间) + GRU(时间)

     used a Gated Recurrent Unit which takes input from a Graph Convolution Network and outputs the predicted traffic

   - [T-37](https://www.researchgate.net/profile/Huaxiu-Yao/publication/323570926_Modeling_Spatial-Temporal_Dynamics_for_Traffic_Prediction/links/5b1e23ea45851587f29f6a61/Modeling-Spatial-Temporal-Dynamics-for-Traffic-Prediction.pdf) 多个 LSTM

     used multiple LSTMs that represent the daily traffic features

   - [T-38](https://www.sciencedirect.com/science/article/pii/S0968090X18302651) 注意力+GRU+CNN

     used a Gated Recurrent Unit to learn feature representation from an attention model which are then fused with the CNN spatial component  

3. 修改结构

   - [T-39](https://ieeexplore.ieee.org/abstract/document/8917706) LSTM: 图卷积+卷积改为RTBL(查不到，疑似自创)

     modified the LSTM calculation to include a graph convolution process as well as using a novel Real-Time Branching Learning (RTBL) which modifies the backpropagation process  

   - [T-40](https://arxiv.org/abs/1707.01926) GRU: 矩阵乘法改为扩散卷积操作

     replaced the matrix multiplication inside Gated Recurrent Unit with the diffusion convolution operation 

#### encoder-decoder

encoder-decoder RNN (编码器-解码器模型)

- Autoencoder 是深层神经网络，两部分组成：

  编码器把输入转成向量表示(通常更低维度)

  解码器把向量还原为近似成输入

- 对 encoder-decoder RNN

  解码器从近似输入改成近似真实答案 目前的 SOTA

(T-ZS1)

In addition to these methods, the encoder-decoder RNNs are also used in many recent studies. Encoder-decoder RNNs are partly inspired by autoencoders. 

Autoencoders are deep neural network structures that consist of two parts: 

- the encoder that takes an input and produces a vector representation of it (usually with a smaller dimension), 
- and the decoder that takes the vector representation and produces an approximation of the original input. 

In encoder-decoder RNNs, both input and output are sequences, and instead of approximating the original input, the target output is a ground-truth sequence (e.g., prediction for 5, 10, 15, 20, 25, and 30 minutes into the future)  

论文例子：

- T-40
- [T-41](https://dl.acm.org/doi/abs/10.1145/3219819.3219895) 200+引用 2018
- [T-42](https://ieeexplore.ieee.org/abstract/document/8580534) STANN 60引用 2019
- [T-43](https://dl.acm.org/doi/abs/10.1145/3292500.3330884) 500+引用 2019
- [T-44](https://www.sciencedirect.com/science/article/pii/S0968090X19301330) 250引用 2019
- [T-45](https://ieeexplore.ieee.org/abstract/document/8708297/) 60+引用 2019

#### other

(T-ZS1) 可以同时获取时空信息

Some authors have used RNNs to capture both the temporal and the spatial aspects of the data  

- T-34 多检测器一次输入 LSTM

  captured the temporal aspect by feeding data from multiple traffic loop detectors at once into an LSTM  

- [T-46](https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/iet-its.2016.0208)  每个检测器一个 LSTM + ODC矩阵表示之间相关

  used one LSTM for each traffic loop detector and incorporates an Origin Destination Correlation (ODC) matrix, which weighs how much the traffic of one loop detector’s location affects another  

- T-30 LSTM 密度核改成卷积来同时捕获时空

  replaced the dense kernels in LSTM with convolutional ones to successfully use an LSTM to capture both the spatial and the temporal aspects of traffic data  

#### 多粒度

(T-ZS1) RNN 输入可能过长，因此只选取多个粒度的代表性数据，分别训练多个粒度的 RNN 组合，如要预测某个时刻的流量，与其输入很长的小时单位，不如输入短的小时，短的天，短的周结合起来，代替可能长达周的小时输入。

In addition, RNN has been used to capture the temporal aspect of the data using different granularities. As discussed in Section 3.2, RNN-based methods are commonly used to learn the temporal patterns of traffic data. However, we also mentioned that RNN-based methods are time-consuming. Consequently, RNN-based methods are not usually fed very long input sequences. Several data modeling-based approaches have been explored to mitigate this problem. The most common method is to use multiple LSTMs with each taking shorter sequences from a specific granularity. 

As an example, if we want to predict the traffic at 09:00 AM at December 25, one RNN can be used to capture the data from 06:00, 07:00, and 08:00 AM at December 25 (hourly granularity), one RNN can be used to capture the data from 09:00 AM at 22, 23 and 24 December (daily granularity) and one RNN can be used to capture the data from 09:00 AM at 4, 11 and 18 December (weekly granularity)  

- 例子如 T-3 T-37 T-38

## 写作技巧

根据阅读归纳：

论文作者人名：一个人就人名(姓)，两个人就 and，三个人就 et al.

中文也名在前，且名两个字只有第一个字大写首字母
